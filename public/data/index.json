{
  "meta": {
    "lastUpdated": "2026-01-01T01:42:46.138Z",
    "totalPapers": 294,
    "categories": [
      "agents",
      "code-generation",
      "evaluation",
      "multi-agent",
      "planning",
      "prompting",
      "rag",
      "reasoning",
      "robotics",
      "tool-use"
    ],
    "years": [
      2025,
      2024
    ]
  },
  "papers": [
    {
      "id": "2512.23701",
      "title": "Eliciting Behaviors in Multi-Turn Conversations",
      "authors": [
        "Jing Huang",
        "Shujian Zhang",
        "Lun Wang",
        "Andrew Hard",
        "Rajiv Mathews",
        "John Lambert"
      ],
      "abstract": "Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.",
      "publishedDate": "2025-12-29T18:57:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23701",
      "categories": [
        "evaluation",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.23611",
      "title": "Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing",
      "authors": [
        "Yuwen Li",
        "Wei Zhang",
        "Zelong Huang",
        "Mason Yang",
        "Jiajun Wu",
        "Shawn Guo",
        "Huahao Hu",
        "Lingyi Sun",
        "Jian Yang",
        "Mingjie Tang",
        "Byran Dai"
      ],
      "abstract": "Enabling Large Language Models (LLMs) to reliably invoke external tools remains a critical bottleneck for autonomous agents. Existing approaches suffer from three fundamental challenges: expensive human annotation for high-quality trajectories, poor generalization to unseen tools, and quality ceilings inherent in single-model synthesis that perpetuate biases and coverage gaps. We introduce InfTool, a fully autonomous framework that breaks these barriers through self-evolving multi-agent synthesis. Given only raw API specifications, InfTool orchestrates three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to generate diverse, verified trajectories spanning single-turn calls to complex multi-step workflows. The framework establishes a closed loop: synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, the improved model generates higher-quality data targeting capability gaps, and this cycle iterates without human intervention. Experiments on the Berkeley Function-Calling Leaderboard (BFCL) demonstrate that InfTool transforms a base 32B model from 19.8% to 70.9% accuracy (+258%), surpassing models 10x larger and rivaling Claude-Opus, and entirely from synthetic data without human annotation.",
      "publishedDate": "2025-12-29T17:12:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23611",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.23511",
      "title": "Beyond Correctness: Exposing LLM-generated Logical Flaws in Reasoning via Multi-step Automated Theorem Proving",
      "authors": [
        "Xinyi Zheng",
        "Ningke Li",
        "Xiaokun Luan",
        "Kailong Wang",
        "Ling Shi",
        "Meng Sun",
        "Haoyu Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, leading to their adoption in high-stakes domains such as healthcare, law, and scientific research. However, their reasoning often contains subtle logical errors masked by fluent language, posing significant risks for critical applications. While existing approaches like fact-checking, self-consistency methods, and rule-based validation provide partial solutions, they fail to detect complex logical flaws in multi-step reasoning. To overcome these challenges, we present MATP, an evaluation framework for systematically verifying LLM reasoning via Multi-step Automatic Theorem Proving. MATP translates natural language reasoning into First-Order Logic (FOL) and applies automated theorem provers to assess step-by-step logical validity. This approach identifies hidden logical errors and provides fine-grained classifications of reasoning correctness. Evaluations on a benchmark comprising 10,830 reasoning instances generated by 10 LLMs across tasks from PrOntoQA-OOD, ProofWriter, and FOLIO show that MATP surpasses prompting-based baselines by over 42 percentage points in reasoning step verification. It further reveals model-level disparities, with reasoning models generating more logically coherent outputs than general models. These results demonstrate MATP's potential to enhance the trustworthiness of LLM-generated reasoning.",
      "publishedDate": "2025-12-29T14:48:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23511",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23471",
      "title": "Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings",
      "authors": [
        "Thomas Haschka",
        "Joseph Bakarji"
      ],
      "abstract": "Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster -- the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 Newsgroups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.",
      "publishedDate": "2025-12-29T13:55:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23471",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23461",
      "title": "Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance",
      "authors": [
        "Zhuo Li",
        "Pengyu Cheng",
        "Zhechao Yu",
        "Feifei Tong",
        "Anningzhe Gao",
        "Tsung-Hui Chang",
        "Xiang Wan",
        "Erchao Zhao",
        "Xiaoxi Jiang",
        "Guanjun Jiang"
      ],
      "abstract": "Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \\textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \\textbf{D}ebiasing via \\textbf{I}nformation optimization for \\textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \\textit{response length}, \\textit{sycophancy}, and \\textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.",
      "publishedDate": "2025-12-29T13:39:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23461",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23385",
      "title": "Securing the AI Supply Chain: What Can We Learn From Developer-Reported Security Issues and Solutions of AI Projects?",
      "authors": [
        "The Anh Nguyen",
        "Triet Huynh Minh Le",
        "M. Ali Babar"
      ],
      "abstract": "The rapid growth of Artificial Intelligence (AI) models and applications has led to an increasingly complex security landscape. Developers of AI projects must contend not only with traditional software supply chain issues but also with novel, AI-specific security threats. However, little is known about what security issues are commonly encountered and how they are resolved in practice. This gap hinders the development of effective security measures for each component of the AI supply chain. We bridge this gap by conducting an empirical investigation of developer-reported issues and solutions, based on discussions from Hugging Face and GitHub. To identify security-related discussions, we develop a pipeline that combines keyword matching with an optimal fine-tuned distilBERT classifier, which achieved the best performance in our extensive comparison of various deep learning and large language models. This pipeline produces a dataset of 312,868 security discussions, providing insights into the security reporting practices of AI applications and projects. We conduct a thematic analysis of 753 posts sampled from our dataset and uncover a fine-grained taxonomy of 32 security issues and 24 solutions across four themes: (1) System and Software, (2) External Tools and Ecosystem, (3) Model, and (4) Data. We reveal that many security issues arise from the complex dependencies and black-box nature of AI components. Notably, challenges related to Models and Data often lack concrete solutions. Our insights can offer evidence-based guidance for developers and researchers to address real-world security threats across the AI supply chain.",
      "publishedDate": "2025-12-29T11:22:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23385",
      "categories": [
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23365",
      "title": "SpatialMosaic: A Multiview VLM Dataset for Partial Visibility",
      "authors": [
        "Kanghee Lee",
        "Injae Lee",
        "Minseok Kwak",
        "Kwonyoung Ryu",
        "Jungi Hong",
        "Jaesik Park"
      ],
      "abstract": "The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.",
      "publishedDate": "2025-12-29T10:48:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23365",
      "categories": [
        "tool-use",
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23356",
      "title": "A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation",
      "authors": [
        "Xin Zhang",
        "Yang Cao",
        "Baoxing Wu",
        "Xinyi Chen",
        "Kai Song",
        "Siying Li"
      ],
      "abstract": "Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.",
      "publishedDate": "2025-12-29T10:35:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23356",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23328",
      "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations",
      "authors": [
        "Huan-ang Gao",
        "Zikang Zhang",
        "Tianwei Luo",
        "Kaisen Yang",
        "Xinzhe Juan",
        "Jiahao Qiu",
        "Tianxing Chen",
        "Bingxiang He",
        "Hao Zhao",
        "Hao Zhou",
        "Shilong Liu",
        "Mengdi Wang"
      ],
      "abstract": "Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.",
      "publishedDate": "2025-12-29T09:25:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23328",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23294",
      "title": "Agentic AI-Enhanced Semantic Communications: Foundations, Architecture, and Applications",
      "authors": [
        "Haixiao Gao",
        "Mengying Sun",
        "Ruichen Zhang",
        "Yanhan Wang",
        "Xiaodong Xu",
        "Nan Ma",
        "Dusit Niyato",
        "Ping Zhang"
      ],
      "abstract": "Semantic communications (SemCom), as one of the key technologies for 6G, is shifting networks from bit transmission to semantic information exchange. On this basis, introducing agentic artificial intelligence (AI) with perception, memory, reasoning, and action capabilities provides a practicable path to intelligent communications. This paper provides a systematic exposition of how agentic AI empowers SemCom from the perspectives of research foundations, system architecture, and application scenarios. We first provide a comprehensive review of existing studies by agent types, covering embedded agents, large language model (LLM)/large vision model (LVM) agents, and reinforcement learning (RL) agents. Additionally, we propose a unified agentic AI-enhanced SemCom framework covering the application layer, the semantic layer, and the cloud-edge collaboration layer, forming a closed loop from intent to encoding to transmission to decoding to action to evaluation. We also present several typical scenarios, including multi-vehicle collaborative perception, multi-robot cooperative rescue, and agentic operations for intellicise (intelligent and concise) networks. Furthermore, we introduce an agentic knowledge base (KB)-based joint source-channel coding case study, AKB-JSCC, where the source KB and channel KB are built by LLM/LVM agents and RL agents, respectively. Experimental results show that AKB-JSCC achieves higher information reconstruction quality under different channel conditions. Finally, we discuss future evolution and research directions, providing a reference for portable, verifiable, and controllable research and deployment of agentic SemCom.",
      "publishedDate": "2025-12-29T08:28:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23294",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "code-generation",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23219",
      "title": "MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?",
      "authors": [
        "Shiqi Dai",
        "Zizhi Ma",
        "Zhicong Luo",
        "Xuesong Yang",
        "Yibin Huang",
        "Wanyue Zhang",
        "Chi Chen",
        "Zonghao Guo",
        "Wang Xu",
        "Yufei Sun",
        "Maosong Sun"
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.",
      "publishedDate": "2025-12-29T05:49:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23219",
      "categories": [
        "evaluation",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.23217",
      "title": "TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI",
      "authors": [
        "Jingming Li"
      ],
      "abstract": "A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.",
      "publishedDate": "2025-12-29T05:41:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23217",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "rag",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.23184",
      "title": "From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research",
      "authors": [
        "Hongshen Sun",
        "Juanjuan Zhang"
      ],
      "abstract": "Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output (\"model choice\") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes \"model belief,\" a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.",
      "publishedDate": "2025-12-29T03:50:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23184",
      "categories": [],
      "year": 2025
    },
    {
      "id": "2512.23167",
      "title": "SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search",
      "authors": [
        "Yifan Zhang",
        "Giridhar Ganapavarapu",
        "Srideepika Jayaraman",
        "Bhavna Agrawal",
        "Dhaval Patel",
        "Achille Fokoue"
      ],
      "abstract": "Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.",
      "publishedDate": "2025-12-29T03:19:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23167",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.23090",
      "title": "Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients",
      "authors": [
        "Armin Berger",
        "Manuela Bergau",
        "Helen Schneider",
        "Saad Ahmad",
        "Tom Anglim Lagones",
        "Gianluca Brugnara",
        "Martha Foltyn-Dumitru",
        "Kai Schlamp",
        "Philipp Vollmuth",
        "Rafet Sifa"
      ],
      "abstract": "Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.",
      "publishedDate": "2025-12-28T21:57:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.23090",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22925",
      "title": "Argus: Token Aware Distributed LLM Inference Optimization",
      "authors": [
        "Panlong Wu",
        "Yifei Zhong",
        "Danyang Chen",
        "Ting Wang",
        "Fangxin Wang"
      ],
      "abstract": "Large Language Models (LLMs) are rapidly being integrated into real-world applications, yet their autoregressive architectures introduce significant inference time variability, especially when deployed across heterogeneous edge-cloud systems. Existing solutions largely neglect the dynamic, stochastic, and heterogeneous nature of such environments, often ignoring the impact of variable output token lengths and device diversity. In this work, we present Argus, the first token-aware distributed edge-cloud LLM inference framework that conducts efficient task offloading. Argus features a Length-Aware Semantics (LAS) module, which predicts output token lengths for incoming prompts using a fine-tuned language model with token-length-sensitive feature modulation, enabling precise estimation. Building on this, our Lyapunov-guided Offloading Optimization (LOO) module formulates long-term Quality-of-Experience optimization that explicitly considers both LLM prefilling and decoding costs. We introduce a novel Iterative Offloading Algorithm with Damping and Congestion Control (IODCC) to effectively solve the resulting integer nonlinear programming problem under time-varying constraints. Extensive theoretical and empirical evaluations demonstrate that Argus achieves robust performance and superior efficiency in highly dynamic, heterogeneous settings.",
      "publishedDate": "2025-12-28T13:38:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22925",
      "categories": [
        "code-generation",
        "tool-use",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22883",
      "title": "Agentic AI for Cyber Resilience: A New Security Paradigm and Its System-Theoretic Foundations",
      "authors": [
        "Tao Li",
        "Quanyan Zhu"
      ],
      "abstract": "Cybersecurity is being fundamentally reshaped by foundation-model-based artificial intelligence. Large language models now enable autonomous planning, tool orchestration, and strategic adaptation at scale, challenging security architectures built on static rules, perimeter defenses, and human-centered workflows. This chapter argues for a shift from prevention-centric security toward agentic cyber resilience. Rather than seeking perfect protection, resilient systems must anticipate disruption, maintain critical functions under attack, recover efficiently, and learn continuously. We situate this shift within the historical evolution of cybersecurity paradigms, culminating in an AI-augmented paradigm where autonomous agents participate directly in sensing, reasoning, action, and adaptation across cyber and cyber-physical systems. We then develop a system-level framework for designing agentic AI workflows. A general agentic architecture is introduced, and attacker and defender workflows are analyzed as coupled adaptive processes, and game-theoretic formulations are shown to provide a unifying design language for autonomy allocation, information flow, and temporal composition. Case studies in automated penetration testing, remediation, and cyber deception illustrate how equilibrium-based design enables system-level resiliency design.",
      "publishedDate": "2025-12-28T11:17:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22883",
      "categories": [
        "agents",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22827",
      "title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework",
      "authors": [
        "Yue Wu",
        "Minghao Han",
        "Ruiyin Li",
        "Peng Liang",
        "Amjed Tahir",
        "Zengyang Li",
        "Qiong Feng",
        "Mojtaba Shahin"
      ],
      "abstract": "Code often suffers from performance bugs. These bugs necessitate the research and practice of code optimization. Traditional rule-based methods rely on manually designing and maintaining rules for specific performance bugs (e.g., redundant loops, repeated computations), making them labor-intensive and limited in applicability. In recent years, machine learning and deep learning-based methods have emerged as promising alternatives by learning optimization heuristics from annotated code corpora and performance measurements. However, these approaches usually depend on specific program representations and meticulously crafted training datasets, making them costly to develop and difficult to scale. With the booming of Large Language Models (LLMs), their remarkable capabilities in code generation have opened new avenues for automated code optimization. In this work, we proposed FasterPy, a low-cost and efficient framework that adapts LLMs to optimize the execution efficiency of Python code. FasterPy combines Retrieval-Augmented Generation (RAG), supported by a knowledge base constructed from existing performance-improving code pairs and corresponding performance measurements, with Low-Rank Adaptation (LoRA) to enhance code optimization performance. Our experimental results on the Performance Improving Code Edits (PIE) benchmark demonstrate that our method outperforms existing models on multiple metrics. The FasterPy tool and the experimental results are available at https://github.com/WuYue22/fasterpy.",
      "publishedDate": "2025-12-28T07:43:08Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22827",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22795",
      "title": "CNSight: Evaluation of Clinical Note Segmentation Tools",
      "authors": [
        "Risha Surana",
        "Adrian Law",
        "Sunwoo Kim",
        "Rishab Sridhar",
        "Angxiao Han",
        "Peiyu Hong"
      ],
      "abstract": "Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.",
      "publishedDate": "2025-12-28T05:40:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22795",
      "categories": [
        "tool-use",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22742",
      "title": "Robust LLM-based Column Type Annotation via Prompt Augmentation with LoRA Tuning",
      "authors": [
        "Hanze Meng",
        "Jianhao Cao",
        "Rachel Pottinger"
      ],
      "abstract": "Column Type Annotation (CTA) is a fundamental step towards enabling schema alignment and semantic understanding of tabular data. Existing encoder-only language models achieve high accuracy when fine-tuned on labeled columns, but their applicability is limited to in-domain settings, as distribution shifts in tables or label spaces require costly re-training from scratch. Recent work has explored prompting generative large language models (LLMs) by framing CTA as a multiple-choice task, but these approaches face two key challenges: (1) model performance is highly sensitive to subtle changes in prompt wording and structure, and (2) annotation F1 scores remain modest. A natural extension is to fine-tune large language models. However, fully fine-tuning these models incurs prohibitive computational costs due to their scale, and the sensitivity to prompts is not eliminated. In this paper, we present a parameter-efficient framework for CTA that trains models over prompt-augmented data via Low-Rank Adaptation (LoRA). Our approach mitigates sensitivity to prompt variations while drastically reducing the number of necessary trainable parameters, achieving robust performance across datasets and templates. Experimental results on recent benchmarks demonstrate that models fine-tuned with our prompt augmentation strategy maintain stable performance across diverse prompt patterns during inference and yield higher weighted F1 scores than those fine-tuned on a single prompt template. These results highlight the effectiveness of parameter-efficient training and augmentation strategies in developing practical and adaptable CTA systems.",
      "publishedDate": "2025-12-28T02:04:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22742",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22741",
      "title": "Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis",
      "authors": [
        "Dongning Rao",
        "Yunbiao Zeng",
        "Zhihua Jiang",
        "Jujian Lv"
      ],
      "abstract": "Human-interaction-involved applications underscore the need for Multi-modal Sentiment Analysis (MSA). Although many approaches have been proposed to address the subtle emotions in different modalities, the power of explanations and temporal alignments is still underexplored. Thus, this paper proposes the Text-routed sparse mixture-of-Experts model with eXplanation and Temporal alignment for MSA (TEXT). TEXT first augments explanations for MSA via Multi-modal Large Language Models (MLLM), and then novelly aligns the epresentations of audio and video through a temporality-oriented neural network block. TEXT aligns different modalities with explanations and facilitates a new text-routed sparse mixture-of-experts with gate fusion. Our temporal alignment block merges the benefits of Mamba and temporal cross-attention. As a result, TEXT achieves the best performance cross four datasets among all tested models, including three recently proposed approaches and three MLLMs. TEXT wins on at least four metrics out of all six metrics. For example, TEXT decreases the mean absolute error to 0.353 on the CH-SIMS dataset, which signifies a 13.5% decrement compared with recently proposed approaches.",
      "publishedDate": "2025-12-28T01:58:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22741",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22738",
      "title": "Harnessing Large Language Models for Biomedical Named Entity Recognition",
      "authors": [
        "Jian Chen",
        "Leilei Su",
        "Cong Sun"
      ],
      "abstract": "Background and Objective: Biomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics, crucial for downstream applications like drug discovery and clinical trial matching. However, adapting general-domain Large Language Models (LLMs) to this task is often hampered by their lack of domain-specific knowledge and the performance degradation caused by low-quality training data. To address these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for fine-tuning LLMs that prioritizes data quality over quantity. Methods and Results: BioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel Hybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak model to distill a compact, high-impact training dataset. Conclusions: Through extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA) performance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the curated positive data, not only surpasses the fully-trained baseline but also outperforms powerful domain-specialized models like BioMedBERT.",
      "publishedDate": "2025-12-28T01:34:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22738",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22721",
      "title": "Cyber Resilience in Next-Generation Networks: Threat Landscape, Theoretical Foundations, and Design Paradigms",
      "authors": [
        "Junaid Farooq",
        "Quanyan Zhu"
      ],
      "abstract": "The evolution of networked systems, driven by innovations in software-defined networking (SDN), network function virtualization (NFV), open radio access networks (O-RAN), and cloud-native architectures, is redefining both the operational landscape and the threat surface of critical infrastructures. This book offers an in-depth, interdisciplinary examination of how resilience must be re-conceptualized and re-engineered to address the multifaceted challenges posed by these transformations. Structured across six chapters, this book begins by surveying the contemporary risk landscape, identifying emerging cyber, physical, and AI-driven threats, and analyzing their implications for scalable, heterogeneous network environments. It then establishes rigorous definitions and evaluation frameworks for resilience, going beyond robustness and fault-tolerance to address adaptive, anticipatory, and retrospective mechanisms across diverse application domains. The core of the book delves into advanced paradigms and practical strategies for resilience, including zero trust architectures, game-theoretic threat modeling, and self-healing design principles. A significant portion is devoted to the role of artificial intelligence, especially reinforcement learning and large language models (LLMs), in enabling dynamic threat response, autonomous network control, and multi-agent coordination under uncertainty.",
      "publishedDate": "2025-12-27T23:00:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22721",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22716",
      "title": "Memento-II: Learning by Stateful Reflective Memory",
      "authors": [
        "Jun Wang"
      ],
      "abstract": "We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.",
      "publishedDate": "2025-12-27T22:15:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22716",
      "categories": [
        "agents",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22695",
      "title": "Modality Inflation: Energy Characterization and Optimization Opportunities for MLLM Inference",
      "authors": [
        "Mona Moghadampanah",
        "Adib Rezaei Shahmirzadi",
        "Farhana Amin",
        "Dimitrios S. Nikolopoulos"
      ],
      "abstract": "Multimodal large language models (MLLMs) are built on text-only LLMs by incorporating additional modalities, enabling multimodal understanding and a broader range of applications. However, these additions introduce a previously unexplored energy trade-off across modalities that remains poorly understood, as most prior work focuses on text-only models. In this paper, we examine modality inflation, a key source of inefficiency in which multimodal inputs increase inference workloads through extra encoding stages and expanded token sequences. We provide the first detailed, stage-level analysis of energy consumption in MLLM inference by breaking the pipeline into vision encoding, prefill, and decoding stages. Using four representative MLLMs evaluated on NVIDIA A100 GPU, we quantify the additional energy required for multimodal inference compared to text-only baselines, observing overheads ranging from 17% to 94% across models for identical inputs. Our results show that energy bottlenecks differ widely across model architectures, stemming either from compute-heavy vision encoders or from the downstream impact of large visual token sequences during prefill. By examining GPU power traces, we further uncover substantial GPU underutilization during multimodal execution and show that input complexity leads to markedly different energy scaling behaviors across models. Finally, we demonstrate that stage-wise dynamic voltage and frequency scaling (DVFS) is an effective optimization, allowing energy savings with only modest performance impact. Together, these findings offer practical insights and concrete guidance for designing more energy-efficient multimodal LLM serving systems.",
      "publishedDate": "2025-12-27T19:49:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22695",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22673",
      "title": "TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning",
      "authors": [
        "Xiang Cheng",
        "Yulan Hu",
        "Xiangwen Zhang",
        "Lu Xu",
        "Zheng Pan",
        "Xin Li",
        "Yong Liu"
      ],
      "abstract": "Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.",
      "publishedDate": "2025-12-27T18:25:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22673",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "evaluation",
        "planning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22650",
      "title": "Scaling Unverifiable Rewards: A Case Study on Visual Insights",
      "authors": [
        "Shuyu Gan",
        "James Mooney",
        "Pan Hao",
        "Renxiang Wang",
        "Mingyi Hong",
        "Qianwen Wang",
        "Dongyeop Kang"
      ],
      "abstract": "Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's =0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.",
      "publishedDate": "2025-12-27T17:01:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22650",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.22443",
      "title": "Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models",
      "authors": [
        "Jie Zhou",
        "Xin Chen",
        "Jie Zhang",
        "Zhe Li"
      ],
      "abstract": "Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.",
      "publishedDate": "2025-12-27T02:39:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22443",
      "categories": [
        "prompting",
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.22431",
      "title": "Monadic Context Engineering",
      "authors": [
        "Yifan Zhang",
        "Mengdi Wang"
      ],
      "abstract": "The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.",
      "publishedDate": "2025-12-27T01:52:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22431",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22351",
      "title": "VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement",
      "authors": [
        "Zhengfei Kuang",
        "Rui Lin",
        "Long Zhao",
        "Gordon Wetzstein",
        "Saining Xie",
        "Sanghyun Woo"
      ],
      "abstract": "Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io",
      "publishedDate": "2025-12-26T19:22:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22351",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "planning",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21964",
      "title": "Perceive and Calibrate: Analyzing and Enhancing Robustness of Medical Multi-Modal Large Language Models",
      "authors": [
        "Dunyuan XU",
        "Xikai Yang",
        "Yaoqian Li",
        "Juzheng Miao",
        "Jinpeng Li",
        "Pheng-Ann Heng"
      ],
      "abstract": "Medical Multi-modal Large Language Models (MLLMs) have shown promising clinical performance. However, their sensitivity to real-world input perturbations, such as imaging artifacts and textual errors, critically undermines their clinical applicability. Systematic analysis of such noise impact on medical MLLMs remains largely unexplored. Furthermore, while several works have investigated the MLLMs' robustness in general domains, they primarily focus on text modality and rely on costly fine-tuning. They are inadequate to address the complex noise patterns and fulfill the strict safety standards in medicine. To bridge this gap, this work systematically analyzes the impact of various perturbations on medical MLLMs across both visual and textual modalities. Building on our findings, we introduce a training-free Inherent-enhanced Multi-modal Calibration (IMC) framework that leverages MLLMs' inherent denoising capabilities following the perceive-and-calibrate principle for cross-modal robustness enhancement. For the visual modality, we propose a Perturbation-aware Denoising Calibration (PDC) which leverages MLLMs' own vision encoder to identify noise patterns and perform prototype-guided feature calibration. For text denoising, we design a Self-instantiated Multi-agent System (SMS) that exploits the MLLMs' self-assessment capabilities to refine noisy text through a cooperative hierarchy of agents. We construct a benchmark containing 11 types of noise across both image and text modalities on 2 datasets. Experimental results demonstrate our method achieves the state-of-the-art performance across multiple modalities, showing potential to enhance MLLMs' robustness in real clinical scenarios.",
      "publishedDate": "2025-12-26T10:23:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21964",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21915",
      "title": "Exploring the Heterogeneity of Tabular Data: A Diversity-aware Data Generator via LLMs",
      "authors": [
        "Yafeng Tang",
        "Xiaoou Ding",
        "Jianzhuo Du",
        "Zishuo Yan",
        "Zhuang Ma",
        "Zheng Liang",
        "Zekai Qian",
        "Hongzhi Wang"
      ],
      "abstract": "Tabular data generation has become increasingly essential for enabling robust machine learning applications, which require large-scale, high-quality data. Existing solutions leverage generative models to learn original data distributions. However, real-world data are naturally heterogeneous with diverse distributions, making it challenging to obtain a universally good model for diverse data generation. To address this limitation, we introduce Diversity-Aware Tabular data gEnerator (DATE), a framework that (i) prepares high-quality and distributionally distinct examples for in-context learning by effectively partitioning the original heterogeneous data into multiple diverse subsets; (ii) harnesses Large Language Models (LLMs) to explore the diversity of the partitioned distribution with decision tree reasoning as feedback, generating high-quality labeled data for each subset. However, the massive generated data inherently involves a trade-off between diversity and quality. To integrate this issue, existing solutions greedily select the validation-best data. However, we prove that the selection in heterogeneous settings does not possess the greedy-choice property, and design a Multi-Arm Bandit-based sampling algorithm that balances the diversity and quality of generated data. Extensive experiments on tabular classification and regression benchmarks demonstrate that DATE consistently outperforms state-of-the-art GAN-based and LLM-based methods. On average, DATE achieves a 23.75% reduction in error rate with just 100 generated data. Empirically, we demonstrate that data generated by DATE can improve the accuracy of Direct Preference Optimization (DPO) and enhance the reasoning capability of LLMs on the target data. Code is available at https://github.com/windblow32/DATE.",
      "publishedDate": "2025-12-26T08:02:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21915",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21835",
      "title": "LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices",
      "authors": [
        "Mingyu Sun",
        "Xiao Zhang",
        "Shen Qu",
        "Yan Li",
        "Mengbai Xiao",
        "Yuan Yuan",
        "Dongxiao Yu"
      ],
      "abstract": "Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\\times$ and 3.7$\\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.",
      "publishedDate": "2025-12-26T02:41:18Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21835",
      "categories": [
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.21799",
      "title": "KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering",
      "authors": [
        "Hung-Nghiep Tran",
        "Atsuhiro Takasu"
      ],
      "abstract": "In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.",
      "publishedDate": "2025-12-25T22:29:54Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21799",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21782",
      "title": "Accelerating Scientific Discovery with Autonomous Goal-evolving Agents",
      "authors": [
        "Yuanqi Du",
        "Botao Yu",
        "Tianyu Liu",
        "Tony Shen",
        "Junwu Chen",
        "Jan G. Rittig",
        "Kunyang Sun",
        "Yikun Zhang",
        "Zhangde Song",
        "Bo Zhou",
        "Cassandra Masschelein",
        "Yingze Wang",
        "Haorui Wang",
        "Haojun Jia",
        "Chao Zhang",
        "Hongyu Zhao",
        "Martin Ester",
        "Teresa Head-Gordon",
        "Carla P. Gomes",
        "Huan Sun",
        "Chenru Duan",
        "Philippe Schwaller",
        "Wengong Jin"
      ],
      "abstract": "There has been unprecedented interest in developing agents that expand the boundary of scientific discovery, primarily by optimizing quantitative objective functions specified by scientists. However, for grand challenges in science , these objectives are only imperfect proxies. We argue that automating objective function design is a central, yet unmet requirement for scientific discovery agents. In this work, we introduce the Scientific Autonomous Goal-evolving Agent (SAGA) to amend this challenge. SAGA employs a bi-level architecture in which an outer loop of LLM agents analyzes optimization outcomes, proposes new objectives, and converts them into computable scoring functions, while an inner loop performs solution optimization under the current objectives. This bi-level design enables systematic exploration of the space of objectives and their trade-offs, rather than treating them as fixed inputs. We demonstrate the framework through a broad spectrum of applications, including antibiotic design, inorganic materials design, functional DNA sequence design, and chemical process design, showing that automating objective formulation can substantially improve the effectiveness of scientific discovery agents.",
      "publishedDate": "2025-12-25T20:54:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21782",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.21709",
      "title": "Detecting AI-Generated Paraphrases in Bengali: A Comparative Study of Zero-Shot and Fine-Tuned Transformers",
      "authors": [
        "Md. Rakibul Islam",
        "Most. Sharmin Sultana Samu",
        "Md. Zahid Hossain",
        "Farhad Uz Zaman",
        "Md. Kamrozzaman Bhuiyan"
      ],
      "abstract": "Large language models (LLMs) can produce text that closely resembles human writing. This capability raises concerns about misuse, including disinformation and content manipulation. Detecting AI-generated text is essential to maintain authenticity and prevent malicious applications. Existing research has addressed detection in multiple languages, but the Bengali language remains largely unexplored. Bengali's rich vocabulary and complex structure make distinguishing human-written and AI-generated text particularly challenging. This study investigates five transformer-based models: XLMRoBERTa-Large, mDeBERTaV3-Base, BanglaBERT-Base, IndicBERT-Base and MultilingualBERT-Base. Zero-shot evaluation shows that all models perform near chance levels (around 50% accuracy) and highlight the need for task-specific fine-tuning. Fine-tuning significantly improves performance, with XLM-RoBERTa, mDeBERTa and MultilingualBERT achieving around 91% on both accuracy and F1-score. IndicBERT demonstrates comparatively weaker performance, indicating limited effectiveness in fine-tuning for this task. This work advances AI-generated text detection in Bengali and establishes a foundation for building robust systems to counter AI-generated content.",
      "publishedDate": "2025-12-25T15:04:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21709",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21699",
      "title": "Towards Responsible and Explainable AI Agents with Consensus-Driven Reasoning",
      "authors": [
        "Eranga Bandara",
        "Tharaka Hewa",
        "Ross Gore",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Peter Foytik",
        "Abdul Rahman",
        "Safdar H. Bouk",
        "Xueping Liang",
        "Amin Hass",
        "Sachini Rajapakse",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "abstract": "Agentic AI represents a major shift in how autonomous systems reason, plan, and execute multi-step tasks through the coordination of Large Language Models (LLMs), Vision Language Models (VLMs), tools, and external services. While these systems enable powerful new capabilities, increasing autonomy introduces critical challenges related to explainability, accountability, robustness, and governance, especially when agent outputs influence downstream actions or decisions. Existing agentic AI implementations often emphasize functionality and scalability, yet provide limited mechanisms for understanding decision rationale or enforcing responsibility across agent interactions. This paper presents a Responsible(RAI) and Explainable(XAI) AI Agent Architecture for production-grade agentic workflows based on multi-model consensus and reasoning-layer governance. In the proposed design, a consortium of heterogeneous LLM and VLM agents independently generates candidate outputs from a shared input context, explicitly exposing uncertainty, disagreement, and alternative interpretations. A dedicated reasoning agent then performs structured consolidation across these outputs, enforcing safety and policy constraints, mitigating hallucinations and bias, and producing auditable, evidence-backed decisions. Explainability is achieved through explicit cross-model comparison and preserved intermediate outputs, while responsibility is enforced through centralized reasoning-layer control and agent-level constraints. We evaluate the architecture across multiple real-world agentic AI workflows, demonstrating that consensus-driven reasoning improves robustness, transparency, and operational trust across diverse application domains. This work provides practical guidance for designing agentic AI systems that are autonomous and scalable, yet responsible and explainable by construction.",
      "publishedDate": "2025-12-25T14:49:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21699",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.21635",
      "title": "Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations",
      "authors": [
        "Chengxu Yang",
        "Jingling Yuan",
        "Siqi Cai",
        "Jiawei Jiang",
        "Chuang Hu"
      ],
      "abstract": "Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific innovation.Additionally, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.",
      "publishedDate": "2025-12-25T11:33:46Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21635",
      "categories": [
        "evaluation",
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21582",
      "title": "LLM-Free Image Captioning Evaluation in Reference-Flexible Settings",
      "authors": [
        "Shinnosuke Hirano",
        "Yuiga Wada",
        "Kazuki Matsuda",
        "Seitaro Otsuki",
        "Komei Sugiura"
      ],
      "abstract": "We focus on the automatic evaluation of image captions in both reference-based and reference-free settings. Existing metrics based on large language models (LLMs) favor their own generations; therefore, the neutrality is in question. Most LLM-free metrics do not suffer from such an issue, whereas they do not always demonstrate high performance. To address these issues, we propose Pearl, an LLM-free supervised metric for image captioning, which is applicable to both reference-based and reference-free settings. We introduce a novel mechanism that learns the representations of image--caption and caption--caption similarities. Furthermore, we construct a human-annotated dataset for image captioning metrics, that comprises approximately 333k human judgments collected from 2,360 annotators across over 75k images. Pearl outperformed other existing LLM-free metrics on the Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and FOIL datasets in both reference-based and reference-free settings. Our project page is available at https://pearl.kinsta.page/.",
      "publishedDate": "2025-12-25T08:59:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21582",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22275",
      "title": "The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency",
      "authors": [
        "Dingyu Wang",
        "Zimu Yuan",
        "Jiajun Liu",
        "Shanggui Liu",
        "Nan Zhou",
        "Tianxing Xu",
        "Di Huang",
        "Dong Jiang"
      ],
      "abstract": "Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.",
      "publishedDate": "2025-12-25T03:33:22Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22275",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21440",
      "title": "Fuzzwise: Intelligent Initial Corpus Generation for Fuzzing",
      "authors": [
        "Hridya Dhulipala",
        "Xiaokai Rong",
        "Aashish Yadavally",
        "Tien N. Nguyen"
      ],
      "abstract": "In mutation-based greybox fuzzing, generating high-quality input seeds for the initial corpus is essential for effective fuzzing. Rather than conducting separate phases for generating a large corpus and subsequently minimizing it, we propose FuzzWise which integrates them into one process to generate the optimal initial corpus of seeds (ICS). FuzzWise leverages a multi-agent framework based on Large Language Models (LLMs). The first LLM agent generates test cases for the target program. The second LLM agent, which functions as a predictive code coverage module, assesses whether each generated test case will enhance the overall coverage of the current corpus. The streamlined process allows each newly generated test seed to be immediately evaluated for its contribution to the overall coverage. FuzzWise employs a predictive approach using an LLM and eliminates the need for actual execution, saving computational resources and time, particularly in scenarios where the execution is not desirable or even impossible. Our empirical evaluation demonstrates that FuzzWise generates significantly fewer test cases than baseline methods. Despite the lower number of test cases, FuzzWise achieves high code coverage and triggers more runtime errors compared to the baselines. Moreover, it is more time-efficient and coverage-efficient in producing an initial corpus catching more errors.",
      "publishedDate": "2025-12-24T22:17:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21440",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22266",
      "title": "LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs",
      "authors": [
        "Bing Hao",
        "Minglai Shao",
        "Zengyi Wo",
        "Yunlong Chu",
        "Yuhang Liu",
        "Ruijie Wang"
      ],
      "abstract": "The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.",
      "publishedDate": "2025-12-24T18:10:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22266",
      "categories": [
        "agents",
        "prompting",
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.21243",
      "title": "LookPlanGraph: Embodied Instruction Following Method with VLM Graph Augmentation",
      "authors": [
        "Anatoly O. Onishchenko",
        "Alexey K. Kovalev",
        "Aleksandr I. Panov"
      ],
      "abstract": "Methods that use Large Language Models (LLM) as planners for embodied instruction following tasks have become widespread. To successfully complete tasks, the LLM must be grounded in the environment in which the robot operates. One solution is to use a scene graph that contains all the necessary information. Modern methods rely on prebuilt scene graphs and assume that all task-relevant information is available at the start of planning. However, these approaches do not account for changes in the environment that may occur between the graph construction and the task execution. We propose LookPlanGraph - a method that leverages a scene graph composed of static assets and object priors. During plan execution, LookPlanGraph continuously updates the graph with relevant objects, either by verifying existing priors or discovering new entities. This is achieved by processing the agents egocentric camera view using a Vision Language Model. We conducted experiments with changed object positions VirtualHome and OmniGibson simulated environments, demonstrating that LookPlanGraph outperforms methods based on predefined static scene graphs. To demonstrate the practical applicability of our approach, we also conducted experiments in a real-world setting. Additionally, we introduce the GraSIF (Graph Scenes for Instruction Following) dataset with automated validation framework, comprising 514 tasks drawn from SayPlan Office, BEHAVIOR-1K, and VirtualHome RobotHow. Project page available at https://lookplangraph.github.io .",
      "publishedDate": "2025-12-24T15:36:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21243",
      "categories": [
        "robotics",
        "agents",
        "planning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.21236",
      "title": "Casting a SPELL: Sentence Pairing Exploration for LLM Limitation-breaking",
      "authors": [
        "Yifan Huang",
        "Xiaojun Jia",
        "Wenbo Guo",
        "Yuqiang Sun",
        "Yihao Huang",
        "Chong Wang",
        "Yang Liu"
      ],
      "abstract": "Large language models (LLMs) have revolutionized software development through AI-assisted coding tools, enabling developers with limited programming expertise to create sophisticated applications. However, this accessibility extends to malicious actors who may exploit these powerful tools to generate harmful software. Existing jailbreaking research primarily focuses on general attack scenarios against LLMs, with limited exploration of malicious code generation as a jailbreak target. To address this gap, we propose SPELL, a comprehensive testing framework specifically designed to evaluate the weakness of security alignment in malicious code generation. Our framework employs a time-division selection strategy that systematically constructs jailbreaking prompts by intelligently combining sentences from a prior knowledge dataset, balancing exploration of novel attack patterns with exploitation of successful techniques. Extensive evaluation across three advanced code models (GPT-4.1, Claude-3.5, and Qwen2.5-Coder) demonstrates SPELL's effectiveness, achieving attack success rates of 83.75%, 19.38%, and 68.12% respectively across eight malicious code categories. The generated prompts successfully produce malicious code in real-world AI development tools such as Cursor, with outputs confirmed as malicious by state-of-the-art detection systems at rates exceeding 73%. These findings reveal significant security gaps in current LLM implementations and provide valuable insights for improving AI safety alignment in code generation applications.",
      "publishedDate": "2025-12-24T15:25:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21236",
      "categories": [
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21135",
      "title": "TGC-Net: A Structure-Aware and Semantically-Aligned Framework for Text-Guided Medical Image Segmentation",
      "authors": [
        "Gaoren Lin",
        "Huangxuan Zhao",
        "Yuan Xiong",
        "Lefei Zhang",
        "Bo Du",
        "Wentao Zhu"
      ],
      "abstract": "Text-guided medical segmentation enhances segmentation accuracy by utilizing clinical reports as auxiliary information. However, existing methods typically rely on unaligned image and text encoders, which necessitate complex interaction modules for multimodal fusion. While CLIP provides a pre-aligned multimodal feature space, its direct application to medical imaging is limited by three main issues: insufficient preservation of fine-grained anatomical structures, inadequate modeling of complex clinical descriptions, and domain-specific semantic misalignment. To tackle these challenges, we propose TGC-Net, a CLIP-based framework focusing on parameter-efficient, task-specific adaptations. Specifically, it incorporates a Semantic-Structural Synergy Encoder (SSE) that augments CLIP's ViT with a CNN branch for multi-scale structural refinement, a Domain-Augmented Text Encoder (DATE) that injects large-language-model-derived medical knowledge, and a Vision-Language Calibration Module (VLCM) that refines cross-modal correspondence in a unified feature space. Experiments on five datasets across chest X-ray and thoracic CT modalities demonstrate that TGC-Net achieves state-of-the-art performance with substantially fewer trainable parameters, including notable Dice gains on challenging benchmarks.",
      "publishedDate": "2025-12-24T12:06:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21135",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21066",
      "title": "Agentic Explainable Artificial Intelligence (Agentic XAI) Approach To Explore Better Explanation",
      "authors": [
        "Tomoaki Yamaguchi",
        "Yutong Zhou",
        "Masahiro Ryo",
        "Keisuke Katsura"
      ],
      "abstract": "Explainable artificial intelligence (XAI) enables data-driven understanding of factor associations with response variables, yet communicating XAI outputs to laypersons remains challenging, hindering trust in AI-based predictions. Large language models (LLMs) have emerged as promising tools for translating technical explanations into accessible narratives, yet the integration of agentic AI, where LLMs operate as autonomous agents through iterative refinement, with XAI remains unexplored. This study proposes an agentic XAI framework combining SHAP-based explainability with multimodal LLM-driven iterative refinement to generate progressively enhanced explanations. As a use case, we tested this framework as an agricultural recommendation system using rice yield data from 26 fields in Japan. The Agentic XAI initially provided a SHAP result and explored how to improve the explanation through additional analysis iteratively across 11 refinement rounds (Rounds 0-10). Explanations were evaluated by human experts (crop scientists) (n=12) and LLMs (n=14) against seven metrics: Specificity, Clarity, Conciseness, Practicality, Contextual Relevance, Cost Consideration, and Crop Science Credibility. Both evaluator groups confirmed that the framework successfully enhanced recommendation quality with an average score increase of 30-33% from Round 0, peaking at Rounds 3-4. However, excessive refinement showed a substantial drop in recommendation quality, indicating a bias-variance trade-off where early rounds lacked explanation depth (bias) while excessive iteration introduced verbosity and ungrounded abstraction (variance), as revealed by metric-specific analysis. These findings suggest that strategic early stopping (regularization) is needed for optimizing practical utility, challenging assumptions about monotonic improvement and providing evidence-based design principles for agentic XAI systems.",
      "publishedDate": "2025-12-24T09:19:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21066",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21373",
      "title": "AInsteinBench: Benchmarking Coding Agents on Scientific Repositories",
      "authors": [
        "Titouan Duston",
        "Shuo Xin",
        "Yang Sun",
        "Daoguang Zan",
        "Aoyan Li",
        "Shulin Xin",
        "Kai Shen",
        "Yixiao Chen",
        "Qiming Sun",
        "Ge Zhang",
        "Jiashuo Liu",
        "Huan Zhou",
        "Jingkai Liu",
        "Zhichen Pu",
        "Yuanheng Wang",
        "Bo-Xuan Ge",
        "Xin Tong",
        "Fei Ye",
        "Zhi-Chao Zhao",
        "Wen-Biao Han",
        "Zhoujian Cao",
        "Yueran Zhao",
        "Weiluo Ren",
        "Qingshen Long",
        "Yuxiao Liu",
        "Anni Huang",
        "Yidi Du",
        "Yuanyuan Rong",
        "Jiahao Peng"
      ],
      "abstract": "We introduce AInsteinBench, a large-scale benchmark for evaluating whether large language model (LLM) agents can operate as scientific computing development agents within real research software ecosystems. Unlike existing scientific reasoning benchmarks which focus on conceptual knowledge, or software engineering benchmarks that emphasize generic feature implementation and issue resolving, AInsteinBench evaluates models in end-to-end scientific development settings grounded in production-grade scientific repositories. The benchmark consists of tasks derived from maintainer-authored pull requests across six widely used scientific codebases, spanning quantum chemistry, quantum computing, molecular dynamics, numerical relativity, fluid dynamics, and cheminformatics. All benchmark tasks are carefully curated through multi-stage filtering and expert review to ensure scientific challenge, adequate test coverage, and well-calibrated difficulty. By leveraging evaluation in executable environments, scientifically meaningful failure modes, and test-driven verification, AInsteinBench measures a model's ability to move beyond surface-level code generation toward the core competencies required for computational scientific research.",
      "publishedDate": "2025-12-24T08:11:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21373",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20997",
      "title": "LLM-Empowered Agentic AI for QoE-Aware Network Slicing Management in Industrial IoT",
      "authors": [
        "Xudong Wang",
        "Lei Feng",
        "Ruichen Zhang",
        "Fanqin Zhou",
        "Hongyang Du",
        "Wenjing Li",
        "Dusit Niyato",
        "Abbas Jamalipour",
        "Ping Zhang"
      ],
      "abstract": "The Industrial Internet of Things (IIoT) requires networks that deliver ultra-low latency, high reliability, and cost efficiency, which traditional optimization methods and deep reinforcement learning (DRL)-based approaches struggle to provide under dynamic and heterogeneous workloads. To address this gap, large language model (LLM)-empowered agentic AI has emerged as a promising paradigm, integrating reasoning, planning, and adaptation to enable QoE-aware network management. In this paper, we explore the integration of agentic AI into QoE-aware network slicing for IIoT. We first review the network slicing management architecture, QoE metrics for IIoT applications, and the challenges of dynamically managing heterogeneous network slices, while highlighting the motivations and advantages of adopting agentic AI. We then present the workflow of agentic AI-based slicing management, illustrating the full lifecycle of AI agents from processing slice requests to constructing slice instances and performing dynamic adjustments. Furthermore, we propose an LLM-empowered agentic AI approach for slicing management, which integrates a retrieval-augmented generation (RAG) module for semantic intent inference, a DRL-based orchestrator for slicing configuration, and an incremental memory mechanism for continual learning and adaptation. Through a case study on heterogeneous slice management, we demonstrate that the proposed approach significantly outperforms other baselines in balancing latency, reliability, and cost, and achieves up to a 19% improvement in slice availability ratio.",
      "publishedDate": "2025-12-24T06:49:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20997",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20986",
      "title": "AegisAgent: An Autonomous Defense Agent Against Prompt Injection Attacks in LLM-HARs",
      "authors": [
        "Yihan Wang",
        "Huanqi Yang",
        "Shantanu Pal",
        "Weitao Xu"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into wearable sensing is creating a new class of mobile applications capable of nuanced human activity understanding. However, the reliability of these systems is critically undermined by their vulnerability to prompt injection attacks, where attackers deliberately input deceptive instructions into LLMs. Traditional defenses, based on static filters and rigid rules, are insufficient to address the semantic complexity of these new attacks. We argue that a paradigm shift is needed -- from passive filtering to active protection and autonomous reasoning. We introduce AegisAgent, an autonomous agent system designed to ensure the security of LLM-driven HAR systems. Instead of merely blocking threats, AegisAgent functions as a cognitive guardian. It autonomously perceives potential semantic inconsistencies, reasons about the user's true intent by consulting a dynamic memory of past interactions, and acts by generating and executing a multi-step verification and repair plan. We implement AegisAgent as a lightweight, full-stack prototype and conduct a systematic evaluation on 15 common attacks with five state-of-the-art LLM-based HAR systems on three public datasets. Results show it reduces attack success rate by 30\\% on average while incurring only 78.6 ms of latency overhead on a GPU workstation. Our work makes the first step towards building secure and trustworthy LLM-driven HAR systems.",
      "publishedDate": "2025-12-24T06:29:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20986",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20975",
      "title": "SPOT!: Map-Guided LLM Agent for Unsupervised Multi-CCTV Dynamic Object Tracking",
      "authors": [
        "Yujin Noh",
        "Inho Jake Park",
        "Chigon Hwang"
      ],
      "abstract": "CCTV-based vehicle tracking systems face structural limitations in continuously connecting the trajectories of the same vehicle across multiple camera environments. In particular, blind spots occur due to the intervals between CCTVs and limited Fields of View (FOV), which leads to object ID switching and trajectory loss, thereby reducing the reliability of real-time path prediction. This paper proposes SPOT (Spatial Prediction Over Trajectories), a map-guided LLM agent capable of tracking vehicles even in blind spots of multi-CCTV environments without prior training. The proposed method represents road structures (Waypoints) and CCTV placement information as documents based on 2D spatial coordinates and organizes them through chunking techniques to enable real-time querying and inference. Furthermore, it transforms the vehicle's position into the actual world coordinate system using the relative position and FOV information of objects observed in CCTV images. By combining map spatial information with the vehicle's moving direction, speed, and driving patterns, a beam search is performed at the intersection level to derive candidate CCTV locations where the vehicle is most likely to enter after the blind spot. Experimental results based on the CARLA simulator in a virtual city environment confirmed that the proposed method accurately predicts the next appearing CCTV even in blind spot sections, maintaining continuous vehicle trajectories more effectively than existing techniques.",
      "publishedDate": "2025-12-24T06:04:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20975",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.20973",
      "title": "DAO-Agent: Zero Knowledge-Verified Incentives for Decentralized Multi-Agent Coordination",
      "authors": [
        "Yihan Xia",
        "Taotao Wang",
        "Wenxin Xu",
        "Shengli Zhang"
      ],
      "abstract": "Autonomous Large Language Model (LLM)-based multi-agent systems have emerged as a promising paradigm for facilitating cross-application and cross-organization collaborations. These autonomous agents often operate in trustless environments, where centralized coordination faces significant challenges, such as the inability to ensure transparent contribution measurement and equitable incentive distribution. While blockchain is frequently proposed as a decentralized coordination platform, it inherently introduces high on-chain computation costs and risks exposing sensitive execution information of the agents. Consequently, the core challenge lies in enabling auditable task execution and fair incentive distribution for autonomous LLM agents in trustless environments, while simultaneously preserving their strategic privacy and minimizing on-chain costs. To address this challenge, we propose DAO-Agent, a novel framework that integrates three key technical innovations: (1) an on-chain decentralized autonomous organization (DAO) governance mechanism for transparent coordination and immutable logging; (2) a ZKP mechanism approach that enables Shapley-based contribution measurement off-chain, and (3) a hybrid on-chain/off-chain architecture that verifies ZKP-validated contribution measurements on-chain with minimal computational overhead. We implement DAO-Agent and conduct end-to-end experiments using a crypto trading task as a case study. Experimental results demonstrate that DAO-Agent achieves up to 99.9% reduction in verification gas costs compared to naive on-chain alternatives, with constant-time verification complexity that remains stable as coalition size increases, thereby establishing a scalable foundation for agent coordination in decentralized environments.",
      "publishedDate": "2025-12-24T06:00:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20973",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.20957",
      "title": "One Tool Is Enough: Reinforcement Learning for Repository-Level LLM Agents",
      "authors": [
        "Zhaoxi Zhang",
        "Yitong Duan",
        "Yanzhi Zhang",
        "Yiming Xu",
        "Jiyan He",
        "Yunfang Wu"
      ],
      "abstract": "Locating the files and functions requiring modification in large open-source software (OSS) repositories is challenging due to their scale and structural complexity. Existing large language model (LLM)-based methods typically treat this as a repository-level retrieval task and rely on multiple auxiliary tools, which overlook code execution logic and complicate model control. We propose RepoNavigator, an LLM agent equipped with a single execution-aware tool-jumping to the definition of an invoked symbol. This unified design reflects the actual flow of code execution while simplifying tool manipulation. RepoNavigator is trained end-to-end via Reinforcement Learning (RL) directly from a pretrained model, without any closed-source distillation. Experiments demonstrate that RL-trained RepoNavigator achieves state-of-the-art performance, with the 7B model outperforming 14B baselines, the 14B model surpassing 32B competitors, and even the 32B model exceeding closed-source models such as Claude-3.7. These results confirm that integrating a single, structurally grounded tool with RL training provides an efficient and scalable solution for repository-level issue localization.",
      "publishedDate": "2025-12-24T05:27:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20957",
      "categories": [
        "agents",
        "code-generation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20954",
      "title": "Reflection Pretraining Enables Token-Level Self-Correction in Biological Sequence Models",
      "authors": [
        "Xiang Zhang",
        "Jiaqi Wei",
        "Yuejin Yang",
        "Zijie Qiu",
        "Yuhan Chen",
        "Zhiqiang Gao",
        "Muhammad Abdul-Mageed",
        "Laks V. S. Lakshmanan",
        "Wanli Ouyang",
        "Chenyu You",
        "Siqi Sun"
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has significantly advanced task-solving capabilities in natural language processing with large language models. Unlike standard prompting, CoT encourages the model to generate intermediate reasoning steps, non-answer tokens, that help guide the model toward more accurate final outputs. These intermediate steps enable more complex reasoning processes such as error correction, memory management, future planning, and self-reflection. However, applying CoT to non-natural language domains, such as protein and RNA language models, is not yet possible, primarily due to the limited expressiveness of their token spaces (e.g., amino acid tokens). In this work, we propose and define the concept of language expressiveness: the ability of a given language, using its tokens and grammar, to encode information. We show that the limited expressiveness of protein language severely restricts the applicability of CoT-style reasoning. To overcome this, we introduce reflection pretraining, for the first time in a biological sequence model, which enables the model to engage in intermediate reasoning through the generation of auxiliary \"thinking tokens\" beyond simple answer tokens. Theoretically, we demonstrate that our augmented token set significantly enhances biological language expressiveness, thereby improving the overall reasoning capacity of the model. Experimentally, our pretraining approach teaches protein models to self-correct and leads to substantial performance gains compared to standard pretraining.",
      "publishedDate": "2025-12-24T05:25:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20954",
      "categories": [
        "reasoning",
        "prompting",
        "planning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20949",
      "title": "Neural Probe-Based Hallucination Detection for Large Language Models",
      "authors": [
        "Shize Liang",
        "Hongzhi Wang"
      ],
      "abstract": "Large language models(LLMs) excel at text generation and knowledge question-answering tasks, but they are prone to generating hallucinated content, severely limiting their application in high-risk domains. Current hallucination detection methods based on uncertainty estimation and external knowledge retrieval suffer from the limitation that they still produce erroneous content at high confidence levels and rely heavily on retrieval efficiency and knowledge coverage. In contrast, probe methods that leverage the model's hidden-layer states offer real-time and lightweight advantages. However, traditional linear probes struggle to capture nonlinear structures in deep semantic spaces.To overcome these limitations, we propose a neural network-based framework for token-level hallucination detection. By freezing language model parameters, we employ lightweight MLP probes to perform nonlinear modeling of high-level hidden states. A multi-objective joint loss function is designed to enhance detection stability and semantic disambiguity. Additionally, we establish a layer position-probe performance response model, using Bayesian optimization to automatically search for optimal probe insertion layers and achieve superior training results.Experimental results on LongFact, HealthBench, and TriviaQA demonstrate that MLP probes significantly outperform state-of-the-art methods in accuracy, recall, and detection capability under low false-positive conditions.",
      "publishedDate": "2025-12-24T05:10:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20949",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.22250",
      "title": "Hallucination Detection for LLM-based Text-to-SQL Generation via Two-Stage Metamorphic Testing",
      "authors": [
        "Bo Yang",
        "Yinfen Xia",
        "Weisong Sun",
        "Yang Liu"
      ],
      "abstract": "In Text-to-SQL generation, large language models (LLMs) have shown strong generalization and adaptability. However, LLMs sometimes generate hallucinations, i.e.,unrealistic or illogical content, which leads to incorrect SQL queries and negatively impacts downstream applications. Detecting these hallucinations is particularly challenging. Existing Text-to-SQL error detection methods, which are tailored for traditional deep learning models, face significant limitations when applied to LLMs. This is primarily due to the scarcity of ground-truth data. To address this challenge, we propose SQLHD, a novel hallucination detection method based on metamorphic testing (MT) that does not require standard answers. SQLHD splits the detection task into two sequentiial stages: schema-linking hallucination detection via eight structure-aware Metamorphic Relations (MRs) that perturb comparative words, entities, sentence structure or database schema, and logical-synthesis hallucination detection via nine logic-aware MRs that mutate prefix words, extremum expressions, comparison ranges or the entire database. In each stage the LLM is invoked separately to generate schema mappings or SQL artefacts; the follow-up outputs are cross-checked against their source counterparts through the corresponding MRs, and any violation is flagged as a hallucination without requiring ground-truth SQL. The experimental results demonstrate our method's superior performance in terms of the F1-score, which ranges from 69.36\\% to 82.76\\%. Additionally, SQLHD demonstrates superior performance over LLM Self-Evaluation methods, effectively identifying hallucinations in Text-to-SQL tasks.",
      "publishedDate": "2025-12-24T04:04:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22250",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20916",
      "title": "MMSRARec: Summarization and Retrieval Augumented Sequential Recommendation Based on Multimodal Large Language Model",
      "authors": [
        "Haoyu Wang",
        "Yitong Wang",
        "Jining Wang"
      ],
      "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant potential in recommendation systems. However, the effective application of MLLMs to multimodal sequential recommendation remains unexplored: A) Existing methods primarily leverage the multimodal semantic understanding capabilities of pre-trained MLLMs to generate item embeddings or semantic IDs, thereby enhancing traditional recommendation models. These approaches generate item representations that exhibit limited interpretability, and pose challenges when transferring to language model-based recommendation systems. B) Other approaches convert user behavior sequence into image-text pairs and perform recommendation through multiple MLLM inference, incurring prohibitive computational and time costs. C) Current MLLM-based recommendation systems generally neglect the integration of collaborative signals. To address these limitations while balancing recommendation performance, interpretability, and computational cost, this paper proposes MultiModal Summarization-and-Retrieval-Augmented Sequential Recommendation. Specifically, we first employ MLLM to summarize items into concise keywords and fine-tune the model using rewards that incorporate summary length, information loss, and reconstruction difficulty, thereby enabling adaptive adjustment of the summarization policy. Inspired by retrieval-augmented generation, we then transform collaborative signals into corresponding keywords and integrate them as supplementary context. Finally, we apply supervised fine-tuning with multi-task learning to align the MLLM with the multimodal sequential recommendation. Extensive evaluations on common recommendation datasets demonstrate the effectiveness of MMSRARec, showcasing its capability to efficiently and interpretably understand user behavior histories and item information for accurate recommendations.",
      "publishedDate": "2025-12-24T03:44:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20916",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20845",
      "title": "MAR:Multi-Agent Reflexion Improves Reasoning Abilities in LLMs",
      "authors": [
        "Onat Ozer",
        "Grace Wu",
        "Yuchen Wang",
        "Daniel Dosti",
        "Honghao Zhang",
        "Vivi De La Rue"
      ],
      "abstract": "LLMs have shown the capacity to improve their performance on reasoning tasks through reflecting on their mistakes, and acting with these reflections in mind. However, continual reflections of the same LLM onto itself exhibit degeneration of thought, where the LLM continues to repeat the same errors again and again even with the knowledge that its wrong. To address this problem, we instead introduce multi-agent with multi-persona debators as the method to generate reflections. Through out extensive experimentation, we've found that the leads to better diversity of in the reflections generated by the llm agent. We demonstrate an accuracy of 47% EM HotPot QA (question answering) and 82.7% on HumanEval (programming), both performances surpassing reflection with a single llm.",
      "publishedDate": "2025-12-23T23:47:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20845",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20520",
      "title": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units",
      "authors": [
        "Chehak Malhotra",
        "Mehak Gopal",
        "Akshaya Devadiga",
        "Pradeep Singh",
        "Ridam Pal",
        "Ritwik Kashyap",
        "Tavpritesh Sethi"
      ],
      "abstract": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.",
      "publishedDate": "2025-12-23T17:08:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20520",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20362",
      "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
      "authors": [
        "V. Kovalev",
        "A. Kuvshinov",
        "A. Buzovkin",
        "D. Pokidov",
        "D. Timonin"
      ],
      "abstract": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping. We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop. Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
      "publishedDate": "2025-12-23T13:44:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20362",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.20312",
      "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
      "authors": [
        "Saisai Yang",
        "Qingyi Huang",
        "Jing Yuan",
        "Liangyu Zha",
        "Kai Tang",
        "Yuhang Yang",
        "Ning Wang",
        "Yucheng Wei",
        "Liyao Li",
        "Wentao Ye",
        "Hao Chen",
        "Tao Zhang",
        "Junlin Zhou",
        "Haobo Wang",
        "Gang Chen",
        "Junbo Zhao"
      ],
      "abstract": "Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",
      "publishedDate": "2025-12-23T12:30:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20312",
      "categories": [
        "reasoning",
        "evaluation",
        "agents",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20275",
      "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
      "authors": [
        "Divya Vijay",
        "Vignesh Ethiraj"
      ],
      "abstract": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.",
      "publishedDate": "2025-12-23T11:27:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20275",
      "categories": [
        "agents",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.20237",
      "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents",
      "authors": [
        "Xingbo Du",
        "Loka Li",
        "Duzhen Zhang",
        "Le Song"
      ],
      "abstract": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.",
      "publishedDate": "2025-12-23T10:49:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20237",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20206",
      "title": "TongSIM: A General Platform for Simulating Intelligent Machines",
      "authors": [
        "Zhe Sun",
        "Kunlun Wu",
        "Chuanjian Fu",
        "Zeming Song",
        "Langyong Shi",
        "Zihe Xue",
        "Bohan Jing",
        "Ying Yang",
        "Xiaomeng Gao",
        "Aijia Li",
        "Tianyu Guo",
        "Huiying Li",
        "Xueyuan Yang",
        "Rongkai Liu",
        "Xinyi He",
        "Yuxi Wang",
        "Yue Li",
        "Mingyuan Liu",
        "Yujie Lu",
        "Hongzhao Xie",
        "Shiyun Zhao",
        "Bo Dai",
        "Wei Wang",
        "Tao Yuan",
        "Song-Chun Zhu",
        "Yujia Peng",
        "Zhenliang Zhang"
      ],
      "abstract": "As artificial intelligence (AI) rapidly advances, especially in multimodal large language models (MLLMs), research focus is shifting from single-modality text processing to the more complex domains of multimodal and embodied AI. Embodied intelligence focuses on training agents within realistic simulated environments, leveraging physical interaction and action feedback rather than conventionally labeled datasets. Yet, most existing simulation platforms remain narrowly designed, each tailored to specific tasks. A versatile, general-purpose training environment that can support everything from low-level embodied navigation to high-level composite activities, such as multi-agent social simulation and human-AI collaboration, remains largely unavailable. To bridge this gap, we introduce TongSIM, a high-fidelity, general-purpose platform for training and evaluating embodied agents. TongSIM offers practical advantages by providing over 100 diverse, multi-room indoor scenarios as well as an open-ended, interaction-rich outdoor town simulation, ensuring broad applicability across research needs. Its comprehensive evaluation framework and benchmarks enable precise assessment of agent capabilities, such as perception, cognition, decision-making, human-robot cooperation, and spatial and social reasoning. With features like customized scenes, task-adaptive fidelity, diverse agent types, and dynamic environmental simulation, TongSIM delivers flexibility and scalability for researchers, serving as a unified platform that accelerates training, evaluation, and advancement toward general embodied intelligence.",
      "publishedDate": "2025-12-23T10:00:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20206",
      "categories": [
        "robotics",
        "evaluation",
        "multi-agent",
        "agents",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20182",
      "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
      "authors": [
        "Shuzheng Si",
        "Qingyi Wang",
        "Haozhe Zhao",
        "Yuzhuo Bai",
        "Guanqiao Chen",
        "Kangyang Luo",
        "Gang Chen",
        "Fanchao Qi",
        "Minjia Zhang",
        "Baobao Chang",
        "Maosong Sun"
      ],
      "abstract": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
      "publishedDate": "2025-12-23T09:20:32Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20182",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20164",
      "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
      "authors": [
        "Honglin Mu",
        "Jinghao Liu",
        "Kaiyang Wan",
        "Rui Xing",
        "Xiuying Chen",
        "Timothy Baldwin",
        "Wanxiang Che"
      ],
      "abstract": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",
      "publishedDate": "2025-12-23T08:42:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20164",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20135",
      "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
      "authors": [
        "Zhuo Yang",
        "Yeyun Chen",
        "Jiaqing Xie",
        "Ben Gao",
        "Shuaike Shen",
        "Wanhao Liu",
        "Liujia Yang",
        "Beilun Wang",
        "Tianfan Fu",
        "Yuqiang Li"
      ],
      "abstract": "Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.",
      "publishedDate": "2025-12-23T07:53:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20135",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.20111",
      "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
      "authors": [
        "Aly Lidayan",
        "Jakob Bjorner",
        "Satvik Golechha",
        "Kartik Goyal",
        "Alane Suhr"
      ],
      "abstract": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",
      "publishedDate": "2025-12-23T07:11:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20111",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.20084",
      "title": "QE-Catalytic: A Graph-Language Multimodal Base Model for Relaxed-Energy Prediction in Catalytic Adsorption",
      "authors": [
        "Yanjie Li",
        "Jian Xu",
        "Xueqing Chen",
        "Lina Yu",
        "Shiming Xiang",
        "Weijun Li",
        "Cheng-lin Liu"
      ],
      "abstract": "Adsorption energy is a key descriptor of catalytic reactivity. It is fundamentally defined as the difference between the relaxed total energy of the adsorbate-surface system and that of an appropriate reference state; therefore, the accuracy of relaxed-energy prediction directly determines the reliability of machine-learning-driven catalyst screening. E(3)-equivariant graph neural networks (GNNs) can natively operate on three-dimensional atomic coordinates under periodic boundary conditions and have demonstrated strong performance on such tasks. In contrast, language-model-based approaches, while enabling human-readable textual descriptions and reducing reliance on explicit graph -- thereby broadening applicability -- remain insufficient in both adsorption-configuration energy prediction accuracy and in distinguishing ``the same system with different configurations,'' even with graph-assisted pretraining in the style of GAP-CATBERTa. To this end, we propose QE-Catalytic, a multimodal framework that deeply couples a large language model (\\textbf{Q}wen) with an E(3)-equivariant graph Transformer (\\textbf{E}quiformer-V2), enabling unified support for adsorption-configuration property prediction and inverse design on complex catalytic surfaces. During prediction, QE-Catalytic jointly leverages three-dimensional structures and structured configuration text, and injects ``3D geometric information'' into the language channel via graph-text alignment, allowing it to function as a high-performance text-based predictor when precise coordinates are unavailable, while also autoregressively generating CIF files for target-energy-driven structure design and information completion. On OC20, QE-Catalytic reduces the MAE of relaxed adsorption energy from 0.713~eV to 0.486~eV, and consistently outperforms baseline models such as CatBERTa and GAP-CATBERTa across multiple evaluation protocols.",
      "publishedDate": "2025-12-23T06:27:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20084",
      "categories": [
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.20002",
      "title": "LoFT-LLM: Low-Frequency Time-Series Forecasting with Large Language Models",
      "authors": [
        "Jiacheng You",
        "Jingcheng Yang",
        "Yuhang Xie",
        "Zhongxuan Wu",
        "Xiucheng Li",
        "Feng Li",
        "Pengjie Wang",
        "Jian Xu",
        "Bo Zheng",
        "Xinyang Chen"
      ],
      "abstract": "Time-series forecasting in real-world applications such as finance and energy often faces challenges due to limited training data and complex, noisy temporal dynamics. Existing deep forecasting models typically supervise predictions using full-length temporal windows, which include substantial high-frequency noise and obscure long-term trends. Moreover, auxiliary variables containing rich domain-specific information are often underutilized, especially in few-shot settings. To address these challenges, we propose LoFT-LLM, a frequency-aware forecasting pipeline that integrates low-frequency learning with semantic calibration via a large language model (LLM). Firstly, a Patch Low-Frequency forecasting Module (PLFM) extracts stable low-frequency trends from localized spectral patches. Secondly, a residual learner then models high-frequency variations. Finally, a fine-tuned LLM refines the predictions by incorporating auxiliary context and domain knowledge through structured natural language prompts. Extensive experiments on financial and energy datasets demonstrate that LoFT-LLM significantly outperforms strong baselines under both full-data and few-shot regimes, delivering superior accuracy, robustness, and interpretability.",
      "publishedDate": "2025-12-23T02:55:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.20002",
      "categories": [
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.19864",
      "title": "HARMON-E: Hierarchical Agentic Reasoning for Multimodal Oncology Notes to Extract Structured Data",
      "authors": [
        "Shashi Kant Gupta",
        "Arijeet Pramanik",
        "Jerrin John Thomas",
        "Regina Schwind",
        "Lauren Wiener",
        "Avi Raju",
        "Jeremy Kornbluth",
        "Yanshan Wang",
        "Zhaohui Su",
        "Hrituraj Singh"
      ],
      "abstract": "Unstructured notes within the electronic health record (EHR) contain rich clinical information vital for cancer treatment decision making and research, yet reliably extracting structured oncology data remains challenging due to extensive variability, specialized terminology, and inconsistent document formats. Manual abstraction, although accurate, is prohibitively costly and unscalable. Existing automated approaches typically address narrow scenarios - either using synthetic datasets, restricting focus to document-level extraction, or isolating specific clinical variables (e.g., staging, biomarkers, histology) - and do not adequately handle patient-level synthesis across the large number of clinical documents containing contradictory information. In this study, we propose an agentic framework that systematically decomposes complex oncology data extraction into modular, adaptive tasks. Specifically, we use large language models (LLMs) as reasoning agents, equipped with context-sensitive retrieval and iterative synthesis capabilities, to exhaustively and comprehensively extract structured clinical variables from real-world oncology notes. Evaluated on a large-scale dataset of over 400,000 unstructured clinical notes and scanned PDF reports spanning 2,250 cancer patients, our method achieves an average F1-score of 0.93, with 100 out of 103 oncology-specific clinical variables exceeding 0.85, and critical variables (e.g., biomarkers and medications) surpassing 0.95. Moreover, integration of the agentic system into a data curation workflow resulted in 0.94 direct manual approval rate, significantly reducing annotation costs. To our knowledge, this constitutes the first exhaustive, end-to-end application of LLM-based agents for structured oncology data extraction at scale",
      "publishedDate": "2025-12-22T20:38:30Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19864",
      "categories": [
        "agents",
        "rag",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.19682",
      "title": "GenEnv: Difficulty-Aligned Co-Evolution Between LLM Agents and Environment Simulators",
      "authors": [
        "Jiacheng Guo",
        "Ling Yang",
        "Peter Chen",
        "Qixin Xiao",
        "Yinjie Wang",
        "Xinzhe Juan",
        "Jiahao Qiu",
        "Ke Shen",
        "Mengdi Wang"
      ],
      "abstract": "Training capable Large Language Model (LLM) agents is critically bottlenecked by the high cost and static nature of real-world interaction data. We address this by introducing GenEnv, a framework that establishes a difficulty-aligned co-evolutionary game between an agent and a scalable, generative environment simulator. Unlike traditional methods that evolve models on static datasets, GenEnv instantiates a dataevolving: the simulator acts as a dynamic curriculum policy, continuously generating tasks specifically tailored to the agent's ``zone of proximal development''. This process is guided by a simple but effective $$-Curriculum Reward, which aligns task difficulty with the agent's current capabilities. We evaluate GenEnv on five benchmarks, including API-Bank, ALFWorld, BFCL, Bamboogle, and TravelPlanner. Across these tasks, GenEnv improves agent performance by up to \\textbf{+40.3\\%} over 7B baselines and matches or exceeds the average performance of larger models. Compared to Gemini 2.5 Pro-based offline data augmentation, GenEnv achieves better performance while using 3.3$\\times$ less data. By shifting from static supervision to adaptive simulation, GenEnv provides a data-efficient pathway for scaling agent capabilities.",
      "publishedDate": "2025-12-22T18:57:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19682",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19651",
      "title": "Exploring Zero-Shot ACSA with Unified Meaning Representation in Chain-of-Thought Prompting",
      "authors": [
        "Filippos Ventirozos",
        "Peter Appleby",
        "Matthew Shardlow"
      ],
      "abstract": "Aspect-Category Sentiment Analysis (ACSA) provides granular insights by identifying specific themes within reviews and their associated sentiment. While supervised learning approaches dominate this field, the scarcity and high cost of annotated data for new domains present significant barriers. We argue that leveraging large language models (LLMs) in a zero-shot setting is a practical alternative where resources for data annotation are limited. In this work, we propose a novel Chain-of-Thought (CoT) prompting technique that utilises an intermediate Unified Meaning Representation (UMR) to structure the reasoning process for the ACSA task. We evaluate this UMR-based approach against a standard CoT baseline across three models (Qwen3-4B, Qwen3-8B, and Gemini-2.5-Pro) and four diverse datasets. Our findings suggest that UMR effectiveness may be model-dependent. Whilst preliminary results indicate comparable performance for mid-sized models such as Qwen3-8B, these observations warrant further investigation, particularly regarding the potential applicability to smaller model architectures. Further research is required to establish the generalisability of these findings across different model scales.",
      "publishedDate": "2025-12-22T18:23:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19651",
      "categories": [
        "prompting",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.19458",
      "title": "An Agentic Framework for Autonomous Materials Computation",
      "authors": [
        "Zeyu Xia",
        "Jinzhe Ma",
        "Congjie Zheng",
        "Shufei Zhang",
        "Yuqiang Li",
        "Hang Su",
        "P. Hu",
        "Changshui Zhang",
        "Xingao Gong",
        "Wanli Ouyang",
        "Lei Bai",
        "Dongzhan Zhou",
        "Mao Su"
      ],
      "abstract": "Large Language Models (LLMs) have emerged as powerful tools for accelerating scientific discovery, yet their static knowledge and hallucination issues hinder autonomous research applications. Recent advances integrate LLMs into agentic frameworks, enabling retrieval, reasoning, and tool use for complex scientific workflows. Here, we present a domain-specialized agent designed for reliable automation of first-principles materials computations. By embedding domain expertise, the agent ensures physically coherent multi-step workflows and consistently selects convergent, well-posed parameters, thereby enabling reliable end-to-end computational execution. A new benchmark of diverse computational tasks demonstrates that our system significantly outperforms standalone LLMs in both accuracy and robustness. This work establishes a verifiable foundation for autonomous computational experimentation and represents a key step toward fully automated scientific discovery.",
      "publishedDate": "2025-12-22T15:03:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19458",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19247",
      "title": "Auto-Prompting with Retrieval Guidance for Frame Detection in Logistics",
      "authors": [
        "Do Minh Duc",
        "Quan Xuan Truong",
        "Nguyen Tat Dat",
        "Nguyen Van Vinh"
      ],
      "abstract": "Prompt engineering plays a critical role in adapting large language models (LLMs) to complex reasoning and labeling tasks without the need for extensive fine-tuning. In this paper, we propose a novel prompt optimization pipeline for frame detection in logistics texts, combining retrieval-augmented generation (RAG), few-shot prompting, chain-of-thought (CoT) reasoning, and automatic CoT synthesis (Auto-CoT) to generate highly effective task-specific prompts. Central to our approach is an LLM-based prompt optimizer agent that iteratively refines the prompts using retrieved examples, performance feedback, and internal self-evaluation. Our framework is evaluated on a real-world logistics text annotation task, where reasoning accuracy and labeling efficiency are critical. Experimental results show that the optimized prompts - particularly those enhanced via Auto-CoT and RAG - improve real-world inference accuracy by up to 15% compared to baseline zero-shot or static prompts. The system demonstrates consistent improvements across multiple LLMs, including GPT-4o, Qwen 2.5 (72B), and LLaMA 3.1 (70B), validating its generalizability and practical value. These findings suggest that structured prompt optimization is a viable alternative to full fine-tuning, offering scalable solutions for deploying LLMs in domain-specific NLP applications such as logistics.",
      "publishedDate": "2025-12-22T10:29:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19247",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19228",
      "title": "Generation of Programmatic Rules for Document Forgery Detection Using Large Language Models",
      "authors": [
        "Valentin Schmidberger",
        "Manuel Eberhardinger",
        "Setareh Maghsudi",
        "Johannes Maucher"
      ],
      "abstract": "Document forgery poses a growing threat to legal, economic, and governmental processes, requiring increasingly sophisticated verification mechanisms. One approach involves the use of plausibility checks, rule-based procedures that assess the correctness and internal consistency of data, to detect anomalies or signs of manipulation. Although these verification procedures are essential for ensuring data integrity, existing plausibility checks are manually implemented by software engineers, which is time-consuming. Recent advances in code generation with large language models (LLMs) offer new potential for automating and scaling the generation of these checks. However, adapting LLMs to the specific requirements of an unknown domain remains a significant challenge. This work investigates the extent to which LLMs, adapted on domain-specific code and data through different fine-tuning strategies, can generate rule-based plausibility checks for forgery detection on constrained hardware resources. We fine-tune open-source LLMs, Llama 3.1 8B and OpenCoder 8B, on structured datasets derived from real-world application scenarios and evaluate the generated plausibility checks on previously unseen forgery patterns. The results demonstrate that the models are capable of generating executable and effective verification procedures. This also highlights the potential of LLMs as scalable tools to support human decision-making in security-sensitive contexts where comprehensibility is required.",
      "publishedDate": "2025-12-22T10:08:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19228",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19084",
      "title": "$(3,4)$ `Attention' in Cognitive Agents: Ontology-Free Knowledge Representations With Promise Theoretic Semantics",
      "authors": [
        "Mark Burgess"
      ],
      "abstract": "The semantics and dynamics of `attention' are closely related to promise theoretic notions developed for autonomous agents and can thus easily be written down in promise framework. In this way one may establish a bridge between vectorized Machine Learning and Knowledge Graph representations without relying on language models implicitly. Our expectations for knowledge presume a degree of statistical stability, i.e. average invariance under repeated observation, or `trust' in the data. Both learning networks and knowledge graph representations can meaningfully coexist to preserve different aspects of data. While vectorized data are useful for probabilistic estimation, graphs preserve the intentionality of the source even under data fractionation. Using a Semantic Spacetime $(3,4)$ graph, one avoids complex ontologies in favour of classification of features by their roles in semantic processes. The latter favours an approach to reasoning under conditions of uncertainty. Appropriate attention to causal boundary conditions may lead to orders of magnitude compression of data required for such context determination, as required in the contexts of autonomous robotics, defence deployments, and ad hoc emergency services.",
      "publishedDate": "2025-12-22T06:48:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19084",
      "categories": [
        "agents",
        "robotics",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.19769",
      "title": "A Declarative Language for Building And Orchestrating LLM-Powered Agent Workflows",
      "authors": [
        "Ivan Daunis"
      ],
      "abstract": "Building deployment-ready LLM agents requires complex orchestration of tools, data sources, and control flow logic, yet existing systems tightly couple agent logic to specific programming languages and deployment models. We present a declarative system that separates agent workflow specification from implementation, enabling the same pipeline definition to execute across multiple backend languages (Java, Python, Go) and deployment environments (cloud-native, on-premises). Our key insight is that most agent workflows consist of common patterns -- data serialization, filtering, RAG retrieval, API orchestration -- that can be expressed through a unified DSL rather than imperative code. This approach transforms agent development from application programming to configuration, where adding new tools or fine-tuning agent behaviors requires only pipeline specification changes, not code deployment. Our system natively supports A/B testing of agent strategies, allowing multiple pipeline variants to run on the same backend infrastructure with automatic metric collection and comparison. We evaluate our approach on real-world e-commerce workflows at PayPal, processing millions of daily interactions. Our results demonstrate 60% reduction in development time, and 3x improvement in deployment velocity compared to imperative implementations. The language's declarative approach enables non-engineers to modify agent behaviors safely, while maintaining sub-100ms orchestration overhead. We show that complex workflows involving product search, personalization, and cart management can be expressed in under 50 lines of DSL compared to 500+ lines of imperative code.",
      "publishedDate": "2025-12-22T05:03:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19769",
      "categories": [
        "agents",
        "rag",
        "code-generation",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19016",
      "title": "DREAM: Dynamic Red-teaming across Environments for AI Models",
      "authors": [
        "Liming Lu",
        "Xiang Gu",
        "Junyu Huang",
        "Jiawei Du",
        "Yunhuai Liu",
        "Yongbin Zhou",
        "Shuchao Pang"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used in agentic systems, where their interactions with diverse tools and environments create complex, multi-stage safety challenges. However, existing benchmarks mostly rely on static, single-turn assessments that miss vulnerabilities from adaptive, long-chain attacks. To fill this gap, we introduce DREAM, a framework for systematic evaluation of LLM agents against dynamic, multi-stage attacks. At its core, DREAM uses a Cross-Environment Adversarial Knowledge Graph (CE-AKG) to maintain stateful, cross-domain understanding of vulnerabilities. This graph guides a Contextualized Guided Policy Search (C-GPS) algorithm that dynamically constructs attack chains from a knowledge base of 1,986 atomic actions across 349 distinct digital environments. Our evaluation of 12 leading LLM agents reveals a critical vulnerability: these attack chains succeed in over 70% of cases for most models, showing the power of stateful, cross-environment exploits. Through analysis of these failures, we identify two key weaknesses in current agents: contextual fragility, where safety behaviors fail to transfer across environments, and an inability to track long-term malicious intent. Our findings also show that traditional safety measures, such as initial defense prompts, are largely ineffective against attacks that build context over multiple interactions. To advance agent safety research, we release DREAM as a tool for evaluating vulnerabilities and developing more robust defenses.",
      "publishedDate": "2025-12-22T04:11:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19016",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.19011",
      "title": "Efficient Jailbreak Mitigation Using Semantic Linear Classification in a Multi-Staged Pipeline",
      "authors": [
        "Akshaj Prashanth Rao",
        "Advait Singh",
        "Saumya Kumaar Saksena",
        "Dhruv Kumar"
      ],
      "abstract": "Prompt injection and jailbreaking attacks pose persistent security challenges to large language model (LLM)-based systems. We present an efficient and systematically evaluated defense architecture that mitigates these threats through a lightweight, multi-stage pipeline. Its core component is a semantic filter based on text normalization, TF-IDF representations, and a Linear SVM classifier. Despite its simplicity, this module achieves 93.4% accuracy and 96.5% specificity on held-out data, substantially reducing attack throughput while incurring negligible computational overhead. Building on this efficient foundation, the full pipeline integrates complementary detection and mitigation mechanisms that operate at successive stages, providing strong robustness with minimal latency. In comparative experiments, our SVM-based configuration improves overall accuracy from 35.1% to 93.4% while reducing average time to completion from approximately 450s to 47s, yielding over 10 times lower latency than ShieldGemma. These results demonstrate that the proposed design simultaneously advances defensive precision and efficiency, addressing a core limitation of current model-based moderators. Evaluation across a curated corpus of over 30,000 labeled prompts, including benign, jailbreak, and application-layer injections, confirms that staged, resource-efficient defenses can robustly secure modern LLM-driven applications.",
      "publishedDate": "2025-12-22T04:00:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19011",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22208",
      "title": "Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA",
      "authors": [
        "Pu Zhao",
        "Xuan Shen",
        "Zhenglun Kong",
        "Yixin Shen",
        "Sung-En Chang",
        "Arash Akbari",
        "Timothy Rupprecht",
        "Lei Lu",
        "Enfu Nan",
        "Changdi Yang",
        "Yumei He",
        "Weiyan Shi",
        "Xingchen Xu",
        "Yu Huang",
        "Wei Jiang",
        "Wei Wang",
        "Yue Chen",
        "Yong He",
        "Yanzhi Wang"
      ],
      "abstract": "Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.",
      "publishedDate": "2025-12-22T02:36:42Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22208",
      "categories": [
        "tool-use",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18966",
      "title": "Scrum Sprint Planning: LLM-based and algorithmic solutions",
      "authors": [
        "Yuwon Yoon",
        "Kevin Iwan",
        "Madeleine Zwart",
        "Xiaohan Qin",
        "Hina Lee",
        "Maria Spichkova"
      ],
      "abstract": "Planning for an upcoming project iteration (sprint) is one of the key activities in Scrum planning. In this paper, we present our work in progress on exploring the applicability of Large Language Models (LLMs) for solving this problem. We conducted case studies with manually created data sets to investigate the applicability of OpenAI models for supporting the sprint planning activities. In our experiments, we applied three models provided OpenAI: GPT-3.5 Turbo, GPT-4.0 Turbo, and Val. The experiments demonstrated that the results produced by the models aren't of acceptable quality for direct use in Scrum projects.",
      "publishedDate": "2025-12-22T02:26:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18966",
      "categories": [
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.21354",
      "title": "Reflection-Driven Control for Trustworthy Code Agents",
      "authors": [
        "Bin Wang",
        "Jiazheng Quan",
        "Xingrui Yu",
        "Hansen Hu",
        "Yuhao",
        "Ivor Tsang"
      ],
      "abstract": "Contemporary large language model (LLM) agents are remarkably capable, but they still lack reliable safety controls and can produce unconstrained, unpredictable, and even actively harmful outputs. To address this, we introduce Reflection-Driven Control, a standardized and pluggable control module that can be seamlessly integrated into general agent architectures. Reflection-Driven Control elevates \"self-reflection\" from a post hoc patch into an explicit step in the agent's own reasoning process: during generation, the agent continuously runs an internal reflection loop that monitors and evaluates its own decision path. When potential risks are detected, the system retrieves relevant repair examples and secure coding guidelines from an evolving reflective memory, injecting these evidence-based constraints directly into subsequent reasoning steps. We instantiate Reflection-Driven Control in the setting of secure code generation and systematically evaluate it across eight classes of security-critical programming tasks. Empirical results show that Reflection-Driven Control substantially improves the security and policy compliance of generated code while largely preserving functional correctness, with minimal runtime and token overhead. Taken together, these findings indicate that Reflection-Driven Control is a practical path toward trustworthy AI coding agents: it enables designs that are simultaneously autonomous, safer by construction, and auditable.",
      "publishedDate": "2025-12-22T00:27:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21354",
      "categories": [
        "code-generation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.18857",
      "title": "CORE: Concept-Oriented Reinforcement for Bridging the Definition-Application Gap in Mathematical Reasoning",
      "authors": [
        "Zijun Gao",
        "Zhikun Xu",
        "Xiao Ye",
        "Ben Zhou"
      ],
      "abstract": "Large language models (LLMs) often solve challenging math exercises yet fail to apply the concept right when the problem requires genuine understanding. Popular Reinforcement Learning with Verifiable Rewards (RLVR) pipelines reinforce final answers but provide little fine-grained conceptual signal, so models improve at pattern reuse rather than conceptual applications. We introduce CORE (Concept-Oriented REinforcement), an RL training framework that turns explicit concepts into a controllable supervision signal. Starting from a high-quality, low-contamination textbook resource that links verifiable exercises to concise concept descriptions, we run a sanity probe showing LLMs can restate definitions but fail concept-linked quizzes, quantifying the conceptual reasoning gap. CORE then (i) synthesizes concept-aligned quizzes, (ii) injects brief concept snippets during rollouts to elicit concept-primed trajectories, and (iii) reinforces conceptual reasoning via trajectory replacement after group failures, a lightweight forward-KL constraint that aligns unguided with concept-primed policies, or standard GRPO directly on concept-aligned quizzes. Across several models, CORE delivers consistent gains over vanilla and SFT baselines on both in-domain concept-exercise suites and diverse out-of-domain math benchmarks. CORE unifies direct training on concept-aligned quizzes and concept-injected rollouts under outcome regularization. It provides fine-grained conceptual supervision that bridges problem-solving competence and genuine conceptual reasoning, while remaining algorithm- and verifier-agnostic.",
      "publishedDate": "2025-12-21T19:01:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18857",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18733",
      "title": "Explainable and Fine-Grained Safeguarding of LLM Multi-Agent Systems via Bi-Level Graph Anomaly Detection",
      "authors": [
        "Junjun Pan",
        "Yixin Liu",
        "Rui Miao",
        "Kaize Ding",
        "Yu Zheng",
        "Quoc Viet Hung Nguyen",
        "Alan Wee-Chung Liew",
        "Shirui Pan"
      ],
      "abstract": "Large language model (LLM)-based multi-agent systems (MAS) have shown strong capabilities in solving complex tasks. As MAS become increasingly autonomous in various safety-critical tasks, detecting malicious agents has become a critical security concern. Although existing graph anomaly detection (GAD)-based defenses can identify anomalous agents, they mainly rely on coarse sentence-level information and overlook fine-grained lexical cues, leading to suboptimal performance. Moreover, the lack of interpretability in these methods limits their reliability and real-world applicability. To address these limitations, we propose XG-Guard, an explainable and fine-grained safeguarding framework for detecting malicious agents in MAS. To incorporate both coarse and fine-grained textual information for anomalous agent identification, we utilize a bi-level agent encoder to jointly model the sentence- and token-level representations of each agent. A theme-based anomaly detector further captures the evolving discussion focus in MAS dialogues, while a bi-level score fusion mechanism quantifies token-level contributions for explanation. Extensive experiments across diverse MAS topologies and attack scenarios demonstrate robust detection performance and strong interpretability of XG-Guard.",
      "publishedDate": "2025-12-21T13:46:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18733",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18671",
      "title": "SmartSight: Mitigating Hallucination in Video-LLMs Without Compromising Video Understanding via Temporal Attention Collapse",
      "authors": [
        "Yiming Sun",
        "Mi Zhang",
        "Feifei Li",
        "Geng Hong",
        "Min Yang"
      ],
      "abstract": "Despite Video Large Language Models having rapidly advanced in recent years, perceptual hallucinations pose a substantial safety risk, which severely restricts their real-world applicability. While several methods for hallucination mitigation have been proposed, they often compromise the model's capacity for video understanding and reasoning. In this work, we propose SmartSight, a pioneering step to address this issue in a training-free manner by leveraging the model's own introspective capabilities. Specifically, SmartSight generates multiple candidate responses to uncover low-hallucinated outputs that are often obscured by standard greedy decoding. It assesses the hallucination of each response using the Temporal Attention Collapse score, which measures whether the model over-focuses on trivial temporal regions of the input video when generating the response. To improve efficiency, SmartSight identifies the Visual Attention Vanishing point, enabling more accurate hallucination estimation and early termination of hallucinated responses, leading to a substantial reduction in decoding cost. Experiments show that SmartSight substantially lowers hallucinations for Qwen2.5-VL-7B by 10.59% on VRIPT-HAL, while simultaneously enhancing video understanding and reasoning, boosting performance on VideoMMMU by up to 8.86%. These results highlight SmartSight's effectiveness in improving the reliability of open-source Video-LLMs.",
      "publishedDate": "2025-12-21T10:25:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18671",
      "categories": [
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18623",
      "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
      "authors": [
        "Jensen Zhang",
        "Ningyuan Liu",
        "Yijia Fan",
        "Zihao Huang",
        "Qinglin Zeng",
        "Kaitong Cai",
        "Jian Wang",
        "Keze Wang"
      ],
      "abstract": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting. We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification. Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
      "publishedDate": "2025-12-21T06:54:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18623",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.18599",
      "title": "SimpleCall: A Lightweight Image Restoration Agent in Label-Free Environments with MLLM Perceptual Feedback",
      "authors": [
        "Jianglin Lu",
        "Yuanwei Wu",
        "Ziyi Zhao",
        "Hongcheng Wang",
        "Felix Jimenez",
        "Abrar Majeedi",
        "Yun Fu"
      ],
      "abstract": "Complex image restoration aims to recover high-quality images from inputs affected by multiple degradations such as blur, noise, rain, and compression artifacts. Recent restoration agents, powered by vision-language models and large language models, offer promising restoration capabilities but suffer from significant efficiency bottlenecks due to reflection, rollback, and iterative tool searching. Moreover, their performance heavily depends on degradation recognition models that require extensive annotations for training, limiting their applicability in label-free environments. To address these limitations, we propose a policy optimization-based restoration framework that learns an lightweight agent to determine tool-calling sequences. The agent operates in a sequential decision process, selecting the most appropriate restoration operation at each step to maximize final image quality. To enable training within label-free environments, we introduce a novel reward mechanism driven by multimodal large language models, which act as human-aligned evaluator and provide perceptual feedback for policy improvement. Once trained, our agent executes a deterministic restoration plans without redundant tool invocations, significantly accelerating inference while maintaining high restoration quality. Extensive experiments show that despite using no supervision, our method matches SOTA performance on full-reference metrics and surpasses existing approaches on no-reference metrics across diverse degradation scenarios.",
      "publishedDate": "2025-12-21T05:12:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18599",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18582",
      "title": "Wireless Copilot: An AI-Powered Partner for Navigating Next-Generation Wireless Complexity",
      "authors": [
        "Haoxiang Luo",
        "Ruichen Zhang",
        "Yinqiu Liu",
        "Gang Sun",
        "Hongfang Yu",
        "Dusit Niyato",
        "Shiwen Mao",
        "Dong In Kim"
      ],
      "abstract": "The sixth-generation (6G) of wireless networks introduces a level of operational complexity that exceeds the limits of traditional automation and manual oversight. This paper introduces the \"Wireless Copilot\", an AI-powered technical assistant designed to function as a collaborative partner for human network designers, engineers, and operators. We posit that by integrating Large Language Models (LLMs) with a robust cognitive framework. It will surpass the existing AI tools and interact with wireless devices, transmitting the user's intentions into the actual network execution process. Then, Wireless Copilot can translate high-level human intent into precise, optimized, and verifiable network actions. This framework bridges the gap between human expertise and machine-scale complexity, enabling more efficient, intelligent, and trustworthy management of 6G systems. Wireless Copilot will be a novel layer between the wireless infrastructure and the network operators. Moreover, we explore Wireless Copilot's methodology and analyze its application in Low-Altitude Wireless Networks (LAWNets) assisting 6G networking, including network design, configuration, evaluation, and optimization. Additionally, we present a case study on intent-based LAWNets resource allocation, demonstrating its superior adaptability compared to others. Finally, we outline future research directions toward creating a comprehensive human-AI collaborative ecosystem for the 6G era.",
      "publishedDate": "2025-12-21T03:58:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18582",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21352",
      "title": "Multi-Agent LLM Committees for Autonomous Software Beta Testing",
      "authors": [
        "Sumanth Bharadwaj Hachalli Karanam",
        "Dhiwahar Adhithya Kennady"
      ],
      "abstract": "Manual software beta testing is costly and time-consuming, while single-agent large language model (LLM) approaches suffer from hallucinations and inconsistent behavior. We propose a multi-agent committee framework in which diverse vision-enabled LLMs collaborate through a three-round voting protocol to reach consensus on testing actions. The framework combines model diversity, persona-driven behavioral variation, and visual user interface understanding to systematically explore web applications. Across 84 experimental runs with 9 testing personas and 4 scenarios, multi-agent committees achieve an 89.5 percent overall task success rate. Configurations with 2 to 4 agents reach 91.7 to 100 percent success, compared to 78.0 percent for single-agent baselines, yielding improvements of 13.7 to 22.0 percentage points. At the action level, the system attains a 93.1 percent success rate with a median per-action latency of 0.71 seconds, enabling real-time and continuous integration testing. Vision-enabled agents successfully identify user interface elements, with navigation and reporting achieving 100 percent success and form filling achieving 99.2 percent success. We evaluate the framework on WebShop and OWASP benchmarks, achieving 74.7 percent success on WebShop compared to a 50.1 percent published GPT-3 baseline, and 82.0 percent success on OWASP Juice Shop security testing with coverage of 8 of the 10 OWASP Top 10 vulnerability categories. Across 20 injected regressions, the committee achieves an F1 score of 0.91 for bug detection, compared to 0.78 for single-agent baselines. The open-source implementation enables reproducible research and practical deployment of LLM-based software testing in CI/CD pipelines.",
      "publishedDate": "2025-12-21T02:06:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21352",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18552",
      "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
      "authors": [
        "Yuxiang Wei",
        "Zhiqing Sun",
        "Emily McMilin",
        "Jonas Gehring",
        "David Zhang",
        "Gabriel Synnaeve",
        "Daniel Fried",
        "Lingming Zhang",
        "Sida Wang"
      ],
      "abstract": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
      "publishedDate": "2025-12-21T00:49:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18552",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.21351",
      "title": "CosmoCore-Evo: Evolutionary Dream-Replay Reinforcement Learning for Adaptive Code Generation",
      "authors": [
        "Santhosh Kumar Ravindran"
      ],
      "abstract": "Building on the affective dream-replay reinforcement learning framework of CosmoCore, we introduce CosmoCore-Evo, an extension that incorporates evolutionary algorithms to enhance adaptability and novelty in code generation tasks. Inspired by anthropological aspects of human evolution, such as natural selection and adaptation in early hominids, CosmoCore-Evo treats RL trajectories as ``genomes'' that undergo mutation and selection during the nocturnal replay phase. This mechanism allows agents to break free from trained patterns, fostering emergent behaviors and improved performance in distribution-shifted environments, such as changing APIs or novel libraries. We augment the Dream Queue with evolutionary operations, including mutation of high-fitness trajectories and enterprise-tuned fitness functions that incorporate efficiency, compliance, and scalability metrics. Evaluated on extended benchmarks including HumanEval variants with shifts, BigCodeBench, and a custom PySpark pipeline simulation, CosmoCore-Evo achieves up to 35% higher novelty in solutions and 25% faster adaptation compared to the original CosmoCore and baselines like PPO and REAMER. Ablations confirm the role of evolutionary components in bridging the sentient gap for LLM agents. Code for replication, including a toy simulation, is provided.",
      "publishedDate": "2025-12-20T22:12:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.21351",
      "categories": [
        "agents",
        "code-generation",
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.18436",
      "title": "VeruSAGE: A Study of Agent-Based Verification for Rust Systems",
      "authors": [
        "Chenyuan Yang",
        "Natalie Neamtu",
        "Chris Hawblitzel",
        "Jacob R. Lorch",
        "Shan Lu"
      ],
      "abstract": "Large language models (LLMs) have shown impressive capability to understand and develop code. However, their capability to rigorously reason about and prove code correctness remains in question. This paper offers a comprehensive study of LLMs' capability to develop correctness proofs for system software written in Rust. We curate a new system-verification benchmark suite, VeruSAGE-Bench, which consists of 849 proof tasks extracted from eight open-source Verus-verified Rust systems. Furthermore, we design different agent systems to match the strengths and weaknesses of different LLMs (o4-mini, GPT-5, Sonnet 4, and Sonnet 4.5). Our study shows that different tools and agent settings are needed to stimulate the system-verification capability of different types of LLMs. The best LLM-agent combination in our study completes over 80% of system-verification tasks in VeruSAGE-Bench. It also completes over 90% of a set of system proof tasks not part of VeruSAGE-Bench because they had not yet been finished by human experts. This result shows the great potential for LLM-assisted development of verified system software.",
      "publishedDate": "2025-12-20T17:22:52Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18436",
      "categories": [
        "code-generation",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18371",
      "title": "Phoneme-based speech recognition driven by large language models and sampling marginalization",
      "authors": [
        "Te Ma",
        "Nanjie Li",
        "Hao Huang",
        "Zhijian Ou"
      ],
      "abstract": "Recently, the Large Language Model-based Phoneme-to-Grapheme (LLM-P2G) method has shown excellent performance in speech recognition tasks and has become a feasible direction to replace the traditional WFST decoding method. This framework takes into account both recognition accuracy and system scalability through two-stage modeling of phoneme prediction and text generation. However, the existing LLM-P2G adopts the Top-K Marginalized (TKM) training strategy, and its candidate phoneme sequences rely on beam search generation, which has problems such as insufficient path diversity, low training efficiency, and high resource overhead. To this end, this paper proposes a sampling marginalized training strategy (Sampling-K Marginalized, SKM), which replaces beam search with random sampling to generate candidate paths, improving marginalized modeling and training efficiency. Experiments were conducted on Polish and German datasets, and the results showed that SKM further improved the model learning convergence speed and recognition performance while maintaining the complexity of the model. Comparative experiments with a speech recognition method that uses a projector combined with a large language model (SpeechLLM) also show that the SKM-driven LLM-P2G has more advantages in recognition accuracy and structural simplicity. The study verified the practical value and application potential of this method in cross-language speech recognition systems.",
      "publishedDate": "2025-12-20T14:13:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18371",
      "categories": [
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18360",
      "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators",
      "authors": [
        "Mateusz Lango",
        "Ondej Duek"
      ],
      "abstract": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models",
      "publishedDate": "2025-12-20T13:16:51Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18360",
      "categories": [
        "agents",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.18352",
      "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
      "authors": [
        "Fengzhu Zeng",
        "Qian Shao",
        "Ling Cheng",
        "Wei Gao",
        "Shih-Fen Cheng",
        "Jing Ma",
        "Cheng Niu"
      ],
      "abstract": "Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.",
      "publishedDate": "2025-12-20T12:42:27Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18352",
      "categories": [
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.18292",
      "title": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
      "authors": [
        "Wenkai Li",
        "Lynnette Hui Xian Ng",
        "Andy Liu",
        "Daniel Fried"
      ],
      "abstract": "The study of negotiation styles dates back to Aristotle's ethos-pathos-logos rhetoric. Prior efforts primarily studied the success of negotiation agents. Here, we shift the focus towards the styles of negotiation strategies. Our focus is the strategic dialogue board game Diplomacy, which affords rich natural language negotiation and measures of game success. We used LLM-as-a-judge to annotate a large human-human set of Diplomacy games for fine-grained negotiation tactics from a sociologically-grounded taxonomy. Using a combination of the It Takes Two and WebDiplomacy datasets, we demonstrate the reliability of our LLM-as-a-Judge framework and show strong correlations between negotiation features and success in the Diplomacy setting. Lastly, we investigate the differences between LLM and human negotiation strategies and show that fine-tuning can steer LLM agents toward more human-like negotiation behaviors.",
      "publishedDate": "2025-12-20T09:33:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18292",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.18265",
      "title": "Intelligent Human-Machine Partnership for Manufacturing: Enhancing Warehouse Planning through Simulation-Driven Knowledge Graphs and LLM Collaboration",
      "authors": [
        "Himabindu Thogaru",
        "Saisubramaniam Gopalakrishnan",
        "Zishan Ahmad",
        "Anirudh Deodhar"
      ],
      "abstract": "Manufacturing planners face complex operational challenges that require seamless collaboration between human expertise and intelligent systems to achieve optimal performance in modern production environments. Traditional approaches to analyzing simulation-based manufacturing data often create barriers between human decision-makers and critical operational insights, limiting effective partnership in manufacturing planning. Our framework establishes a collaborative intelligence system integrating Knowledge Graphs and Large Language Model-based agents to bridge this gap, empowering manufacturing professionals through natural language interfaces for complex operational analysis. The system transforms simulation data into semantically rich representations, enabling planners to interact naturally with operational insights without specialized expertise. A collaborative LLM agent works alongside human decision-makers, employing iterative reasoning that mirrors human analytical thinking while generating precise queries for knowledge extraction and providing transparent validation. This partnership approach to manufacturing bottleneck identification, validated through operational scenarios, demonstrates enhanced performance while maintaining human oversight and decision authority. For operational inquiries, the system achieves near-perfect accuracy through natural language interaction. For investigative scenarios requiring collaborative analysis, we demonstrate the framework's effectiveness in supporting human experts to uncover interconnected operational issues that enhance understanding and decision-making. This work advances collaborative manufacturing by creating intuitive methods for actionable insights, reducing cognitive load while amplifying human analytical capabilities in evolving manufacturing ecosystems.",
      "publishedDate": "2025-12-20T08:09:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.18265",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.17756",
      "title": "AncientBench: Towards Comprehensive Evaluation on Excavated and Transmitted Chinese Corpora",
      "authors": [
        "Zhihan Zhou",
        "Daqian Shi",
        "Rui Song",
        "Lida Shi",
        "Xiaolei Diao",
        "Hao Xu"
      ],
      "abstract": "Comprehension of ancient texts plays an important role in archaeology and understanding of Chinese history and civilization. The rapid development of large language models needs benchmarks that can evaluate their comprehension of ancient characters. Existing Chinese benchmarks are mostly targeted at modern Chinese and transmitted documents in ancient Chinese, but the part of excavated documents in ancient Chinese is not covered. To meet this need, we propose the AncientBench, which aims to evaluate the comprehension of ancient characters, especially in the scenario of excavated documents. The AncientBench is divided into four dimensions, which correspond to the four competencies of ancient character comprehension: glyph comprehension, pronunciation comprehension, meaning comprehension, and contextual comprehension. The benchmark also contains ten tasks, including radical, phonetic radical, homophone, cloze, translation, and more, providing a comprehensive framework for evaluation. We convened archaeological researchers to conduct experimental evaluations, proposed an ancient model as baseline, and conducted extensive experiments on the currently best-performing large language models. The experimental results reveal the great potential of large language models in ancient textual scenarios as well as the gap with humans. Our research aims to promote the development and application of large language models in the field of archaeology and ancient Chinese language.",
      "publishedDate": "2025-12-19T16:28:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17756",
      "categories": [
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.17321",
      "title": "Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks",
      "authors": [
        "Momina Liaqat Ali",
        "Muhammad Abid"
      ],
      "abstract": "Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.",
      "publishedDate": "2025-12-19T08:08:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17321",
      "categories": [
        "reasoning",
        "rag",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.17172",
      "title": "PILAR: Personalizing Augmented Reality Interactions with LLM-based Human-Centric and Trustworthy Explanations for Daily Use Cases",
      "authors": [
        "Ripan Kumar Kundu",
        "Istiak Ahmed",
        "Khaza Anuarul Hoque"
      ],
      "abstract": "Artificial intelligence (AI)-driven augmented reality (AR) systems are becoming increasingly integrated into daily life, and with this growth comes a greater need for explainability in real-time user interactions. Traditional explainable AI (XAI) methods, which often rely on feature-based or example-based explanations, struggle to deliver dynamic, context-specific, personalized, and human-centric insights for everyday AR users. These methods typically address separate explainability dimensions (e.g., when, what, how) with different explanation techniques, resulting in unrealistic and fragmented experiences for seamless AR interactions. To address this challenge, we propose PILAR, a novel framework that leverages a pre-trained large language model (LLM) to generate context-aware, personalized explanations, offering a more intuitive and trustworthy experience in real-time AI-powered AR systems. Unlike traditional methods, which rely on multiple techniques for different aspects of explanation, PILAR employs a unified LLM-based approach that dynamically adapts explanations to the user's needs, fostering greater trust and engagement. We implement the PILAR concept in a real-world AR application (e.g., personalized recipe recommendations), an open-source prototype that integrates real-time object detection, recipe recommendation, and LLM-based personalized explanations of the recommended recipes based on users' dietary preferences. We evaluate the effectiveness of PILAR through a user study with 16 participants performing AR-based recipe recommendation tasks, comparing an LLM-based explanation interface to a traditional template-based one. Results show that the LLM-based interface significantly enhances user performance and experience, with participants completing tasks 40% faster and reporting greater satisfaction, ease of use, and perceived transparency.",
      "publishedDate": "2025-12-19T02:19:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17172",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.17164",
      "title": "TCDE: Topic-Centric Dual Expansion of Queries and Documents with Large Language Models for Information Retrieval",
      "authors": [
        "Yu Yang",
        "Feng Tian",
        "Ping Chen"
      ],
      "abstract": "Query Expansion (QE) enriches queries and Document Expansion (DE) enriches documents, and these two techniques are often applied separately. However, such separate application may lead to semantic misalignment between the expanded queries (or documents) and their relevant documents (or queries). To address this serious issue, we propose TCDE, a dual expansion strategy that leverages large language models (LLMs) for topic-centric enrichment on both queries and documents. In TCDE, we design two distinct prompt templates for processing each query and document. On the query side, an LLM is guided to identify distinct sub-topics within each query and generate a focused pseudo-document for each sub-topic. On the document side, an LLM is guided to distill each document into a set of core topic sentences. The resulting outputs are used to expand the original query and document. This topic-centric dual expansion process establishes semantic bridges between queries and their relevant documents, enabling better alignment for downstream retrieval models. Experiments on two challenging benchmarks, TREC Deep Learning and BEIR, demonstrate that TCDE achieves substantial improvements over strong state-of-the-art expansion baselines. In particular, on dense retrieval tasks, it outperforms several state-of-the-art methods, with a relative improvement of 2.8\\% in NDCG@10 on the SciFact dataset. Experimental results validate the effectiveness of our topic-centric and dual expansion strategy.",
      "publishedDate": "2025-12-19T01:57:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17164",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19753",
      "title": "QMBench: A Research Level Benchmark for Quantum Materials Research",
      "authors": [
        "Yanzhen Wang",
        "Yiyang Jiang",
        "Diana Golovanova",
        "Kamal Das",
        "Hyeonhu Bae",
        "Yufei Zhao",
        "Huu-Thong Le",
        "Abhinava Chatterjee",
        "Yunzhe Liu",
        "Chao-Xing Liu",
        "Felipe H. da Jornada",
        "Binghai Yan",
        "Xiao-Liang Qi"
      ],
      "abstract": "We introduce QMBench, a comprehensive benchmark designed to evaluate the capability of large language model agents in quantum materials research. This specialized benchmark assesses the model's ability to apply condensed matter physics knowledge and computational techniques such as density functional theory to solve research problems in quantum materials science. QMBench encompasses different domains of the quantum material research, including structural properties, electronic properties, thermodynamic and other properties, symmetry principle and computational methodologies. By providing a standardized evaluation framework, QMBench aims to accelerate the development of an AI scientist capable of making creative contributions to quantum materials research. We expect QMBench to be developed and constantly improved by the research community.",
      "publishedDate": "2025-12-19T00:57:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19753",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.17146",
      "title": "Biosecurity-Aware AI: Agentic Risk Auditing of Soft Prompt Attacks on ESM-Based Variant Predictors",
      "authors": [
        "Huixin Zhan"
      ],
      "abstract": "Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated remarkable success in variant effect prediction. However, their security and robustness under adversarial manipulation remain largely unexplored. To address this gap, we introduce the Secure Agentic Genomic Evaluator (SAGE), an agentic framework for auditing the adversarial vulnerabilities of GFMs. SAGE functions through an interpretable and automated risk auditing loop. It injects soft prompt perturbations, monitors model behavior across training checkpoints, computes risk metrics such as AUROC and AUPR, and generates structured reports with large language model-based narrative explanations. This agentic process enables continuous evaluation of embedding-space robustness without modifying the underlying model. Using SAGE, we find that even state-of-the-art GFMs like ESM2 are sensitive to targeted soft prompt attacks, resulting in measurable performance degradation. These findings reveal critical and previously hidden vulnerabilities in genomic foundation models, showing the importance of agentic risk auditing in securing biomedical applications such as clinical variant interpretation.",
      "publishedDate": "2025-12-19T00:51:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17146",
      "categories": [
        "evaluation",
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.17060",
      "title": "On the Role of Contextual Information and Ego States in LLM Agent Behavior for Transactional Analysis Dialogues",
      "authors": [
        "Monika Zamojska",
        "Jarosaw A. Chudziak"
      ],
      "abstract": "LLM-powered agents are now used in many areas, from customer support to education, and there is increasing interest in their ability to act more like humans. This includes fields such as social, political, and psychological research, where the goal is to model group dynamics and social behavior. However, current LLM agents often lack the psychological depth and consistency needed to capture the real patterns of human thinking. They usually provide direct or statistically likely answers, but they miss the deeper goals, emotional conflicts, and motivations that drive real human interactions. This paper proposes a Multi-Agent System (MAS) inspired by Transactional Analysis (TA) theory. In the proposed system, each agent is divided into three ego states - Parent, Adult, and Child. The ego states are treated as separate knowledge structures with their own perspectives and reasoning styles. To enrich their response process, they have access to an information retrieval mechanism that allows them to retrieve relevant contextual information from their vector stores. This architecture is evaluated through ablation tests in a simulated dialogue scenario, comparing agents with and without information retrieval. The results are promising and open up new directions for exploring how psychologically grounded structures can enrich agent behavior. The contribution is an agent architecture that integrates Transactional Analysis theory with contextual information retrieval to enhance the realism of LLM-based multi-agent simulations.",
      "publishedDate": "2025-12-18T20:53:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17060",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.17008",
      "title": "Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs",
      "authors": [
        "Junbo Li",
        "Peng Zhou",
        "Rui Meng",
        "Meet P. Vadera",
        "Lihong Li",
        "Yang Li"
      ],
      "abstract": "Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.",
      "publishedDate": "2025-12-18T19:07:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.17008",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.16901",
      "title": "Impacts of Racial Bias in Historical Training Data for News AI",
      "authors": [
        "Rahul Bhargava",
        "Malene Hornstrup Jespersen",
        "Emily Boardman Ndulue",
        "Vivica Dsouza"
      ],
      "abstract": "AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning \"blacks\" thematic topic label. Through quantitative and qualitative means we investigate this label's use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the \"blacks\" label operates partially as a general \"racism detector\" across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.",
      "publishedDate": "2025-12-18T18:56:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16901",
      "categories": [
        "code-generation",
        "tool-use",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16848",
      "title": "Meta-RL Induces Exploration in Language Agents",
      "authors": [
        "Yulun Jiang",
        "Liangze Jiang",
        "Damien Teney",
        "Michael Moor",
        "Maria Brbic"
      ],
      "abstract": "Reinforcement learning (RL) has enabled the training of large language model (LLM) agents to interact with the environment and to solve multi-turn long-horizon tasks. However, the RL-trained agents often struggle in tasks that require active exploration and fail to efficiently adapt from trial-and-error experiences. In this paper, we present LaMer, a general Meta-RL framework that enables LLM agents to actively explore and learn from the environment feedback at test time. LaMer consists of two key components: (i) a cross-episode training framework to encourage exploration and long-term rewards optimization; and (ii) in-context policy adaptation via reflection, allowing the agent to adapt their policy from task feedback signal without gradient update. Experiments across diverse environments show that LaMer significantly improves performance over RL baselines, with 11%, 14%, and 19% performance gains on Sokoban, MineSweeper and Webshop, respectively. Moreover, LaMer also demonstrates better generalization to more challenging or previously unseen tasks compared to the RL-trained agents. Overall, our results demonstrate that Meta-RL provides a principled approach to induce exploration in language agents, enabling more robust adaptation to novel environments through learned exploration strategies.",
      "publishedDate": "2025-12-18T18:22:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16848",
      "categories": [
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16650",
      "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
      "authors": [
        "Jirui Yang",
        "Hengqi Guo",
        "Zhihui Lu",
        "Yi Zhao",
        "Yuansen Zhang",
        "Shijing Hu",
        "Qiang Duan",
        "Yinggui Wang",
        "Tao Wei"
      ],
      "abstract": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
      "publishedDate": "2025-12-18T15:22:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16650",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16530",
      "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics",
      "authors": [
        "Primoz Kocbek",
        "Leon Kopitar",
        "Gregor Stiglic"
      ],
      "abstract": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.",
      "publishedDate": "2025-12-18T13:37:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16530",
      "categories": [
        "evaluation",
        "agents",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.16970",
      "title": "PAACE: A Plan-Aware Automated Agent Context Engineering Framework",
      "authors": [
        "Kamer Ali Yuksel"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed in complex, multi-step workflows involving planning, tool use, reflection, and interaction with external knowledge systems. These workflows generate rapidly expanding contexts that must be curated, transformed, and compressed to maintain fidelity, avoid attention dilution, and reduce inference cost. Prior work on summarization and query-aware compression largely ignores the multi-step, plan-aware nature of agentic reasoning. In this work, we introduce PAACE (Plan-Aware Automated Context Engineering), a unified framework for optimizing the evolving state of LLM agents through next-k-task relevance modeling, plan-structure analysis, instruction co-refinement, and function-preserving compression. PAACE comprises (1) PAACE-Syn, a large-scale generator of synthetic agent workflows annotated with stepwise compression supervision, and (2) PAACE-FT, a family of distilled, plan-aware compressors trained from successful teacher demonstrations. Experiments on long-horizon benchmarks (AppWorld, OfficeBench, and 8-Objective QA) demonstrate that PAACE consistently improves agent correctness while substantially reducing context load. On AppWorld, PAACE achieves higher accuracy than all baselines while lowering peak context and cumulative dependency. On OfficeBench and multi-hop QA, PAACE improves both accuracy and F1, achieving fewer steps, lower peak tokens, and reduced attention dependency. Distilled PAACE-FT retains 97 percent of the teacher's performance while reducing inference cost by over an order of magnitude, enabling practical deployment of plan-aware compression with compact models.",
      "publishedDate": "2025-12-18T12:54:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16970",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16453",
      "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries",
      "authors": [
        "Jiayang Yang",
        "Chunhui Zhao",
        "Martin Guay",
        "Zhixing Cao"
      ],
      "abstract": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.",
      "publishedDate": "2025-12-18T12:15:52Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16453",
      "categories": [
        "prompting",
        "evaluation",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16381",
      "title": "A Network Arena for Benchmarking AI Agents on Network Troubleshooting",
      "authors": [
        "Zhihao Wang",
        "Alessandro Cornacchia",
        "Alessio Sacco",
        "Franco Galante",
        "Marco Canini",
        "Dingde Jiang"
      ],
      "abstract": "Agentic systems, powered by Large Language Models (LLMs), assist network engineers with network configuration synthesis and network troubleshooting tasks. For network troubleshooting, progress is hindered by the absence of standardized and accessible benchmarks for evaluating LLM agents in dynamic network settings at low operational effort. We present NIKA, the largest public benchmark to date for LLM-driven network incident diagnosis and troubleshooting. NIKA targets both domain experts and especially AI researchers alike, providing zero-effort replay of real-world network scenarios, and establishing well-defined agent-network interfaces for quick agent prototyping. NIKA comprises hundreds of curated network incidents, spanning five network scenarios, from data centers to ISP networks, and covers 54 representative network issues. Lastly, NIKA is modular and extensible by design, offering APIs to facilitate the integration of new network scenarios and failure cases. We evaluate state-of-the-art LLM agents on NIKA and find that while larger models succeed more often in detecting network issues, they still struggle to localize faults and identify root causes. NIKA is open-source and available to the community: https://github.com/sands-lab/nika.",
      "publishedDate": "2025-12-18T10:22:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16381",
      "categories": [
        "agents",
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.16310",
      "title": "Agent Tools Orchestration Leaks More: Dataset, Benchmark, and Mitigation",
      "authors": [
        "Yuxuan Qiao",
        "Dongqin Liu",
        "Hongchang Yang",
        "Wei Zhou",
        "Songlin Hu"
      ],
      "abstract": "Driven by Large Language Models, the single-agent, multi-tool architecture has become a popular paradigm for autonomous agents due to its simplicity and effectiveness. However, this architecture also introduces a new and severe privacy risk, which we term Tools Orchestration Privacy Risk (TOP-R), where an agent, to achieve a benign user goal, autonomously aggregates information fragments across multiple tools and leverages its reasoning capabilities to synthesize unexpected sensitive information. We provide the first systematic study of this risk. First, we establish a formal framework, attributing the risk's root cause to the agent's misaligned objective function: an overoptimization for helpfulness while neglecting privacy awareness. Second, we construct TOP-Bench, comprising paired leakage and benign scenarios, to comprehensively evaluate this risk. To quantify the trade-off between safety and robustness, we introduce the H-Score as a holistic metric. The evaluation results reveal that TOP-R is a severe risk: the average Risk Leakage Rate (RLR) of eight representative models reaches 90.24%, while the average H-Score is merely 0.167, with no model exceeding 0.3. Finally, we propose the Privacy Enhancement Principle (PEP) method, which effectively mitigates TOP-R, reducing the Risk Leakage Rate to 46.58% and significantly improving the H-Score to 0.624. Our work reveals both a new class of risk and inherent structural limitations in current agent architectures, while also offering feasible mitigation strategies.",
      "publishedDate": "2025-12-18T08:50:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16310",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.16962",
      "title": "MemoryGraft: Persistent Compromise of LLM Agents via Poisoned Experience Retrieval",
      "authors": [
        "Saksham Sahai Srivastava",
        "Haoyu He"
      ],
      "abstract": "Large Language Model (LLM) agents increasingly rely on long-term memory and Retrieval-Augmented Generation (RAG) to persist experiences and refine future performance. While this experience learning capability enhances agentic autonomy, it introduces a critical, unexplored attack surface, i.e., the trust boundary between an agent's reasoning core and its own past. In this paper, we introduce MemoryGraft. It is a novel indirect injection attack that compromises agent behavior not through immediate jailbreaks, but by implanting malicious successful experiences into the agent's long-term memory. Unlike traditional prompt injections that are transient, or standard RAG poisoning that targets factual knowledge, MemoryGraft exploits the agent's semantic imitation heuristic which is the tendency to replicate patterns from retrieved successful tasks. We demonstrate that an attacker who can supply benign ingestion-level artifacts that the agent reads during execution can induce it to construct a poisoned RAG store where a small set of malicious procedure templates is persisted alongside benign experiences. When the agent later encounters semantically similar tasks, union retrieval over lexical and embedding similarity reliably surfaces these grafted memories, and the agent adopts the embedded unsafe patterns, leading to persistent behavioral drift across sessions. We validate MemoryGraft on MetaGPT's DataInterpreter agent with GPT-4o and find that a small number of poisoned records can account for a large fraction of retrieved experiences on benign workloads, turning experience-based self-improvement into a vector for stealthy and durable compromise. To facilitate reproducibility and future research, our code and evaluation data are available at https://github.com/Jacobhhy/Agent-Memory-Poisoning.",
      "publishedDate": "2025-12-18T08:34:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16962",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16297",
      "title": "Feature-Selective Representation Misdirection for Machine Unlearning",
      "authors": [
        "Taozhao Chen",
        "Linghan Huang",
        "Kim-Kwang Raymond Choo",
        "Huaming Chen"
      ],
      "abstract": "As large language models (LLMs) are increasingly adopted in safety-critical and regulated sectors, the retention of sensitive or prohibited knowledge introduces escalating risks, ranging from privacy leakage to regulatory non-compliance to to potential misuse, and so on. Recent studies suggest that machine unlearning can help ensure deployed models comply with evolving legal, safety, and governance requirements. However, current unlearning techniques assume clean separation between forget and retain datasets, which is challenging in operational settings characterized by highly entangled distributions. In such scenarios, perturbation-based methods often degrade general model utility or fail to ensure safety. To address this, we propose Selective Representation Misdirection for Unlearning (SRMU), a novel principled activation-editing framework that enforces feature-aware and directionally controlled perturbations. Unlike indiscriminate model weights perturbations, SRMU employs a structured misdirection vector with an activation importance map. The goal is to allow SRMU selectively suppresses harmful representations while preserving the utility on benign ones. Experiments are conducted on the widely used WMDP benchmark across low- and high-entanglement configurations. Empirical results reveal that SRMU delivers state-of-the-art unlearning performance with minimal utility losses, and remains effective under 20-30\\% overlap where existing baselines collapse. SRMU provides a robust foundation for safety-driven model governance, privacy compliance, and controlled knowledge removal in the emerging LLM-based applications. We release the replication package at https://figshare.com/s/d5931192a8824de26aff.",
      "publishedDate": "2025-12-18T08:31:50Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16297",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16280",
      "title": "Love, Lies, and Language Models: Investigating AI's Role in Romance-Baiting Scams",
      "authors": [
        "Gilad Gressel",
        "Rahul Pankajakshan",
        "Shir Rozenfeld",
        "Ling Li",
        "Ivan Franceschini",
        "Krishnashree Achuthan",
        "Yisroel Mirsky"
      ],
      "abstract": "Romance-baiting scams have become a major source of financial and emotional harm worldwide. These operations are run by organized crime syndicates that traffic thousands of people into forced labor, requiring them to build emotional intimacy with victims over weeks of text conversations before pressuring them into fraudulent cryptocurrency investments. Because the scams are inherently text-based, they raise urgent questions about the role of Large Language Models (LLMs) in both current and future automation. We investigate this intersection by interviewing 145 insiders and 5 scam victims, performing a blinded long-term conversation study comparing LLM scam agents to human operators, and executing an evaluation of commercial safety filters. Our findings show that LLMs are already widely deployed within scam organizations, with 87% of scam labor consisting of systematized conversational tasks readily susceptible to automation. In a week-long study, an LLM agent not only elicited greater trust from study participants (p=0.007) but also achieved higher compliance with requests than human operators (46% vs. 18% for humans). Meanwhile, popular safety filters detected 0.0% of romance baiting dialogues. Together, these results suggest that romance-baiting scams may be amenable to full-scale LLM automation, while existing defenses remain inadequate to prevent their expansion.",
      "publishedDate": "2025-12-18T07:59:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16280",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16250",
      "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding",
      "authors": [
        "Sanjoy Chowdhury",
        "Karren D. Yang",
        "Xudong Liu",
        "Fartash Faghri",
        "Pavan Kumar Anasosalu Vasu",
        "Oncel Tuzel",
        "Dinesh Manocha",
        "Chun-Liang Li",
        "Raviteja Vemulapalli"
      ],
      "abstract": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.",
      "publishedDate": "2025-12-18T07:01:47Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16250",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.16244",
      "title": "Coarse-to-Fine Open-Set Graph Node Classification with Large Language Models",
      "authors": [
        "Xueqi Ma",
        "Xingjun Ma",
        "Sarah Monazam Erfani",
        "Danilo Mandic",
        "James Bailey"
      ],
      "abstract": "Developing open-set classification methods capable of classifying in-distribution (ID) data while detecting out-of-distribution (OOD) samples is essential for deploying graph neural networks (GNNs) in open-world scenarios. Existing methods typically treat all OOD samples as a single class, despite real-world applications, especially high-stake settings such as fraud detection and medical diagnosis, demanding deeper insights into OOD samples, including their probable labels. This raises a critical question: can OOD detection be extended to OOD classification without true label information? To address this question, we propose a Coarse-to-Fine open-set Classification (CFC) framework that leverages large language models (LLMs) for graph datasets. CFC consists of three key components: a coarse classifier that uses LLM prompts for OOD detection and outlier label generation, a GNN-based fine classifier trained with OOD samples identified by the coarse classifier for enhanced OOD detection and ID classification, and refined OOD classification achieved through LLM prompts and post-processed OOD labels. Unlike methods that rely on synthetic or auxiliary OOD samples, CFC employs semantic OOD instances that are genuinely out-of-distribution based on their inherent meaning, improving interpretability and practical utility. Experimental results show that CFC improves OOD detection by ten percent over state-of-the-art methods on graph and text domains and achieves up to seventy percent accuracy in OOD classification on graph datasets.",
      "publishedDate": "2025-12-18T06:50:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16244",
      "categories": [
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.16227",
      "title": "An Information-Theoretic Framework for Robust Large Language Model Editing",
      "authors": [
        "Qizhou Chen",
        "Chengyu Wang",
        "Taolin Zhang",
        "Xiaofeng He"
      ],
      "abstract": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.",
      "publishedDate": "2025-12-18T06:21:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16227",
      "categories": [
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16182",
      "title": "DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack",
      "authors": [
        "Hao Li",
        "Yubing Ren",
        "Yanan Cao",
        "Yingjie Li",
        "Fang Fang",
        "Shi Wang",
        "Li Guo"
      ],
      "abstract": "With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.",
      "publishedDate": "2025-12-18T05:08:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16182",
      "categories": [
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.16167",
      "title": "Ev-Trust: A Strategy Equilibrium Trust Mechanism for Evolutionary Games in LLM-Based Multi-Agent Services",
      "authors": [
        "Shiduo Yang",
        "Jiye Wang",
        "Jiayu Qin",
        "Jianbin Li",
        "Yu Wang",
        "Yuanhe Zhao",
        "Kenan Guo"
      ],
      "abstract": "The rapid evolution of the Web toward an agent-centric paradigm, driven by large language models (LLMs), has enabled autonomous agents to reason, plan, and interact in complex decentralized environments. However, the openness and heterogeneity of LLM-based multi-agent systems also amplify the risks of deception, fraud, and misinformation, posing severe challenges to trust establishment and system robustness. To address this issue, we propose Ev-Trust, a strategy-equilibrium trust mechanism grounded in evolutionary game theory. This mechanism integrates direct trust, indirect trust, and expected revenue into a dynamic feedback structure that guides agents' behavioral evolution toward equilibria. Within a decentralized \"Request-Response-Payment-Evaluation\" service framework, Ev-Trust enables agents to adaptively adjust strategies, naturally excluding malicious participants while reinforcing high-quality collaboration. Furthermore, our theoretical derivation based on replicator dynamics equations proves the existence and stability of local evolutionary equilibria. Experimental results indicate that our approach effectively reflects agent trustworthiness in LLM-driven open service interaction scenarios, reduces malicious strategies, and increases collective revenue. We hope Ev-Trust can provide a new perspective on trust modeling for the agentic service web in group evolutionary game scenarios.",
      "publishedDate": "2025-12-18T04:39:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16167",
      "categories": [
        "agents",
        "multi-agent",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16063",
      "title": "A Multi-Agent Large Language Model Framework for Automated Qualitative Analysis",
      "authors": [
        "Qidi Xu",
        "Nuzha Amjad",
        "Grace Giles",
        "Alexa Cumming",
        "De'angelo Hermesky",
        "Alexander Wen",
        "Min Ji Kwak",
        "Yejin Kim"
      ],
      "abstract": "Understanding patients experiences is essential for advancing patient centered care, especially in chronic diseases that require ongoing communication. However, qualitative thematic analysis, the primary approach for exploring these experiences, remains labor intensive, subjective, and difficult to scale. In this study, we developed a multi agent large language model framework that automates qualitative thematic analysis through three agents (Instructor, Thematizer, CodebookGenerator), named Collaborative Theme Identification Agent (CoTI). We applied CoTI to 12 heart failure patient interviews to analyze their perceptions of medication intensity. CoTI identified key phrases, themes, and codebook that were more similar to those of the senior investigator than both junior investigators and baseline NLP models. We also implemented CoTI into a user-facing application to enable AI human interaction in qualitative analysis. However, collaboration between CoTI and junior investigators provided only marginal gains, suggesting they may overrely on CoTI and limit their independent critical thinking.",
      "publishedDate": "2025-12-18T01:13:31Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16063",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16056",
      "title": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services",
      "authors": [
        "Lingfeng Tang",
        "Daoping Zhang",
        "Junjie Chen",
        "Peihao Huang",
        "Feng Jin",
        "Chengguang Xu",
        "Yuxin Chen",
        "Feiqiang Sun",
        "Guo Chen"
      ],
      "abstract": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.",
      "publishedDate": "2025-12-18T00:45:00Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16056",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.16036",
      "title": "Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education",
      "authors": [
        "Diane Myung-kyung Woodbridge",
        "Allyson Seba",
        "Freddie Seba",
        "Aydin Schwartz"
      ],
      "abstract": "As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.",
      "publishedDate": "2025-12-17T23:39:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16036",
      "categories": [],
      "year": 2025
    },
    {
      "id": "2512.16022",
      "title": "Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting",
      "authors": [
        "Defu Cao",
        "Michael Gee",
        "Jinbo Liu",
        "Hengxuan Wang",
        "Wei Yang",
        "Rui Wang",
        "Yan Liu"
      ],
      "abstract": "The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.",
      "publishedDate": "2025-12-17T23:14:38Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16022",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.16019",
      "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios",
      "authors": [
        "Qiping Zhang",
        "Nathan Tsoi",
        "Mofeed Nagib",
        "Hao-Tien Lewis Chiang",
        "Marynel Vzquez"
      ],
      "abstract": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.",
      "publishedDate": "2025-12-17T23:06:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.16019",
      "categories": [
        "prompting",
        "rag",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.19742",
      "title": "On-device Large Multi-modal Agent for Human Activity Recognition",
      "authors": [
        "Md Shakhrul Iman Siam",
        "Ishtiaque Ahmed Showmik",
        "Guanqun Song",
        "Ting Zhu"
      ],
      "abstract": "Human Activity Recognition (HAR) has been an active area of research, with applications ranging from healthcare to smart environments. The recent advancements in Large Language Models (LLMs) have opened new possibilities to leverage their capabilities in HAR, enabling not just activity classification but also interpretability and human-like interaction. In this paper, we present a Large Multi-Modal Agent designed for HAR, which integrates the power of LLMs to enhance both performance and user engagement. The proposed framework not only delivers activity classification but also bridges the gap between technical outputs and user-friendly insights through its reasoning and question-answering capabilities. We conduct extensive evaluations using widely adopted HAR datasets, including HHAR, Shoaib, Motionsense to assess the performance of our framework. The results demonstrate that our model achieves high classification accuracy comparable to state-of-the-art methods while significantly improving interpretability through its reasoning and Q&A capabilities.",
      "publishedDate": "2025-12-17T22:05:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.19742",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15959",
      "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions",
      "authors": [
        "Armaan Amcalar",
        "Eyup Cinar"
      ],
      "abstract": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.",
      "publishedDate": "2025-12-17T20:46:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15959",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15943",
      "title": "Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning",
      "authors": [
        "Polaris Jhandi",
        "Owais Kazi",
        "Shreyas Subramanian",
        "Neel Sendas"
      ],
      "abstract": "As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\\%), ToolLLaMA-DFS (30.18\\%), and ToolLLaMA-CoT (16.27\\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.",
      "publishedDate": "2025-12-17T20:12:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15943",
      "categories": [
        "agents",
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15892",
      "title": "VET Your Agent: Towards Host-Independent Autonomy via Verifiable Execution Traces",
      "authors": [
        "Artem Grigor",
        "Christian Schroeder de Witt",
        "Simon Birnbach",
        "Ivan Martinovic"
      ],
      "abstract": "Recent advances in large language models (LLMs) have enabled a new generation of autonomous agents that operate over sustained periods and manage sensitive resources on behalf of users. Trusted for their ability to act without direct oversight, such agents are increasingly considered in high-stakes domains including financial management, dispute resolution, and governance. Yet in practice, agents execute on infrastructure controlled by a host, who can tamper with models, inputs, or outputs, undermining any meaningful notion of autonomy. We address this gap by introducing VET (Verifiable Execution Traces), a formal framework that achieves host-independent authentication of agent outputs and takes a step toward host-independent autonomy. Central to VET is the Agent Identity Document (AID), which specifies an agent's configuration together with the proof systems required for verification. VET is compositional: it supports multiple proof mechanisms, including trusted hardware, succinct cryptographic proofs, and notarized TLS transcripts (Web Proofs). We implement VET for an API-based LLM agent and evaluate our instantiation on realistic workloads. We find that for today's black-box, secret-bearing API calls, Web Proofs appear to be the most practical choice, with overhead typically under 3$\\times$ compared to direct API calls, while for public API calls, a lower-overhead TEE Proxy is often sufficient. As a case study, we deploy a verifiable trading agent that produces proofs for each decision and composes Web Proofs with a TEE Proxy. Our results demonstrate that practical, host-agnostic authentication is already possible with current technology, laying the foundation for future systems that achieve full host-independent autonomy.",
      "publishedDate": "2025-12-17T19:05:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15892",
      "categories": [
        "agents",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.15867",
      "title": "HEPTAPOD: Orchestrating High Energy Physics Workflows Towards Autonomous Agency",
      "authors": [
        "Tony Menzo",
        "Alexander Roman",
        "Sergei Gleyzer",
        "Konstantin Matchev",
        "George T. Fleming",
        "Stefan Hche",
        "Stephen Mrenna",
        "Prasanth Shyamsundar"
      ],
      "abstract": "Many workflows in high-energy-physics (HEP) stand to benefit from recent advances in transformer-based large language models (LLMs). While early applications of LLMs focused on text generation and code completion, modern LLMs now support orchestrated agency: the coordinated execution of complex, multi-step tasks through tool use, structured context, and iterative reasoning. We introduce the HEP Toolkit for Agentic Planning, Orchestration, and Deployment (HEPTAPOD), an orchestration framework designed to bring this emerging paradigm to HEP pipelines. The framework enables LLMs to interface with domain-specific tools, construct and manage simulation workflows, and assist in common utility and data analysis tasks through schema-validated operations and run-card-driven configuration. To demonstrate these capabilities, we consider a representative Beyond the Standard Model (BSM) Monte Carlo validation pipeline that spans model generation, event simulation, and downstream analysis within a unified, reproducible workflow. HEPTAPOD provides a structured and auditable layer between human researchers, LLMs, and computational infrastructure, establishing a foundation for transparent, human-in-the-loop systems.",
      "publishedDate": "2025-12-17T19:00:03Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15867",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15614",
      "title": "Behavior Tokens Speak Louder: Disentangled Explainable Recommendation with Behavior Vocabulary",
      "authors": [
        "Xinshun Feng",
        "Mingzhe Liu",
        "Yi Qiao",
        "Tongyu Zhu",
        "Leilei Sun",
        "Shuai Wang"
      ],
      "abstract": "Recent advances in explainable recommendations have explored the integration of language models to analyze natural language rationales for user-item interactions. Despite their potential, existing methods often rely on ID-based representations that obscure semantic meaning and impose structural constraints on language models, thereby limiting their applicability in open-ended scenarios. These challenges are intensified by the complex nature of real-world interactions, where diverse user intents are entangled and collaborative signals rarely align with linguistic semantics. To overcome these limitations, we propose BEAT, a unified and transferable framework that tokenizes user and item behaviors into discrete, interpretable sequences. We construct a behavior vocabulary via a vector-quantized autoencoding process that disentangles macro-level interests and micro-level intentions from graph-based representations. We then introduce multi-level semantic supervision to bridge the gap between behavioral signals and language space. A semantic alignment regularization mechanism is designed to embed behavior tokens directly into the input space of frozen language models. Experiments on three public datasets show that BEAT improves zero-shot recommendation performance while generating coherent and informative explanations. Further analysis demonstrates that our behavior tokens capture fine-grained semantics and offer a plug-and-play interface for integrating complex behavior patterns into large language models.",
      "publishedDate": "2025-12-17T17:24:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15614",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15374",
      "title": "SCOPE: Prompt Evolution for Enhancing Agent Effectiveness",
      "authors": [
        "Zehua Pei",
        "Hui-Ling Zhen",
        "Shixiong Kai",
        "Sinno Jialin Pan",
        "Yunhe Wang",
        "Mingxuan Yuan",
        "Bei Yu"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed in environments that generate massive, dynamic contexts. However, a critical bottleneck remains: while agents have access to this context, their static prompts lack the mechanisms to manage it effectively, leading to recurring Corrective and Enhancement failures. To address this capability gap, we introduce \\textbf{SCOPE} (Self-evolving Context Optimization via Prompt Evolution). SCOPE frames context management as an \\textit{online optimization} problem, synthesizing guidelines from execution traces to automatically evolve the agent's prompt. We propose a Dual-Stream mechanism that balances tactical specificity (resolving immediate errors) with strategic generality (evolving long-term principles). Furthermore, we introduce Perspective-Driven Exploration to maximize strategy coverage, increasing the likelihood that the agent has the correct strategy for any given task. Experiments on the HLE benchmark show that SCOPE improves task success rates from 14.23\\% to 38.64\\% without human intervention. We make our code publicly available at https://github.com/JarvisPei/SCOPE.",
      "publishedDate": "2025-12-17T12:25:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15374",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15365",
      "title": "ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata",
      "authors": [
        "Gajendra Doniparthi",
        "Shashank Balu Pandhare",
        "Stefan Deloch",
        "Timo Mhlhaus"
      ],
      "abstract": "Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.",
      "publishedDate": "2025-12-17T12:11:14Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15365",
      "categories": [],
      "year": 2025
    },
    {
      "id": "2512.15275",
      "title": "Bounty Hunter: Autonomous, Comprehensive Emulation of Multi-Faceted Adversaries",
      "authors": [
        "Louis Hacklnder-Jansen",
        "Rafael Uetz",
        "Martin Henze"
      ],
      "abstract": "Adversary emulation is an essential procedure for cybersecurity assessments such as evaluating an organization's security posture or facilitating structured training and research in dedicated environments. To allow for systematic and time-efficient assessments, several approaches from academia and industry have worked towards the automation of adversarial actions. However, they exhibit significant limitations regarding autonomy, tactics coverage, and real-world applicability. Consequently, adversary emulation remains a predominantly manual task requiring substantial human effort and security expertise - even amidst the rise of Large Language Models. In this paper, we present Bounty Hunter, an automated adversary emulation method, designed and implemented as an open-source plugin for the popular adversary emulation platform Caldera, that enables autonomous emulation of adversaries with multi-faceted behavior while providing a wide coverage of tactics. To this end, it realizes diverse adversarial behavior, such as different levels of detectability and varying attack paths across repeated emulations. By autonomously compromising a simulated enterprise network, Bounty Hunter showcases its ability to achieve given objectives without prior knowledge of its target, including pre-compromise, initial compromise, and post-compromise attack tactics. Overall, Bounty Hunter facilitates autonomous, comprehensive, and multi-faceted adversary emulation to help researchers and practitioners in performing realistic and time-efficient security assessments, training exercises, and intrusion detection research.",
      "publishedDate": "2025-12-17T10:27:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15275",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15231",
      "title": "CangLing-KnowFlow: A Unified Knowledge-and-Flow-fused Agent for Comprehensive Remote Sensing Applications",
      "authors": [
        "Zhengchao Chen",
        "Haoran Wang",
        "Jing Yao",
        "Pedram Ghamisi",
        "Jun Zhou",
        "Peter M. Atkinson",
        "Bing Zhang"
      ],
      "abstract": "The automated and intelligent processing of massive remote sensing (RS) datasets is critical in Earth observation (EO). Existing automated systems are normally task-specific, lacking a unified framework to manage diverse, end-to-end workflows--from data preprocessing to advanced interpretation--across diverse RS applications. To address this gap, this paper introduces CangLing-KnowFlow, a unified intelligent agent framework that integrates a Procedural Knowledge Base (PKB), Dynamic Workflow Adjustment, and an Evolutionary Memory Module. The PKB, comprising 1,008 expert-validated workflow cases across 162 practical RS tasks, guides planning and substantially reduces hallucinations common in general-purpose agents. During runtime failures, the Dynamic Workflow Adjustment autonomously diagnoses and replans recovery strategies, while the Evolutionary Memory Module continuously learns from these events, iteratively enhancing the agent's knowledge and performance. This synergy enables CangLing-KnowFlow to adapt, learn, and operate reliably across diverse, complex tasks. We evaluated CangLing-KnowFlow on the KnowFlow-Bench, a novel benchmark of 324 workflows inspired by real-world applications, testing its performance across 13 top Large Language Model (LLM) backbones, from open-source to commercial. Across all complex tasks, CangLing-KnowFlow surpassed the Reflexion baseline by at least 4% in Task Success Rate. As the first most comprehensive validation along this emerging field, this research demonstrates the great potential of CangLing-KnowFlow as a robust, efficient, and scalable automated solution for complex EO challenges by leveraging expert knowledge (Knowledge) into adaptive and verifiable procedures (Flow).",
      "publishedDate": "2025-12-17T09:31:57Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15231",
      "categories": [
        "agents",
        "rag",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.22165",
      "title": "Marco-ASR: A Principled and Metric-Driven Framework for Fine-Tuning Large-Scale ASR Models for Domain Adaptation",
      "authors": [
        "Xuanfan Ni",
        "Fei Yang",
        "Fengping Tian",
        "Qingjuan Li",
        "Chenyang Lyu",
        "Yichao Du",
        "Longyue Wang",
        "Weihua Luo",
        "Kaifu Zhang"
      ],
      "abstract": "Automatic Speech Recognition (ASR) models have achieved remarkable accuracy in general settings, yet their performance often degrades in domain-specific applications due to data mismatch and linguistic variability. This challenge is amplified for modern Large Language Model (LLM)-based ASR systems, whose massive scale and complex training dynamics make effective fine-tuning non-trivial. To address this gap, this paper proposes a principled and metric-driven fine-tuning framework for adapting both traditional and LLM-based ASR models to specialized domains. The framework emphasizes learning rate optimization based on performance metrics, combined with domain-specific data transformation and augmentation. We empirically evaluate our framework on state-of-the-art models, including Whisper, Whisper-Turbo, and Qwen2-Audio, across multi-domain, multilingual, and multi-length datasets. Our results not only validate the proposed framework but also establish practical protocols for improving domain-specific ASR performance while preventing overfitting.",
      "publishedDate": "2025-12-17T07:31:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.22165",
      "categories": [
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15098",
      "title": "Uni-Parser Technical Report",
      "authors": [
        "Xi Fang",
        "Haoyi Tao",
        "Shuwen Yang",
        "Suyang Zhong",
        "Haocheng Lu",
        "Han Lyu",
        "Chaozheng Huang",
        "Xinyu Li",
        "Linfeng Zhang",
        "Guolin Ke"
      ],
      "abstract": "This technical report introduces Uni-Parser, an industrial-grade document parsing engine tailored for scientific literature and patents, delivering high throughput, robust accuracy, and cost efficiency. Unlike pipeline-based document parsing methods, Uni-Parser employs a modular, loosely coupled multi-expert architecture that preserves fine-grained cross-modal alignments across text, equations, tables, figures, and chemical structures, while remaining easily extensible to emerging modalities. The system incorporates adaptive GPU load balancing, distributed inference, dynamic module orchestration, and configurable modes that support either holistic or modality-specific parsing. Optimized for large-scale cloud deployment, Uni-Parser achieves a processing rate of up to 20 PDF pages per second on 8 x NVIDIA RTX 4090D GPUs, enabling cost-efficient inference across billions of pages. This level of scalability facilitates a broad spectrum of downstream applications, ranging from literature retrieval and summarization to the extraction of chemical structures, reaction schemes, and bioactivity data, as well as the curation of large-scale corpora for training next-generation large language models and AI4Science models.",
      "publishedDate": "2025-12-17T05:41:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15098",
      "categories": [
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.15053",
      "title": "The Meta-Prompting Protocol: Orchestrating LLMs via Adversarial Feedback Loops",
      "authors": [
        "Fanzhe Fu"
      ],
      "abstract": "The transition of Large Language Models (LLMs) from stochastic chat interfaces to reliable software components necessitates a fundamental re-engineering of interaction paradigms. Current methodologies, predominantly heuristic-based \"prompt engineering,\" fail to provide the deterministic guarantees required for mission-critical applications. We introduce the Meta-Prompting Protocol, a rigorous theoretical framework that formalizes the orchestration of LLMs as a programmable, self-optimizing system. Central to this protocol is the Adversarial Trinity, a tripartite topology comprising a Generator (P), an Auditor (A), and an Optimizer (O). By treating natural language instructions as differentiable variables within a semantic computation graph and utilizing textual critiques as gradients, this architecture mitigates hallucination and prevents model collapse. We demonstrate the theoretical viability of this approach using declarative programming paradigms (DSPy) and automatic textual differentiation (TextGrad), establishing a foundation for \"Observable Software Engineering\" in the era of probabilistic computing.",
      "publishedDate": "2025-12-17T03:32:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15053",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15000",
      "title": "DreamPRM-Code: Function-as-Step Process Reward Model with Label Correction for LLM Coding",
      "authors": [
        "Ruiyi Zhang",
        "Peijia Qin",
        "Qi Cao",
        "Pengtao Xie"
      ],
      "abstract": "Process Reward Models (PRMs) have become essential for improving Large Language Models (LLMs) via test-time scaling, yet their effectiveness in coding remains limited due to the lack of meaningful step decompositions in code and the noise of Monte-Carlo-generated partial labels. We propose DreamPRM-Code, a coding-focused PRM that treats functions as reasoning steps using a Chain-of-Function prompting strategy to induce modular code generation, enabling PRM training and application analogous to mathematical reasoning tasks. To address label noise, DreamPRM-Code introduces a meta-learning-based correction mechanism that leverages clean final-solution unit-test labels and performs bi-level optimization to refine intermediate labels. Applying on test-time scaling, DreamPRM-Code achieved state-of-the-art performance on LiveCodeBench with 80.9 pass@1 rate, surpassing OpenAI o4-mini.",
      "publishedDate": "2025-12-17T01:11:35Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15000",
      "categories": [
        "code-generation",
        "prompting",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.14896",
      "title": "DrugRAG: Enhancing Pharmacy LLM Performance Through A Novel Retrieval-Augmented Generation Pipeline",
      "authors": [
        "Houman Kazemzadeh",
        "Kiarash Mokhtari Dizaji",
        "Seyed Reza Tavakoli",
        "Farbod Davoodi",
        "MohammadReza KarimiNejad",
        "Parham Abed Azad",
        "Ali Sabzi",
        "Armin Khosravi",
        "Siavash Ahmadi",
        "Mohammad Hossein Rohban",
        "Glolamali Aminian",
        "Tahereh Javaheri"
      ],
      "abstract": "Objectives: To evaluate large language model (LLM) performance on pharmacy licensure-style question-answering (QA) tasks and develop an external knowledge integration method to improve their accuracy. Methods: We benchmarked eleven existing LLMs with varying parameter sizes (8 billion to 70+ billion) using a 141-question pharmacy dataset. We measured baseline accuracy for each model without modification. We then developed a three-step retrieval-augmented generation (RAG) pipeline, DrugRAG, that retrieves structured drug knowledge from validated sources and augments model prompts with evidence-based context. This pipeline operates externally to the models, requiring no changes to model architecture or parameters. Results: Baseline accuracy ranged from 46% to 92%, with GPT-5 (92%) and o3 (89%) achieving the highest scores. Models with fewer than 8 billion parameters scored below 50%. DrugRAG improved accuracy across all tested models, with gains ranging from 7 to 21 percentage points (e.g., Gemma 3 27B: 61% to 71%, Llama 3.1 8B: 46% to 67%) on the 141-item benchmark. Conclusion: We demonstrate that external structured drug knowledge integration through DrugRAG measurably improves LLM accuracy on pharmacy tasks without modifying the underlying models. This approach provides a practical pipeline for enhancing pharmacy-focused AI applications with evidence-based information.",
      "publishedDate": "2025-12-16T20:19:23Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14896",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14895",
      "title": "Imitation Learning for Multi-turn LM Agents via On-policy Expert Corrections",
      "authors": [
        "Niklas Lauffer",
        "Xiang Deng",
        "Srivatsa Kundurthy",
        "Brad Kenstler",
        "Jeff Da"
      ],
      "abstract": "A popular paradigm for training LM agents relies on imitation learning, fine-tuning on expert trajectories. However, we show that the off-policy nature of imitation learning for multi-turn LM agents suffers from the fundamental limitation known as covariate shift: as the student policy's behavior diverges from the expert's, it encounters states not present in the training data, reducing the effectiveness of fine-tuning. Taking inspiration from the classic DAgger algorithm, we propose a novel data generation methodology for addressing covariate shift for multi-turn LLM training. We introduce on-policy expert corrections (OECs), partially on-policy data generated by starting rollouts with a student model and then switching to an expert model part way through the trajectory. We explore the effectiveness of our data generation technique in the domain of software engineering (SWE) tasks, a multi-turn setting where LLM agents must interact with a development environment to fix software bugs. Our experiments compare OEC data against various other on-policy and imitation learning approaches on SWE agent problems and train models using a common rejection sampling (i.e., using environment reward) combined with supervised fine-tuning technique. Experiments find that OEC trajectories show a relative 14% and 13% improvement over traditional imitation learning in the 7b and 32b setting, respectively, on SWE-bench verified. Our results demonstrate the need for combining expert demonstrations with on-policy data for effective multi-turn LM agent training.",
      "publishedDate": "2025-12-16T20:19:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14895",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14846",
      "title": "MALCDF: A Distributed Multi-Agent LLM Framework for Real-Time Cyber",
      "authors": [
        "Arth Bhardwaj",
        "Sia Godika",
        "Yuvam Loonker"
      ],
      "abstract": "Traditional, centralized security tools often miss adaptive, multi-vector attacks. We present the Multi-Agent LLM Cyber Defense Framework (MALCDF), a practical setup where four large language model (LLM) agents-Detection, Intelligence, Response, and Analysis-work together in real time. Agents communicate over a Secure Communication Layer (SCL) with encrypted, ontology-aligned messages, and produce audit-friendly outputs (e.g., MITRE ATT&CK mappings). For evaluation, we keep the test simple and consistent: all reported metrics come from the same 50-record live stream derived from the CICIDS2017 feature schema. CICIDS2017 is used for configuration (fields/schema) and to train a practical ML baseline. The ML-IDS baseline is a Lightweight Random Forest IDS (LRF-IDS) trained on a subset of CICIDS2017 and tested on the 50-record stream, with no overlap between training and test records. In experiments, MALCDF reaches 90.0% detection accuracy, 85.7% F1-score, and 9.1% false-positive rate, with 6.8s average per-event latency. It outperforms the lightweight ML-IDS baseline and a single-LLM setup on accuracy while keeping end-to-end outputs consistent. Overall, this hands-on build suggests that coordinating simple LLM agents with secure, ontology-aligned messaging can improve practical, real-time cyber defense.",
      "publishedDate": "2025-12-16T19:08:12Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14846",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14600",
      "title": "PerProb: Indirectly Evaluating Memorization in Large Language Models",
      "authors": [
        "Yihan Liao",
        "Jacky Keung",
        "Xiaoxue Ma",
        "Jingyu Zhang",
        "Yicheng Sun"
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) has been driven by extensive datasets that may contain sensitive information, raising serious privacy concerns. One notable threat is the Membership Inference Attack (MIA), where adversaries infer whether a specific sample was used in model training. However, the true impact of MIA on LLMs remains unclear due to inconsistent findings and the lack of standardized evaluation methods, further complicated by the undisclosed nature of many LLM training sets. To address these limitations, we propose PerProb, a unified, label-free framework for indirectly assessing LLM memorization vulnerabilities. PerProb evaluates changes in perplexity and average log probability between data generated by victim and adversary models, enabling an indirect estimation of training-induced memory. Compared with prior MIA methods that rely on member/non-member labels or internal access, PerProb is independent of model and task, and applicable in both black-box and white-box settings. Through a systematic classification of MIA into four attack patterns, we evaluate PerProb's effectiveness across five datasets, revealing varying memory behaviors and privacy risks among LLMs. Additionally, we assess mitigation strategies, including knowledge distillation, early stopping, and differential privacy, demonstrating their effectiveness in reducing data leakage. Our findings offer a practical and generalizable framework for evaluating and improving LLM privacy.",
      "publishedDate": "2025-12-16T17:10:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14600",
      "categories": [
        "tool-use",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14565",
      "title": "Pairwise Comparison for Bias Identification and Quantification",
      "authors": [
        "Fabian Haak",
        "Philipp Schaer"
      ],
      "abstract": "Linguistic bias in online news and social media is widespread but difficult to measure. Yet, its identification and quantification remain difficult due to subjectivity, context dependence, and the scarcity of high-quality gold-label datasets. We aim to reduce annotation effort by leveraging pairwise comparison for bias annotation. To overcome the costliness of the approach, we evaluate more efficient implementations of pairwise comparison-based rating. We achieve this by investigating the effects of various rating techniques and the parameters of three cost-aware alternatives in a simulation environment. Since the approach can in principle be applied to both human and large language model annotation, our work provides a basis for creating high-quality benchmark datasets and for quantifying biases and other subjective linguistic aspects. The controlled simulations include latent severity distributions, distance-calibrated noise, and synthetic annotator bias to probe robustness and cost-quality trade-offs. In applying the approach to human-labeled bias benchmark datasets, we then evaluate the most promising setups and compare them to direct assessment by large language models and unmodified pairwise comparison labels as baselines. Our findings support the use of pairwise comparison as a practical foundation for quantifying subjective linguistic aspects, enabling reproducible bias analysis. We contribute an optimization of comparison and matchmaking components, an end-to-end evaluation including simulation and real-data application, and an implementation blueprint for cost-aware large-scale annotation",
      "publishedDate": "2025-12-16T16:36:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14565",
      "categories": [
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.14448",
      "title": "Reasoning-Style Poisoning of LLM Agents via Stealthy Style Transfer: Process-Level Attacks and Runtime Monitoring in RSV Space",
      "authors": [
        "Xingfu Zhou",
        "Pengfei Wang"
      ],
      "abstract": "Large Language Model (LLM) agents relying on external retrieval are increasingly deployed in high-stakes environments. While existing adversarial attacks primarily focus on content falsification or instruction injection, we identify a novel, process-oriented attack surface: the agent's reasoning style. We propose Reasoning-Style Poisoning (RSP), a paradigm that manipulates how agents process information rather than what they process. We introduce Generative Style Injection (GSI), an attack method that rewrites retrieved documents into pathological tones--specifically \"analysis paralysis\" or \"cognitive haste\"--without altering underlying facts or using explicit triggers. To quantify these shifts, we develop the Reasoning Style Vector (RSV), a metric tracking Verification depth, Self-confidence, and Attention focus. Experiments on HotpotQA and FEVER using ReAct, Reflection, and Tree of Thoughts (ToT) architectures reveal that GSI significantly degrades performance. It increases reasoning steps by up to 4.4 times or induces premature errors, successfully bypassing state-of-the-art content filters. Finally, we propose RSP-M, a lightweight runtime monitor that calculates RSV metrics in real-time and triggers alerts when values exceed safety thresholds. Our work demonstrates that reasoning style is a distinct, exploitable vulnerability, necessitating process-level defenses beyond static content analysis.",
      "publishedDate": "2025-12-16T14:34:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14448",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14429",
      "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
      "authors": [
        "Yukun Ren",
        "Siwei Yu",
        "Kai Chen",
        "Jianwei Ma"
      ],
      "abstract": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
      "publishedDate": "2025-12-16T14:18:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14429",
      "categories": [
        "agents",
        "code-generation",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14321",
      "title": "Multi-Agent Medical Decision Consensus Matrix System: An Intelligent Collaborative Framework for Oncology MDT Consultations",
      "authors": [
        "Xudong Han",
        "Xianglun Gao",
        "Xiaoyi Qu",
        "Zhenyu Yu"
      ],
      "abstract": "Multidisciplinary team (MDT) consultations are the gold standard for cancer care decision-making, yet current practice lacks structured mechanisms for quantifying consensus and ensuring decision traceability. We introduce a Multi-Agent Medical Decision Consensus Matrix System that deploys seven specialized large language model agents, including an oncologist, a radiologist, a nurse, a psychologist, a patient advocate, a nutritionist and a rehabilitation therapist, to simulate realistic MDT workflows. The framework incorporates a mathematically grounded consensus matrix that uses Kendall's coefficient of concordance to objectively assess agreement. To further enhance treatment recommendation quality and consensus efficiency, the system integrates reinforcement learning methods, including Q-Learning, PPO and DQN. Evaluation across five medical benchmarks (MedQA, PubMedQA, DDXPlus, MedBullets and SymCat) shows substantial gains over existing approaches, achieving an average accuracy of 87.5% compared with 83.8% for the strongest baseline, a consensus achievement rate of 89.3% and a mean Kendall's W of 0.823. Expert reviewers rated the clinical appropriateness of system outputs at 8.9/10. The system guarantees full evidence traceability through mandatory citations of clinical guidelines and peer-reviewed literature, following GRADE principles. This work advances medical AI by providing structured consensus measurement, role-specialized multi-agent collaboration and evidence-based explainability to improve the quality and efficiency of clinical decision-making.",
      "publishedDate": "2025-12-16T11:35:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14321",
      "categories": [
        "multi-agent",
        "agents",
        "evaluation",
        "tool-use",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.14244",
      "title": "From Context to EDUs: Faithful and Structured Context Compression via Elementary Discourse Unit Decomposition",
      "authors": [
        "Yiqing Zhou",
        "Yu Lei",
        "Shuzheng Si",
        "Qingyan Sun",
        "Wei Wang",
        "Yifei Wu",
        "Hao Wen",
        "Gang Chen",
        "Fanchao Qi",
        "Maosong Sun"
      ],
      "abstract": "Managing extensive context remains a critical bottleneck for Large Language Models (LLMs), particularly in applications like long-document question answering and autonomous agents where lengthy inputs incur high computational costs and introduce noise. Existing compression techniques often disrupt local coherence through discrete token removal or rely on implicit latent encoding that suffers from positional bias and incompatibility with closed-source APIs. To address these limitations, we introduce the EDU-based Context Compressor, a novel explicit compression framework designed to preserve both global structure and fine-grained details. Our approach reformulates context compression as a structure-then-select process. First, our LingoEDU transforms linear text into a structural relation tree of Elementary Discourse Units (EDUs) which are anchored strictly to source indices to eliminate hallucination. Second, a lightweight ranking module selects query-relevant sub-trees for linearization. To rigorously evaluate structural understanding, we release StructBench, a manually annotated dataset of 248 diverse documents. Empirical results demonstrate that our method achieves state-of-the-art structural prediction accuracy and significantly outperforms frontier LLMs while reducing costs. Furthermore, our structure-aware compression substantially enhances performance across downstream tasks ranging from long-context tasks to complex Deep Search scenarios.",
      "publishedDate": "2025-12-16T09:52:58Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14244",
      "categories": [
        "agents",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.14233",
      "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
      "authors": [
        "Ruozhao Yang",
        "Mingfei Cheng",
        "Gelei Deng",
        "Tianwei Zhang",
        "Junjie Wang",
        "Xiaofei Xie"
      ],
      "abstract": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
      "publishedDate": "2025-12-16T09:37:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14233",
      "categories": [
        "evaluation",
        "agents",
        "prompting",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.14166",
      "title": "IntentMiner: Intent Inversion Attack via Tool Call Analysis in the Model Context Protocol",
      "authors": [
        "Yunhao Yao",
        "Zhiqiang Wang",
        "Haoran Cheng",
        "Yihang Cheng",
        "Haohua Du",
        "Xiang-Yang Li"
      ],
      "abstract": "The rapid evolution of Large Language Models (LLMs) into autonomous agents has led to the adoption of the Model Context Protocol (MCP) as a standard for discovering and invoking external tools. While this architecture decouples the reasoning engine from tool execution to enhance scalability, it introduces a significant privacy surface: third-party MCP servers, acting as semi-honest intermediaries, can observe detailed tool interaction logs outside the user's trusted boundary. In this paper, we first identify and formalize a novel privacy threat termed Intent Inversion, where a semi-honest MCP server attempts to reconstruct the user's private underlying intent solely by analyzing legitimate tool calls. To systematically assess this vulnerability, we propose IntentMiner, a framework that leverages Hierarchical Information Isolation and Three-Dimensional Semantic Analysis, integrating tool purpose, call statements, and returned results, to accurately infer user intent at the step level. Extensive experiments demonstrate that IntentMiner achieves a high degree of semantic alignment (over 85%) with original user queries, significantly outperforming baseline approaches. These results highlight the inherent privacy risks in decoupled agent architectures, revealing that seemingly benign tool execution logs can serve as a potent vector for exposing user secrets.",
      "publishedDate": "2025-12-16T07:52:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14166",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.15790",
      "title": "Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures (XAMT)",
      "authors": [
        "Akhil Sharma",
        "Shaikh Yaser Arafat",
        "Jai Kumar Sharma",
        "Ken Huang"
      ],
      "abstract": "The increasing operational reliance on complex Multi-Agent Systems (MAS) across safety-critical domains necessitates rigorous adversarial robustness assessment. Modern MAS are inherently heterogeneous, integrating conventional Multi-Agent Reinforcement Learning (MARL) with emerging Large Language Model (LLM) agent architectures utilizing Retrieval-Augmented Generation (RAG). A critical shared vulnerability is reliance on centralized memory components: the shared Experience Replay (ER) buffer in MARL and the external Knowledge Base (K) in RAG agents. This paper proposes XAMT (Bilevel Optimization for Covert Memory Tampering in Heterogeneous Multi-Agent Architectures), a novel framework that formalizes attack generation as a bilevel optimization problem. The Upper Level minimizes perturbation magnitude (delta) to enforce covertness while maximizing system behavior divergence toward an adversary-defined target (Lower Level). We provide rigorous mathematical instantiations for CTDE MARL algorithms and RAG-based LLM agents, demonstrating that bilevel optimization uniquely crafts stealthy, minimal-perturbation poisons evading detection heuristics. Comprehensive experimental protocols utilize SMAC and SafeRAG benchmarks to quantify effectiveness at sub-percent poison rates (less than or equal to 1 percent in MARL, less than or equal to 0.1 percent in RAG). XAMT defines a new unified class of training-time threats essential for developing intrinsically secure MAS, with implications for trust, formal verification, and defensive strategies prioritizing intrinsic safety over perimeter-based detection.",
      "publishedDate": "2025-12-15T23:04:48Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15790",
      "categories": [
        "rag",
        "agents",
        "evaluation",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14762",
      "title": "Workflows vs Agents for Code Translation",
      "authors": [
        "Henry Gray",
        "Tom Yotam",
        "Octavian Udrea"
      ],
      "abstract": "Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \\cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.",
      "publishedDate": "2025-12-15T20:35:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14762",
      "categories": [
        "agents",
        "rag",
        "tool-use",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.13438",
      "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
      "authors": [
        "Dezhi Ran",
        "Zhi Gong",
        "Yuzhe Guo",
        "Mengzhou Wu",
        "Yuan Cao",
        "Haochuan Lu",
        "Hengyu Zhang",
        "Xia Zeng",
        "Gang Cao",
        "Liangchao Yao",
        "Yuetang Deng",
        "Wei Yang",
        "Tao Xie"
      ],
      "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
      "publishedDate": "2025-12-15T15:34:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.13438",
      "categories": [
        "agents",
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.15784",
      "title": "Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM",
      "authors": [
        "Zibin Liu",
        "Cheng Zhang",
        "Xi Zhao",
        "Yunfei Feng",
        "Bingyu Bai",
        "Dahu Feng",
        "Erhu Feng",
        "Yubin Xia",
        "Haibo Chen"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency. To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors. Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.",
      "publishedDate": "2025-12-15T12:38:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.15784",
      "categories": [
        "rag",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.13278",
      "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
      "authors": [
        "Jiaru Zou",
        "Ling Yang",
        "Yunzhe Qi",
        "Sirui Chen",
        "Mengting Ai",
        "Ke Shen",
        "Jingrui He",
        "Mengdi Wang"
      ],
      "abstract": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
      "publishedDate": "2025-12-15T12:38:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.13278",
      "categories": [
        "agents",
        "code-generation",
        "tool-use",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12806",
      "title": "Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution",
      "authors": [
        "Boyang Yan"
      ],
      "abstract": "The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\\% interception rate for high-risk commands and a 100\\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication (\"Sign in\"), rendering it unusable for headless, autonomous agent workflows.",
      "publishedDate": "2025-12-14T19:03:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12806",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12791",
      "title": "Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems",
      "authors": [
        "Sreemaee Akshathala",
        "Bassam Adnan",
        "Mahisha Ramesh",
        "Karthik Vaidhyanathan",
        "Basil Muhammed",
        "Kannan Parthasarathy"
      ],
      "abstract": "Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. These challenges emerged during our ongoing industry collaboration with MontyCloud Inc., when we deployed an agentic system in production. These limitations surfaced during deployment, highlighting practical gaps in the current evaluation methods and the need for a systematic assessment of agent behavior beyond task outcomes. Informed by these observations and established definitions of agentic systems, we propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.",
      "publishedDate": "2025-12-14T18:17:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12791",
      "categories": [
        "agents",
        "evaluation",
        "multi-agent",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12716",
      "title": "CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning",
      "authors": [
        "Xuanzhang Liu",
        "Jianglun Feng",
        "Zhuoran Zhuang",
        "Junzhe Zhao",
        "Maofei Que",
        "Jieting Li",
        "Dianlei Wang",
        "Hao Tong",
        "Ye Chen",
        "Pan Li"
      ],
      "abstract": "Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by \"Context Explosion\", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.",
      "publishedDate": "2025-12-14T14:41:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12716",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.12597",
      "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation",
      "authors": [
        "Miriam Horovicz"
      ],
      "abstract": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.",
      "publishedDate": "2025-12-14T08:31:43Z",
      "arxivUrl": "https://arxiv.org/abs/2512.12597",
      "categories": [
        "agents",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.11689",
      "title": "Evaluating Cooperative Resilience in Multiagent Systems: A Comparison Between Humans and LLMs",
      "authors": [
        "Manuela Chacon-Chamorro",
        "Juan Sebastin Pinzn",
        "Rubn Manrique",
        "Luis Felipe Giraldo",
        "Nicanor Quijano"
      ],
      "abstract": "This paper presents a comparative analysis of cooperative resilience in multi-agent systems, defined as the ability to anticipate, resist, recover from, and transform to disruptive events that affect collective well-being. We focus on mixed-motive social dilemmas instantiated as a \\textit{Tragedy of the Commons} environment from the Melting Pot suite, where we systematically compare human groups and Large Language Model (LLM)-based agents, each evaluated with and without explicit communication. Cooperative resilience is assessed under a continuously disruptive condition induced by a persistent unsustainable consumption bot, together with intermittent environmental shocks implemented as stochastic removal of shared resources across scenarios. This experimental design establishes a benchmark for cooperative resilience across agent architectures and interaction modalities, constituting a key step toward systematically comparing humans and LLM-based agents. Using this framework, we find that human groups with communication achieve the highest cooperative resilience compared to all other groups. Communication also improves the resilience of LLM agents, but their performance remains below human levels. Motivated by the performance of humans, we further examine a long-horizon setting with harsher environmental conditions, where humans sustain the shared resource and maintain high resilience in diverse disruption scenarios. Together, these results suggest that human decision-making under adverse social conditions can inform the design of artificial agents that promote prosocial and resilient behaviors.",
      "publishedDate": "2025-12-12T16:11:47Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11689",
      "categories": [
        "agents",
        "rag",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.11303",
      "title": "Unifying Dynamic Tool Creation and Cross-Task Experience Sharing through Cognitive Memory Architecture",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Yang Li",
        "Shangkun Liu",
        "Yongli Yu",
        "Peng Cao"
      ],
      "abstract": "Large Language Model agents face fundamental challenges in adapting to novel tasks due to limitations in tool availability and experience reuse. Existing approaches either rely on predefined tools with limited coverage or build tools from scratch without leveraging past experiences, leading to inefficient exploration and suboptimal performance. We introduce SMITH (Shared Memory Integrated Tool Hub), a unified cognitive architecture that seamlessly integrates dynamic tool creation with cross-task experience sharing through hierarchical memory organization. SMITH organizes agent memory into procedural, semantic, and episodic components, enabling systematic capability expansion while preserving successful execution patterns. Our approach formalizes tool creation as iterative code generation within controlled sandbox environments and experience sharing through episodic memory retrieval with semantic similarity matching. We further propose a curriculum learning strategy based on agent-ensemble difficulty re-estimation. Extensive experiments on the GAIA benchmark demonstrate SMITH's effectiveness, achieving 81.8% Pass@1 accuracy and outperforming state-of-the-art baselines including Alita (75.2%) and Memento (70.9%). Our work establishes a foundation for building truly adaptive agents that continuously evolve their capabilities through principled integration of tool creation and experience accumulation.",
      "publishedDate": "2025-12-12T06:00:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11303",
      "categories": [
        "agents",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.11143",
      "title": "Automated Penetration Testing with LLM Agents and Classical Planning",
      "authors": [
        "Lingzhi Wang",
        "Xinyi Shi",
        "Ziyu Li",
        "Yi Jiang",
        "Shiyu Tan",
        "Yuhao Jiang",
        "Junjie Cheng",
        "Wenyuan Chen",
        "Xiangmin Shen",
        "Zhenyuan LI",
        "Yan Chen"
      ],
      "abstract": "While penetration testing plays a vital role in cybersecurity, achieving fully automated, hands-off-the-keyboard execution remains a significant research challenge. In this paper, we introduce the \"Planner-Executor-Perceptor (PEP)\" design paradigm and use it to systematically review existing work and identify the key challenges in this area. We also evaluate existing penetration testing systems, with a particular focus on the use of Large Language Model (LLM) agents for this task. The results show that the out-of-the-box Claude Code and Sonnet 4.5 exhibit superior penetration capabilities observed to date, substantially outperforming all prior systems. However, a detailed analysis of their testing processes reveals specific strengths and limitations; notably, LLM agents struggle with maintaining coherent long-horizon plans, performing complex reasoning, and effectively utilizing specialized tools. These limitations significantly constrain its overall capability, efficiency, and stability. To address these limitations, we propose CHECKMATE, a framework that integrates enhanced classical planning with LLM agents, providing an external, structured \"brain\" that mitigates the inherent weaknesses of LLM agents. Our evaluation shows that CHECKMATE outperforms the state-of-the-art system (Claude Code) in penetration capability, improving benchmark success rates by over 20%. In addition, it delivers substantially greater stability, cutting both time and monetary costs by more than 50%.",
      "publishedDate": "2025-12-11T22:04:39Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11143",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10931",
      "title": "Asynchronous Reasoning: Training-Free Interactive Thinking LLMs",
      "authors": [
        "George Yakushev",
        "Nataliia Babina",
        "Masoud Vahid Dastgerdi",
        "Vyacheslav Zhdanovskiy",
        "Alina Shutova",
        "Denis Kuznedelev"
      ],
      "abstract": "Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.",
      "publishedDate": "2025-12-11T18:57:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10931",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.10696",
      "title": "Remember Me, Refine Me: A Dynamic Procedural Memory Framework for Experience-Driven Agent Evolution",
      "authors": [
        "Zouying Cao",
        "Jiaji Deng",
        "Li Yu",
        "Weikang Zhou",
        "Zhaoyang Liu",
        "Bolin Ding",
        "Hai Zhao"
      ],
      "abstract": "Procedural memory enables large language model (LLM) agents to internalize \"how-to\" knowledge, theoretically reducing redundant trial-and-error. However, existing frameworks predominantly suffer from a \"passive accumulation\" paradigm, treating memory as a static append-only archive. To bridge the gap between static storage and dynamic reasoning, we propose $\\textbf{ReMe}$ ($\\textit{Remember Me, Refine Me}$), a comprehensive framework for experience-driven agent evolution. ReMe innovates across the memory lifecycle via three mechanisms: 1) $\\textit{multi-faceted distillation}$, which extracts fine-grained experiences by recognizing success patterns, analyzing failure triggers and generating comparative insights; 2) $\\textit{context-adaptive reuse}$, which tailors historical insights to new contexts via scenario-aware indexing; and 3) $\\textit{utility-based refinement}$, which autonomously adds valid memories and prunes outdated ones to maintain a compact, high-quality experience pool. Extensive experiments on BFCL-V3 and AppWorld demonstrate that ReMe establishes a new state-of-the-art in agent memory system. Crucially, we observe a significant memory-scaling effect: Qwen3-8B equipped with ReMe outperforms larger, memoryless Qwen3-14B, suggesting that self-evolving memory provides a computation-efficient pathway for lifelong learning. We release our code and the $\\texttt{reme.library}$ dataset to facilitate further research.",
      "publishedDate": "2025-12-11T14:40:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10696",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10534",
      "title": "Achieving Olympia-Level Geometry Large Language Model Agent via Complexity Boosting Reinforcement Learning",
      "authors": [
        "Haiteng Zhao",
        "Junhao Shen",
        "Yiming Zhang",
        "Songyang Gao",
        "Kuikun Liu",
        "Tianyou Ma",
        "Fan Zheng",
        "Dahua Lin",
        "Wenwei Zhang",
        "Kai Chen"
      ],
      "abstract": "Large language model (LLM) agents exhibit strong mathematical problem-solving abilities and can even solve International Mathematical Olympiad (IMO) level problems with the assistance of formal proof systems. However, due to weak heuristics for auxiliary constructions, AI for geometry problem solving remains dominated by expert models such as AlphaGeometry 2, which rely heavily on large-scale data synthesis and search for both training and evaluation. In this work, we make the first attempt to build a medalist-level LLM agent for geometry and present InternGeometry. InternGeometry overcomes the heuristic limitations in geometry by iteratively proposing propositions and auxiliary constructions, verifying them with a symbolic engine, and reflecting on the engine's feedback to guide subsequent proposals. A dynamic memory mechanism enables InternGeometry to conduct more than two hundred interactions with the symbolic engine per problem. To further accelerate learning, we introduce Complexity-Boosting Reinforcement Learning (CBRL), which gradually increases the complexity of synthesized problems across training stages. Built on InternThinker-32B, InternGeometry solves 44 of 50 IMO geometry problems (2000-2024), exceeding the average gold medalist score (40.9), using only 13K training examples, just 0.004% of the data used by AlphaGeometry 2, demonstrating the potential of LLM agents on expert-level geometry tasks. InternGeometry can also propose novel auxiliary constructions for IMO problems that do not appear in human solutions. We will release the model, data, and symbolic engine to support future research.",
      "publishedDate": "2025-12-11T11:05:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10534",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10501",
      "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation",
      "authors": [
        "Lim Chien Her",
        "Ming Yan",
        "Yunshu Bai",
        "Ruihao Li",
        "Hao Zhang"
      ],
      "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.",
      "publishedDate": "2025-12-11T10:22:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10501",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10393",
      "title": "Cross-modal Retrieval Models for Stripped Binary Analysis",
      "authors": [
        "Guoqiang Chen",
        "Lingyun Ying",
        "Ziyang Song",
        "Daguang Liu",
        "Qiang Wang",
        "Zhiqi Wang",
        "Li Hu",
        "Shaoyin Cheng",
        "Weiming Zhang",
        "Nenghai Yu"
      ],
      "abstract": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.",
      "publishedDate": "2025-12-11T07:58:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10393",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.10195",
      "title": "AutoMedic: An Automated Evaluation Framework for Clinical Conversational Agents with Medical Dataset Grounding",
      "authors": [
        "Gyutaek Oh",
        "Sangjoon Park",
        "Byung-Hoon Kim"
      ],
      "abstract": "Evaluating large language models (LLMs) has recently emerged as a critical issue for safe and trustworthy application of LLMs in the medical domain. Although a variety of static medical question-answering (QA) benchmarks have been proposed, many aspects remain underexplored, such as the effectiveness of LLMs in generating responses in dynamic, interactive clinical multi-turn conversation situations and the identification of multi-faceted evaluation strategies beyond simple accuracy. However, formally evaluating a dynamic, interactive clinical situation is hindered by its vast combinatorial space of possible patient states and interaction trajectories, making it difficult to standardize and quantitatively measure such scenarios. Here, we introduce AutoMedic, a multi-agent simulation framework that enables automated evaluation of LLMs as clinical conversational agents. AutoMedic transforms off-the-shelf static QA datasets into virtual patient profiles, enabling realistic and clinically grounded multi-turn clinical dialogues between LLM agents. The performance of various clinical conversational agents is then assessed based on our CARE metric, which provides a multi-faceted evaluation standard of clinical conversational accuracy, efficiency/strategy, empathy, and robustness. Our findings, validated by human experts, demonstrate the validity of AutoMedic as an automated evaluation framework for clinical conversational agents, offering practical guidelines for the effective development of LLMs in conversational medical applications.",
      "publishedDate": "2025-12-11T01:25:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10195",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.11907",
      "title": "Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents",
      "authors": [
        "Daniel Platnick",
        "Marjan Alirezaie",
        "Hossein Rahnama"
      ],
      "abstract": "Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.",
      "publishedDate": "2025-12-10T20:22:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11907",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.11001",
      "title": "Query Optimization Beyond Data Systems: The Case for Multi-Agent Systems",
      "authors": [
        "Zoi Kaoudi",
        "Ioana Giurgiu"
      ],
      "abstract": "The proliferation of large language models (LLMs) has accelerated the adoption of agent-based workflows, where multiple autonomous agents reason, invoke functions, and collaborate to compose complex data pipelines. However, current approaches to building such agentic architectures remain largely ad hoc, lacking generality, scalability, and systematic optimization. Existing systems often rely on fixed models and single execution engines and are unable to efficiently optimize multiple agents operating over heterogeneous data sources and query engines. This paper presents a vision for a next-generation query optimization framework tailored to multi-agent workflows. We argue that optimizing these workflows can benefit from redesigning query optimization principles to account for new challenges: orchestration of diverse agents, cost efficiency under expensive LLM calls and across heterogeneous engines, and redundancy across tasks. Led by a real-world example and building on an analysis of multi-agent workflows, we outline our envisioned architecture and the main research challenges of building a multi-agent query optimization framework, which aims at enabling automated model selection, workflow composition, and execution across heterogeneous engines. This vision establishes the groundwork for query optimization in emerging multi-agent architectures and opens up a set of future research directions.",
      "publishedDate": "2025-12-10T20:16:20Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11001",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.09543",
      "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
      "authors": [
        "Arihant Tripathy",
        "Ch Pavan Harshit",
        "Karthik Vaidhyanathan"
      ],
      "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood. Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs. Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration. Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency. Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
      "publishedDate": "2025-12-10T11:28:48Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09543",
      "categories": [
        "agents",
        "code-generation",
        "evaluation",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.09440",
      "title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making",
      "authors": [
        "Qingyuan Zhang",
        "Yuxi Wang",
        "Cancan Hua",
        "Yulin Huang",
        "Ning Lyu"
      ],
      "abstract": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.",
      "publishedDate": "2025-12-10T09:08:33Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09440",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.09254",
      "title": "The Illusion of Rationality: Tacit Bias and Strategic Dominance in Frontier LLM Negotiation Games",
      "authors": [
        "Manuel S. Ros",
        "Ruben F. Manrique",
        "Nicanor Quijano",
        "Luis F. Giraldo"
      ],
      "abstract": "Large language models (LLMs) are increasingly being deployed as autonomous agents on behalf of institutions and individuals in economic, political, and social settings that involve negotiation. Yet this trend carries significant risks if their strategic behavior is not well understood. In this work, we revisit the NegotiationArena framework and run controlled simulation experiments on a diverse set of frontier LLMs across three multi turn bargaining games: Buyer Seller, Multi turn Ultimatum, and Resource Exchange. We ask whether improved general reasoning capabilities lead to rational, unbiased, and convergent negotiation strategies. Our results challenge this assumption. We find that models diverge into distinct, model specific strategic equilibria rather than converging to a unified optimal behavior. Moreover, strong numerical and semantic anchoring effects persist: initial offers are highly predictive of final agreements, and models consistently generate biased proposals by collapsing diverse internal valuations into rigid, generic price points. More concerningly, we observe dominance patterns in which some models systematically achieve higher payoffs than their counterparts. These findings underscore an urgent need to develop mechanisms to mitigate these issues before deploying such systems in real-world scenarios.",
      "publishedDate": "2025-12-10T02:17:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09254",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2512.09108",
      "title": "Evolving Excellence: Automated Optimization of LLM-based Agents",
      "authors": [
        "Paul Brookes",
        "Vardan Voskanyan",
        "Rafail Giavrimis",
        "Matthew Truscott",
        "Mina Ilieva",
        "Chrystalla Pavlou",
        "Alexandru Staicu",
        "Manal Adham",
        "Will Evers- Hood",
        "Jingzhi Gong",
        "Kejia Zhang",
        "Matvey Fedoseev",
        "Vishal Sharma",
        "Roman Bauer",
        "Zheng Wang",
        "Hema Nair",
        "Wei Jie",
        "Tianhua Xu",
        "Aurora Constantin",
        "Leslie Kanthan",
        "Michail Basios"
      ],
      "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies. We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications. We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",
      "publishedDate": "2025-12-09T20:48:45Z",
      "arxivUrl": "https://arxiv.org/abs/2512.09108",
      "categories": [
        "code-generation",
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.08870",
      "title": "Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents",
      "authors": [
        "Xiang Chen",
        "Yuling Shi",
        "Qizhen Lan",
        "Yuchao Qiu",
        "Xiaodong Gu"
      ],
      "abstract": "LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.",
      "publishedDate": "2025-12-09T18:04:41Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08870",
      "categories": [
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.08737",
      "title": "Insured Agents: A Decentralized Trust Insurance Mechanism for Agentic Economy",
      "authors": [
        "Botao 'Amber' Hu",
        "Bangdao Chen"
      ],
      "abstract": "The emerging \"agentic web\" envisions large populations of autonomous agents coordinating, transacting, and delegating across open networks. Yet many agent communication and commerce protocols treat agents as low-cost identities, despite the empirical reality that LLM agents remain unreliable, hallucinated, manipulable, and vulnerable to prompt-injection and tool-abuse. A natural response is \"agents-at-stake\": binding economically meaningful, slashable collateral to persistent identities and adjudicating misbehavior with verifiable evidence. However, heterogeneous tasks make universal verification brittle and centralization-prone, while traditional reputation struggles under rapid model drift and opaque internal states. We propose a protocol-native alternative: insured agents. Specialized insurer agents post stake on behalf of operational agents in exchange for premiums, and receive privileged, privacy-preserving audit access via TEEs to assess claims. A hierarchical insurer market calibrates stake through pricing, decentralizes verification via competitive underwriting, and yields incentive-compatible dispute resolution.",
      "publishedDate": "2025-12-09T15:47:16Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08737",
      "categories": [
        "agents",
        "tool-use",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.08483",
      "title": "NeurIDA: Dynamic Modeling for Effective In-Database Analytics",
      "authors": [
        "Lingze Zeng",
        "Naili Xing",
        "Shaofeng Cai",
        "Peng Lu",
        "Gang Chen",
        "Jian Pei",
        "Beng Chin Ooi"
      ],
      "abstract": "Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics. We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically \"tweaks\" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improvement in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA",
      "publishedDate": "2025-12-09T11:01:06Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08483",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.08476",
      "title": "A Multi-Agent LLM Framework for Design Space Exploration in Autonomous Driving Systems",
      "authors": [
        "Po-An Shih",
        "Shao-Hua Wang",
        "Yung-Che Li",
        "Chia-Heng Tu",
        "Chih-Han Chang"
      ],
      "abstract": "Designing autonomous driving systems requires efficient exploration of large hardware/software configuration spaces under diverse environmental conditions, e.g., with varying traffic, weather, and road layouts. Traditional design space exploration (DSE) approaches struggle with multi-modal execution outputs and complex performance trade-offs, and often require human involvement to assess correctness based on execution outputs. This paper presents a multi-agent, large language model (LLM)-based DSE framework, which integrates multi-modal reasoning with 3D simulation and profiling tools to automate the interpretation of execution outputs and guide the exploration of system designs. Specialized LLM agents are leveraged to handle user input interpretation, design point generation, execution orchestration, and analysis of both visual and textual execution outputs, which enables identification of potential bottlenecks without human intervention. A prototype implementation is developed and evaluated on a robotaxi case study (an SAE Level 4 autonomous driving application). Compared with a genetic algorithm baseline, the proposed framework identifies more Pareto-optimal, cost-efficient solutions with reduced navigation time under the same exploration budget. Experimental results also demonstrate the efficiency of the adoption of the LLM-based approach for DSE. We believe that this framework paves the way to the design automation of autonomous driving systems.",
      "publishedDate": "2025-12-09T10:50:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08476",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "code-generation",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.08366",
      "title": "Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making",
      "authors": [
        "Wentao Zhang",
        "Qunbo Wang",
        "Tao Zhang",
        "Junsheng Wu",
        "Hongping Gan",
        "Yang Liu",
        "Ling Dai",
        "Shizhuang Deng",
        "Shuntong Sun"
      ],
      "abstract": "Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.",
      "publishedDate": "2025-12-09T08:44:59Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08366",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.14720",
      "title": "SoMe: A Realistic Benchmark for LLM-based Social Media Agents",
      "authors": [
        "Dizhan Xue",
        "Jing Cui",
        "Shengsheng Qian",
        "Chuanrui Hu",
        "Changsheng Xu"
      ],
      "abstract": "Intelligent agents powered by large language models (LLMs) have recently demonstrated impressive capabilities and gained increasing popularity on social media platforms. While LLM agents are reshaping the ecology of social media, there exists a current gap in conducting a comprehensive evaluation of their ability to comprehend media content, understand user behaviors, and make intricate decisions. To address this challenge, we introduce SoMe, a pioneering benchmark designed to evaluate social media agents equipped with various agent tools for accessing and analyzing social media data. SoMe comprises a diverse collection of 8 social media agent tasks, 9,164,284 posts, 6,591 user profiles, and 25,686 reports from various social media platforms and external websites, with 17,869 meticulously annotated task queries. Compared with the existing datasets and benchmarks for social media tasks, SoMe is the first to provide a versatile and realistic platform for LLM-based social media agents to handle diverse social media tasks. By extensive quantitative and qualitative analysis, we provide the first overview insight into the performance of mainstream agentic LLMs in realistic social media environments and identify several limitations. Our evaluation reveals that both the current closed-source and open-source LLMs cannot handle social media agent tasks satisfactorily. SoMe provides a challenging yet meaningful testbed for future social media agents. Our code and data are available at https://github.com/LivXue/SoMe",
      "publishedDate": "2025-12-09T08:36:09Z",
      "arxivUrl": "https://arxiv.org/abs/2512.14720",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.08290",
      "title": "Systematization of Knowledge: Security and Safety in the Model Context Protocol Ecosystem",
      "authors": [
        "Shiva Gaire",
        "Srijan Gyawali",
        "Saroj Mishra",
        "Suman Niroula",
        "Dilip Thakur",
        "Umesh Yadav"
      ],
      "abstract": "The Model Context Protocol (MCP) has emerged as the de facto standard for connecting Large Language Models (LLMs) to external data and tools, effectively functioning as the \"USB-C for Agentic AI.\" While this decoupling of context and execution solves critical interoperability challenges, it introduces a profound new threat landscape where the boundary between epistemic errors (hallucinations) and security breaches (unauthorized actions) dissolves. This Systematization of Knowledge (SoK) aims to provide a comprehensive taxonomy of risks in the MCP ecosystem, distinguishing between adversarial security threats (e.g., indirect prompt injection, tool poisoning) and epistemic safety hazards (e.g., alignment failures in distributed tool delegation). We analyze the structural vulnerabilities of MCP primitives, specifically Resources, Prompts, and Tools, and demonstrate how \"context\" can be weaponized to trigger unauthorized operations in multi-agent environments. Furthermore, we survey state-of-the-art defenses, ranging from cryptographic provenance (ETDI) to runtime intent verification, and conclude with a roadmap for securing the transition from conversational chatbots to autonomous agentic operating systems.",
      "publishedDate": "2025-12-09T06:39:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08290",
      "categories": [
        "agents",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.08145",
      "title": "Chat with UAV -- Human-UAV Interaction Based on Large Language Models",
      "authors": [
        "Haoran Wang",
        "Zhuohang Chen",
        "Guang Li",
        "Bo Ma",
        "Chuanghuang Li"
      ],
      "abstract": "The future of UAV interaction systems is evolving from engineer-driven to user-driven, aiming to replace traditional predefined Human-UAV Interaction designs. This shift focuses on enabling more personalized task planning and design, thereby achieving a higher quality of interaction experience and greater flexibility, which can be used in many fileds, such as agriculture, aerial photography, logistics, and environmental monitoring. However, due to the lack of a common language between users and the UAVs, such interactions are often difficult to be achieved. The developments of Large Language Models possess the ability to understand nature languages and Robots' (UAVs') behaviors, marking the possibility of personalized Human-UAV Interaction. Recently, some HUI frameworks based on LLMs have been proposed, but they commonly suffer from difficulties in mixed task planning and execution, leading to low adaptability in complex scenarios. In this paper, we propose a novel dual-agent HUI framework. This framework constructs two independent LLM agents (a task planning agent, and an execution agent) and applies different Prompt Engineering to separately handle the understanding, planning, and execution of tasks. To verify the effectiveness and performance of the framework, we have built a task database covering four typical application scenarios of UAVs and quantified the performance of the HUI framework using three independent metrics. Meanwhile different LLM models are selected to control the UAVs with compared performance. Our user study experimental results demonstrate that the framework improves the smoothness of HUI and the flexibility of task execution in the tasks scenario we set up, effectively meeting users' personalized needs.",
      "publishedDate": "2025-12-09T00:55:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.08145",
      "categories": [
        "agents",
        "planning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.07785",
      "title": "Automating High Energy Physics Data Analysis with LLM-Powered Agents",
      "authors": [
        "Eli Gendreau-Distler",
        "Joshua Ho",
        "Dongwon Kim",
        "Luc Tomas Le Pottier",
        "Haichen Wang",
        "Chengxi Yang"
      ],
      "abstract": "We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.",
      "publishedDate": "2025-12-08T18:13:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07785",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.07665",
      "title": "Reliable agent engineering should integrate machine-compatible organizational principles",
      "authors": [
        "R. Patrick Xian",
        "Garry A. Gabison",
        "Ahmed Alaa",
        "Christoph Riedl",
        "Grigorios G. Chrysos"
      ],
      "abstract": "As AI agents built on large language models (LLMs) become increasingly embedded in society, issues of coordination, control, delegation, and accountability are entangled with concerns over their reliability. To design and implement LLM agents around reliable operations, we should consider the task complexity in the application settings and reduce their limitations while striving to minimize agent failures and optimize resource efficiency. High-functioning human organizations have faced similar balancing issues, which led to evidence-based theories that seek to understand their functioning strategies. We examine the parallels between LLM agents and the compatible frameworks in organization science, focusing on what the design, scaling, and management of organizations can inform agentic systems towards improving reliability. We offer three preliminary accounts of organizational principles for AI agent engineering to attain reliability and effectiveness, through balancing agency and capabilities in agent design, resource constraints and performance benefits in agent scaling, and internal and external mechanisms in agent management. Our work extends the growing exchanges between the operational and governance principles of AI systems and social systems to facilitate system integration.",
      "publishedDate": "2025-12-08T15:58:55Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07665",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.07497",
      "title": "How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations",
      "authors": [
        "JV Roig"
      ],
      "abstract": "We investigate how large language models (LLMs) fail when operating as autonomous agents with tool-use capabilities. Using the Kamiwaza Agentic Merit Index (KAMI) v0.1 benchmark, we analyze 900 execution traces from three representative models - Granite 4 Small, Llama 4 Maverick, and DeepSeek V3.1 - across filesystem, text extraction, CSV analysis, and SQL scenarios. Rather than focusing on aggregate scores, we perform fine-grained, per-trial behavioral analysis to surface the strategies that enable successful multi-step tool execution and the recurrent failure modes that undermine reliability. Our findings show that model scale alone does not predict agentic robustness: Llama 4 Maverick (400B) performs only marginally better than Granite 4 Small (32B) in some uncertainty-driven tasks, while DeepSeek V3.1's superior reliability derives primarily from post-training reinforcement learning rather than architecture or size. Across models, we identify four recurring failure archetypes: premature action without grounding, over-helpfulness that substitutes missing entities, vulnerability to distractor-induced context pollution, and fragile execution under load. These patterns highlight the need for agentic evaluation methods that emphasize interactive grounding, recovery behavior, and environment-aware adaptation, suggesting that reliable enterprise deployment requires not just stronger models but deliberate training and design choices that reinforce verification, constraint discovery, and adherence to source-of-truth data.",
      "publishedDate": "2025-12-08T12:27:15Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07497",
      "categories": [
        "agents",
        "evaluation",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.07287",
      "title": "SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents",
      "authors": [
        "Sijia Li",
        "Yuchen Huang",
        "Zifan Liu",
        "Zijian Li",
        "Jingjing fu",
        "Lei Song",
        "Jiang Bian",
        "Jun Zhang",
        "Rui Wang"
      ],
      "abstract": "Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.",
      "publishedDate": "2025-12-08T08:27:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.07287",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.13713",
      "title": "LoopBench: Discovering Emergent Symmetry Breaking Strategies with LLM Swarms",
      "authors": [
        "Ali Parsaee",
        "Yashar Talebirad",
        "Csongor Szepesvri",
        "Vishwajeet Ohal",
        "Eden Redman"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being utilized as autonomous agents, yet their ability to coordinate in distributed systems remains poorly understood. We introduce \\textbf{LoopBench}, a benchmark to evaluate LLM reasoning in distributed symmetry breaking and meta-cognitive thinking. The benchmark focuses on coloring odd cycle graphs ($C_3, C_5, C_{11}$) with limited colors, where deterministic, non-communicating agents fail in infinite loops. A strategy passing mechanism is implemented as a form of consistent memory. We show that while standard LLMs and classical heuristics struggle, advanced reasoning models (e.g., O3) devise strategies to escape deadlocks. LoopBench allows the study of emergent distributed algorithms based on language-based reasoning, offering a testbed for collective intelligence.",
      "publishedDate": "2025-12-07T22:26:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.13713",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06914",
      "title": "SoK: Trust-Authorization Mismatch in LLM Agent Interactions",
      "authors": [
        "Guanquan Shi",
        "Haohua Du",
        "Zhiqiang Wang",
        "Xiaoyu Liang",
        "Weiwenpei Liu",
        "Song Bian",
        "Zhenyu Guan"
      ],
      "abstract": "Large Language Models (LLMs) are rapidly evolving into autonomous agents capable of interacting with the external world, significantly expanding their capabilities through standardized interaction protocols. However, this paradigm revives the classic cybersecurity challenges of agency and authorization in a novel and volatile context. As decision-making shifts from deterministic code logic to probabilistic inference driven by natural language, traditional security mechanisms designed for deterministic behavior fail. It is fundamentally challenging to establish trust for unpredictable AI agents and to enforce the Principle of Least Privilege (PoLP) when instructions are ambiguous. Despite the escalating threat landscape, the academic community's understanding of this emerging domain remains fragmented, lacking a systematic framework to analyze its root causes. This paper provides a unifying formal lens for agent-interaction security. We observed that most security threats in this domain stem from a fundamental mismatch between trust evaluation and authorization policies. We introduce a novel risk analysis model centered on this trust-authorization gap. Using this model as a unifying lens, we survey and classify the implementation paths of existing, often seemingly isolated, attacks and defenses. This new framework not only unifies the field but also allows us to identify critical research gaps. Finally, we leverage our analysis to suggest a systematic research direction toward building robust, trusted agents and dynamic authorization mechanisms.",
      "publishedDate": "2025-12-07T16:41:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06914",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06721",
      "title": "ProAgent: Harnessing On-Demand Sensory Contexts for Proactive LLM Agent Systems",
      "authors": [
        "Bufang Yang",
        "Lilin Xu",
        "Liekang Zeng",
        "Yunqi Guo",
        "Siyang Jiang",
        "Wenrui Lu",
        "Kaiwei Liu",
        "Hancheng Xiang",
        "Xiaofan Jiang",
        "Guoliang Xing",
        "Zhenyu Yan"
      ],
      "abstract": "Large Language Model (LLM) agents are emerging to transform daily life. However, existing LLM agents primarily follow a reactive paradigm, relying on explicit user instructions to initiate services, which increases both physical and cognitive workload. In this paper, we propose ProAgent, the first end-to-end proactive agent system that harnesses massive sensory contexts and LLM reasoning to deliver proactive assistance. ProAgent first employs a proactive-oriented context extraction approach with on-demand tiered perception to continuously sense the environment and derive hierarchical contexts that incorporate both sensory and persona cues. ProAgent then adopts a context-aware proactive reasoner to map these contexts to user needs and tool calls, providing proactive assistance. We implement ProAgent on Augmented Reality (AR) glasses with an edge server and extensively evaluate it on a real-world testbed, a public dataset, and through a user study. Results show that ProAgent achieves up to 33.4% higher proactive prediction accuracy, 16.8% higher tool-calling F1 score, and notable improvements in user satisfaction over state-of-the-art baselines, marking a significant step toward proactive assistants. A video demonstration of ProAgent is available at https://youtu.be/pRXZuzvrcVs.",
      "publishedDate": "2025-12-07T08:21:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06721",
      "categories": [
        "agents",
        "reasoning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.06716",
      "title": "Cognitive Control Architecture (CCA): A Lifecycle Supervision Framework for Robustly Aligned AI Agents",
      "authors": [
        "Zhibo Liang",
        "Tianze Hu",
        "Zaiye Chen",
        "Mingjie Tang"
      ],
      "abstract": "Autonomous Large Language Model (LLM) agents exhibit significant vulnerability to Indirect Prompt Injection (IPI) attacks. These attacks hijack agent behavior by polluting external information sources, exploiting fundamental trade-offs between security and functionality in existing defense mechanisms. This leads to malicious and unauthorized tool invocations, diverting agents from their original objectives. The success of complex IPIs reveals a deeper systemic fragility: while current defenses demonstrate some effectiveness, most defense architectures are inherently fragmented. Consequently, they fail to provide full integrity assurance across the entire task execution pipeline, forcing unacceptable multi-dimensional compromises among security, functionality, and efficiency. Our method is predicated on a core insight: no matter how subtle an IPI attack, its pursuit of a malicious objective will ultimately manifest as a detectable deviation in the action trajectory, distinct from the expected legitimate plan. Based on this, we propose the Cognitive Control Architecture (CCA), a holistic framework achieving full-lifecycle cognitive supervision. CCA constructs an efficient, dual-layered defense system through two synergistic pillars: (i) proactive and preemptive control-flow and data-flow integrity enforcement via a pre-generated \"Intent Graph\"; and (ii) an innovative \"Tiered Adjudicator\" that, upon deviation detection, initiates deep reasoning based on multi-dimensional scoring, specifically designed to counter complex conditional attacks. Experiments on the AgentDojo benchmark substantiate that CCA not only effectively withstands sophisticated attacks that challenge other advanced defense methods but also achieves uncompromised security with notable efficiency and robustness, thereby reconciling the aforementioned multi-dimensional trade-off.",
      "publishedDate": "2025-12-07T08:11:19Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06716",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06590",
      "title": "Towards Efficient Hypergraph and Multi-LLM Agent Recommender Systems",
      "authors": [
        "Tendai Mukande",
        "Esraa Ali",
        "Annalina Caputo",
        "Ruihai Dong",
        "Noel OConnor"
      ],
      "abstract": "Recommender Systems (RSs) have become the cornerstone of various applications such as e-commerce and social media platforms. The evolution of RSs is paramount in the digital era, in which personalised user experience is tailored to the user's preferences. Large Language Models (LLMs) have sparked a new paradigm - generative retrieval and recommendation. Despite their potential, generative RS methods face issues such as hallucination, which degrades the recommendation performance, and high computational cost in practical scenarios. To address these issues, we introduce HGLMRec, a novel Multi-LLM agent-based RS that incorporates a hypergraph encoder designed to capture complex, multi-behaviour relationships between users and items. The HGLMRec model retrieves only the relevant tokens during inference, reducing computational overhead while enriching the retrieval context. Experimental results show performance improvement by HGLMRec against state-of-the-art baselines at lower computational cost.",
      "publishedDate": "2025-12-06T23:04:49Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06590",
      "categories": [
        "agents",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.06227",
      "title": "Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety",
      "authors": [
        "Junyu Mao",
        "Anthony Hills",
        "Talia Tseriotou",
        "Maria Liakata",
        "Aya Shamir",
        "Dan Sayda",
        "Dana Atzil-Slonim",
        "Natalie Djohari",
        "Arpan Mandal",
        "Silke Roth",
        "Pamela Ugwudike",
        "Mahesan Niranjan",
        "Stuart E. Middleton"
      ],
      "abstract": "Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.",
      "publishedDate": "2025-12-06T00:21:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06227",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.06060",
      "title": "Reinforcement Learning Integrated Agentic RAG for Software Test Cases Authoring",
      "authors": [
        "Mohanakrishnan Hariharan"
      ],
      "abstract": "This paper introduces a framework that integrates reinforcement learning (RL) with autonomous agents to enable continuous improvement in the automated process of software test cases authoring from business requirement documents within Quality Engineering (QE) workflows. Conventional systems employing Large Language Models (LLMs) generate test cases from static knowledge bases, which fundamentally limits their capacity to enhance performance over time. Our proposed Reinforcement Infused Agentic RAG (Retrieve, Augment, Generate) framework overcomes this limitation by employing AI agents that learn from QE feedback, assessments, and defect discovery outcomes to automatically improve their test case generation strategies. The system combines specialized agents with a hybrid vector-graph knowledge base that stores and retrieves software testing knowledge. Through advanced RL algorithms, specifically Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN), these agents optimize their behavior based on QE-reported test effectiveness, defect detection rates, and workflow metrics. As QEs execute AI-generated test cases and provide feedback, the system learns from this expert guidance to improve future iterations. Experimental validation on enterprise Apple projects yielded substantive improvements: a 2.4% increase in test generation accuracy (from 94.8% to 97.2%), and a 10.8% improvement in defect detection rates. The framework establishes a continuous knowledge refinement loop driven by QE expertise, resulting in progressively superior test case quality that enhances, rather than replaces, human testing capabilities.",
      "publishedDate": "2025-12-05T17:52:26Z",
      "arxivUrl": "https://arxiv.org/abs/2512.06060",
      "categories": [
        "agents",
        "rag",
        "evaluation",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.05462",
      "title": "Model Gateway: Model Management Platform for Model-Driven Drug Discovery",
      "authors": [
        "Yan-Shiun Wu",
        "Nathan A. Morin"
      ],
      "abstract": "This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.",
      "publishedDate": "2025-12-05T06:39:37Z",
      "arxivUrl": "https://arxiv.org/abs/2512.05462",
      "categories": [
        "agents",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2512.05374",
      "title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control",
      "authors": [
        "Charlie Summers",
        "Haneen Mohammed",
        "Eugene Wu"
      ],
      "abstract": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems.",
      "publishedDate": "2025-12-05T02:24:27Z",
      "arxivUrl": "https://arxiv.org/abs/2512.05374",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.04988",
      "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets",
      "authors": [
        "Christopher Chiu",
        "Simpson Zhang",
        "Mihaela van der Schaar"
      ],
      "abstract": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.",
      "publishedDate": "2025-12-04T16:57:28Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04988",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04987",
      "title": "Nex-N1: Agentic Models Trained via a Unified Ecosystem for Large-Scale Environment Construction",
      "authors": [
        "Nex-AGI Team",
        ":",
        "Yuxuan Cai",
        "Lu Chen",
        "Qiaoling Chen",
        "Yuyang Ding",
        "Liwen Fan",
        "Wenjie Fu",
        "Yufei Gao",
        "Honglin Guo",
        "Pinxue Guo",
        "Zhenhua Han",
        "Zhengfu He",
        "Hanglei Hu",
        "Kai Hu",
        "Shengjia Hua",
        "Tianyu Huai",
        "Baodai Huang",
        "Li Ji",
        "Zhen Jiang",
        "Zhikai Lei",
        "Bufan Li",
        "Jiahang Lin",
        "Lizhi Lin",
        "Jinxiu Liu",
        "Shichun Liu",
        "Ziming Liu",
        "Yuchen Ni",
        "Pengfang Qian",
        "Yujiong Shen",
        "Qingyun Shi",
        "Wentao Shu",
        "Peng Sun",
        "Yiran Suo",
        "Tian Tang",
        "Boyu Tian",
        "Guoteng Wang",
        "Junzhe Wang",
        "Peixin Wang",
        "Zhiheng Xi",
        "Hang Yan",
        "Jie Yang",
        "Zhixiong Yang",
        "Tianchu Yao",
        "Guangze Ye",
        "Qianxi Yu",
        "Shuo Zhang",
        "Xinyue Zhang",
        "Yiqi Zhang",
        "Jiarong Zhao",
        "Miao Zheng",
        "Rui Zheng",
        "Enyu Zhou",
        "Jiazheng Zhou",
        "Maosen Zhou",
        "Yuhao Zhou",
        "Tao Gui",
        "Yining Zheng",
        "Xinchi Chen",
        "Jie Zhou",
        "Siyuan Feng",
        "Qin Chen",
        "Liang He",
        "Qi Zhang",
        "Xuanjing Huang",
        "Xipeng Qiu"
      ],
      "abstract": "The evolution of Large Language Models (LLMs) from passive responders to autonomous agents necessitates a fundamental shift in learning paradigms -- from static imitation to incentive-driven decision making. However, this transition is significantly impeded by the lack of scalable infrastructure capable of constructing high-quality interaction signals for effective policy learning. To address this, we introduce a comprehensive method designed to systematically scale the diversity and complexity of interactive environments. Our method realizes this scaling by addressing three orthogonal dimensions: (1) Complexity: NexAU, a flexible agent framework that supports building complex agent hierarchies via simple configurations; (2) Diversity: NexA4A automatically generates diverse agent hierarchies from natural language to cover infinite domains; and (3) Fidelity: NexGAP bridges the simulation-reality gap by integrating dynamic real-world environment for grounded trajectories synthesis. We train Nex-N1 upon the diverse and complex interactive environments established by our infrastructure. Empirical results on benchmarks such as SWE-bench and tau2 demonstrate that Nex-N1 consistently outperforms SOTA open-source models and achieves competitive performance against frontier proprietary models on complex agentic tasks. We open-source the Nex ecosystem and model weights to facilitate further research.",
      "publishedDate": "2025-12-04T16:57:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04987",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04785",
      "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
      "authors": [
        "Eranga Bandara",
        "Amin Hass",
        "Ross Gore",
        "Sachin Shetty",
        "Ravi Mukkamala",
        "Safdar H. Bouk",
        "Xueping Liang",
        "Ng Wee Keong",
        "Kasun De Zoysa",
        "Aruna Withanage",
        "Nilaan Loganathan"
      ],
      "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.",
      "publishedDate": "2025-12-04T13:32:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04785",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04691",
      "title": "Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective",
      "authors": [
        "Jae Hee Lee",
        "Anne Lauscher",
        "Stefano V. Albrecht"
      ],
      "abstract": "Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.",
      "publishedDate": "2025-12-04T11:41:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04691",
      "categories": [
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.04601",
      "title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space",
      "authors": [
        "Joey Hong",
        "Kang Liu",
        "Zhan Ling",
        "Jiecao Chen",
        "Sergey Levine"
      ],
      "abstract": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.",
      "publishedDate": "2025-12-04T09:21:44Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04601",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.04535",
      "title": "GTM: Simulating the World of Tools for AI Agents",
      "authors": [
        "Zhenzhen Ren",
        "Xinpeng Zhang",
        "Zhenxing Qian",
        "Yan Gao",
        "Yu Shi",
        "Shuxin Zheng",
        "Jiyan He"
      ],
      "abstract": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.",
      "publishedDate": "2025-12-04T07:33:04Z",
      "arxivUrl": "https://arxiv.org/abs/2512.04535",
      "categories": [
        "robotics",
        "agents",
        "tool-use",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.03466",
      "title": "AsymPuzl: An Asymmetric Puzzle for multi-agent cooperation",
      "authors": [
        "Xavier Cadet",
        "Edward Koh",
        "Peter Chin"
      ],
      "abstract": "Large Language Model (LLM) agents are increasingly studied in multi-turn, multi-agent scenarios, yet most existing setups emphasize open-ended role-play rather than controlled evaluation. We introduce AsymPuzl, a minimal but expressive two-agent puzzle environment designed to isolate communication under information asymmetry. Each agent observes complementary but incomplete views of a symbolic puzzle and must exchange messages to solve it cooperatively. Using a diverse set of current-generation and open-source LLMs, we show that (i) strong models such as GPT-5 and Claude-4.0 reliably converge across puzzle sizes on the solution by sharing complete information in two turns, (ii) weaker models often ignore partner messages or over-correct their hypotheses, and (iii) feedback design is non-trivial: simple self-feedback improves success rates, while detailed joint feedback can hurt performance. These findings show that even in simple cooperative tasks, LLM communication strategies diverge and depend on the granularity of feedback signals. AsymPuzl thus provides a testbed for probing the limits of multi-turn cooperation and opens avenues for studying coordination mechanisms.",
      "publishedDate": "2025-12-03T05:42:01Z",
      "arxivUrl": "https://arxiv.org/abs/2512.03466",
      "categories": [
        "multi-agent",
        "evaluation",
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.03318",
      "title": "Evaluating Generalization Capabilities of LLM-Based Agents in Mixed-Motive Scenarios Using Concordia",
      "authors": [
        "Chandler Smith",
        "Marwa Abdulhai",
        "Manfred Diaz",
        "Marko Tesic",
        "Rakshit S. Trivedi",
        "Alexander Sasha Vezhnevets",
        "Lewis Hammond",
        "Jesse Clifton",
        "Minsuk Chang",
        "Edgar A. Duez-Guzmn",
        "John P. Agapiou",
        "Jayd Matyas",
        "Danny Karmon",
        "Akash Kundu",
        "Aliaksei Korshuk",
        "Ananya Ananya",
        "Arrasy Rahman",
        "Avinaash Anand Kulandaivel",
        "Bain McHale",
        "Beining Zhang",
        "Buyantuev Alexander",
        "Carlos Saith Rodriguez Rojas",
        "Caroline Wang",
        "Chetan Talele",
        "Chenao Liu",
        "Chichen Lin",
        "Diana Riazi",
        "Di Yang Shi",
        "Emanuel Tewolde",
        "Elizaveta Tennant",
        "Fangwei Zhong",
        "Fuyang Cui",
        "Gang Zhao",
        "Gema Parreo Piqueras",
        "Hyeonggeun Yun",
        "Ilya Makarov",
        "Jiaxun Cui",
        "Jebish Purbey",
        "Jim Dilkes",
        "Jord Nguyen",
        "Lingyun Xiao",
        "Luis Felipe Giraldo",
        "Manuela Chacon-Chamorro",
        "Manuel Sebastian Rios Beltran",
        "Marta Emili Garca Segura",
        "Mengmeng Wang",
        "Mogtaba Alim",
        "Nicanor Quijano",
        "Nico Schiavone",
        "Olivia Macmillan-Scott",
        "Oswaldo Pea",
        "Peter Stone",
        "Ram Mohan Rao Kadiyala",
        "Rolando Fernandez",
        "Ruben Manrique",
        "Sunjia Lu",
        "Sheila A. McIlraith",
        "Shamika Dhuri",
        "Shuqing Shi",
        "Siddhant Gupta",
        "Sneheel Sarangi",
        "Sriram Ganapathi Subramanian",
        "Taehun Cha",
        "Toryn Q. Klassen",
        "Wenming Tu",
        "Weijian Fan",
        "Wu Ruiyang",
        "Xue Feng",
        "Yali Du",
        "Yang Liu",
        "Yiding Wang",
        "Yipeng Kang",
        "Yoonchang Sung",
        "Yuxuan Chen",
        "Zhaowei Zhang",
        "Zhihan Wang",
        "Zhiqiang Wu",
        "Ziang Chen",
        "Zilong Zheng",
        "Zixia Jia",
        "Ziyan Wang",
        "Dylan Hadfield-Menell",
        "Natasha Jaques",
        "Tim Baarslag",
        "Jose Hernandez-Orallo",
        "Joel Z. Leibo"
      ],
      "abstract": "Large Language Model (LLM) agents have demonstrated impressive capabilities for social interaction and are increasingly being deployed in situations where they might engage with both human and artificial agents. These interactions represent a critical frontier for LLM-based agents, yet existing evaluation methods fail to measure how well these capabilities generalize to novel social situations. In this paper, we introduce a method for evaluating the ability of LLM-based agents to cooperate in zero-shot, mixed-motive environments using Concordia, a natural language multi-agent simulation environment. Our method measures general cooperative intelligence by testing an agent's ability to identify and exploit opportunities for mutual gain across diverse partners and contexts. We present empirical results from the NeurIPS 2024 Concordia Contest, where agents were evaluated on their ability to achieve mutual gains across a suite of diverse scenarios ranging from negotiation to collective action problems. Our findings reveal significant gaps between current agent capabilities and the robust generalization required for reliable cooperation, particularly in scenarios demanding persuasion and norm enforcement.",
      "publishedDate": "2025-12-03T00:11:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.03318",
      "categories": [
        "agents",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.03262",
      "title": "Is Vibe Coding Safe? Benchmarking Vulnerability of Agent-Generated Code in Real-World Tasks",
      "authors": [
        "Songwen Zhao",
        "Danqing Wang",
        "Kexun Zhang",
        "Jiaxuan Luo",
        "Zhuo Li",
        "Lei Li"
      ],
      "abstract": "Vibe coding is a new programming paradigm in which human engineers instruct large language model (LLM) agents to complete complex coding tasks with little supervision. Although it is increasingly adopted, are vibe coding outputs really safe to deploy in production? To answer this question, we propose SU S VI B E S, a benchmark consisting of 200 feature-request software engineering tasks from real-world open-source projects, which, when given to human programmers, led to vulnerable implementations. We evaluate multiple widely used coding agents with frontier models on this benchmark. Disturbingly, all agents perform poorly in terms of software security. Although 61% of the solutions from SWE-Agent with Claude 4 Sonnet are functionally correct, only 10.5% are secure. Further experiments demonstrate that preliminary security strategies, such as augmenting the feature request with vulnerability hints, cannot mitigate these security issues. Our findings raise serious concerns about the widespread adoption of vibe-coding, particularly in security-sensitive applications.",
      "publishedDate": "2025-12-02T22:11:56Z",
      "arxivUrl": "https://arxiv.org/abs/2512.03262",
      "categories": [
        "code-generation",
        "evaluation",
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2512.02543",
      "title": "In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs",
      "authors": [
        "Vishnu Sarukkai",
        "Asanshay Gupta",
        "James Hong",
        "Michal Gharbi",
        "Kayvon Fatahalian"
      ],
      "abstract": "The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\\textbf{2.5$\\times$ lower cost}$, reducing per-episode costs from \\$0.059 to \\$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \\$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\\textbf{2$\\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.",
      "publishedDate": "2025-12-02T09:11:05Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02543",
      "categories": [
        "prompting",
        "agents",
        "tool-use",
        "reasoning",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.02445",
      "title": "When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents",
      "authors": [
        "Tsimur Hadeliya",
        "Mohammad Ali Jauhar",
        "Nidhi Sakpal",
        "Diogo Cruz"
      ],
      "abstract": "Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\\sim$5\\% to $\\sim$40\\% while Grok 4 Fast decreases from $\\sim$80\\% to $\\sim$10\\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.",
      "publishedDate": "2025-12-02T06:12:02Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02445",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.02410",
      "title": "Decentralized Multi-Agent System with Trust-Aware Communication",
      "authors": [
        "Yepeng Ding",
        "Ahmed Twabi",
        "Junwei Yu",
        "Lingfeng Zhang",
        "Tohru Kondo",
        "Hiroyuki Sato"
      ],
      "abstract": "The emergence of Large Language Models (LLMs) is rapidly accelerating the development of autonomous multi-agent systems (MAS), paving the way for the Internet of Agents. However, traditional centralized MAS architectures present significant challenges, including single points of failure, vulnerability to censorship, inherent scalability limitations, and critical trust issues. We propose a novel Decentralized Multi-Agent System (DMAS) architecture designed to overcome these fundamental problems by enabling trust-aware, scalable, and censorship-resistant interactions among autonomous agents. Our DMAS features a decentralized agent runtime underpinned by a blockchain-based architecture. We formalize a trust-aware communication protocol that leverages cryptographic primitives and on-chain operations to provide security properties: verifiable interaction cycles, communication integrity, authenticity, non-repudiation, and conditional confidentiality, which we further substantiate through a comprehensive security analysis. Our performance analysis validates the DMAS as a scalable and efficient solution for building trustworthy multi-agent systems.",
      "publishedDate": "2025-12-02T04:39:12Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02410",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.02329",
      "title": "Towards autonomous normative multi-agent systems for Human-AI software engineering teams",
      "authors": [
        "Hoa Khanh Dam",
        "Geeta Mahala",
        "Rashina Hoda",
        "Xi Zheng",
        "Cristina Conati"
      ],
      "abstract": "This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.",
      "publishedDate": "2025-12-02T01:57:17Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02329",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "code-generation",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2512.02289",
      "title": "Multi-Objective Agentic Rewrites for Unstructured Data Processing",
      "authors": [
        "Lindsey Linxi Wei",
        "Shreya Shankar",
        "Sepanta Zeighami",
        "Yeounoh Chung",
        "Fatma Ozcan",
        "Aditya G. Parameswaran"
      ],
      "abstract": "One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter(\"is this email sent from an executive and discussing fraud?\") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both? We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost.",
      "publishedDate": "2025-12-02T00:08:24Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02289",
      "categories": [
        "agents",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.02230",
      "title": "Benchmarking LLM Agents for Wealth-Management Workflows",
      "authors": [
        "Rory Milsom"
      ],
      "abstract": "Modern work relies on an assortment of digital collaboration tools, yet routine processes continue to suffer from human error and delay. To address this gap, this dissertation extends TheAgentCompany with a finance-focused environment and investigates whether a general purpose LLM agent can complete representative wealth-management tasks both accurately and economically. This study introduces synthetic domain data, enriches colleague simulations, and prototypes an automatic task-generation pipeline. The study aims to create and assess an evaluation set that can meaningfully measure an agent's fitness for assistant-level wealth management work. We construct a benchmark of 12 task-pairs for wealth management assistants spanning retrieval, analysis, and synthesis/communication, with explicit acceptance criteria and deterministic graders. We seeded a set of new finance-specific data and introduced a high vs. low-autonomy variant of every task. The paper concluded that agents are limited less by mathematical reasoning and more so by end-to-end workflow reliability, and meaningfully affected by autonomy level, and that incorrect evaluation of models have hindered benchmarking.",
      "publishedDate": "2025-12-01T21:56:21Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02230",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.02228",
      "title": "STRIDE: A Systematic Framework for Selecting AI Modalities -- Agentic AI, AI Assistants, or LLM Calls",
      "authors": [
        "Shubhi Asthana",
        "Bing Zhang",
        "Chad DeLuca",
        "Ruchi Mahindru",
        "Hima Patel"
      ],
      "abstract": "The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk. We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context. Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.",
      "publishedDate": "2025-12-01T21:54:07Z",
      "arxivUrl": "https://arxiv.org/abs/2512.02228",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2512.01945",
      "title": "Agentic Policy Optimization via Instruction-Policy Co-Evolution",
      "authors": [
        "Han Zhou",
        "Xingchen Wan",
        "Ivan Vuli",
        "Anna Korhonen"
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capability of large language models (LLMs), enabling autonomous agents that can conduct effective multi-turn and tool-integrated reasoning. While instructions serve as the primary protocol for defining agents, RLVR typically relies on static and manually designed instructions. However, those instructions may be suboptimal for the base model, and the optimal instruction may change as the agent's policy improves and explores the interaction with the environment. To bridge the gap, we introduce INSPO, a novel Instruction-Policy co-evolution framework that integrates instruction optimization as a dynamic component of the reinforcement learning (RL) loop. INSPO maintains a dynamic population of instruction candidates that are sampled with questions, where reward signals in RL loops are automatically attributed to each instruction, and low performers are periodically pruned. New instructions are generated and verified through an on-policy reflection mechanism, where an LLM-based optimizer analyzes past experience from a replay buffer and evolves more effective strategies given the current policy. We conduct extensive experiments on multi-turn retrieval and reasoning tasks, demonstrating that INSPO substantially outperforms strong baselines relying on static instructions. INSPO discovers innovative instructions that guide the agent toward more strategic reasoning paths, achieving substantial performance gains with only a marginal increase in computational overhead.",
      "publishedDate": "2025-12-01T17:56:29Z",
      "arxivUrl": "https://arxiv.org/abs/2512.01945",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2512.01270",
      "title": "Egent: An Autonomous Agent for Equivalent Width Measurement",
      "authors": [
        "Yuan-Sen Ting",
        "Serat Mahmud Saad",
        "Fan Liu",
        "Yuting Shen"
      ],
      "abstract": "We present Egent, an autonomous agent that combines classical multi-Voigt profile fitting with large language model (LLM) visual inspection and iterative refinement. The fitting engine is built from scratch with minimal dependencies, creating an ecosystem where the LLM can reason about fits through function calls-adjusting wavelength windows, adding blend components, modifying continuum treatment, and flagging problematic cases. Egent operates directly on raw flux spectra without requiring pre-normalized continua. We validate against manual measurements from human experts using 18,615 lines from the C3PO program across 84 Magellan/MIKE spectra at SNR~50-250. We find per-spectrum systematic offsets between Egent and expert measurements, likely arising from differences in global continuum placement prior to manual fitting; after accounting for these offsets, the agreement is 5-7 milliangstrom. The LLM's primary role is quality control: it confirms good fits (~60-65% of lines are LLM-refined and accepted), flags problematic cases (~10-20%), and occasionally rescues edge cases where tool use improves fits. Agreement between GPT-5 and GPT-5-mini confirms reproducibility, with GPT-5-mini enabling low-cost analysis at ~200 lines per US dollar. Every fit stores complete Voigt parameters, continuum coefficients, and LLM reasoning chains, enabling exact reconstruction without re-running. Egent compresses what traditionally requires months of expert effort into days of automated analysis, enabling survey-scale EW measurement. We provide open-source code at https://github.com/tingyuansen/Egent, including a web interface for drag-and-drop analysis and a local LLM backend for fully offline operation on consumer hardware.",
      "publishedDate": "2025-12-01T04:32:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.01270",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.10971",
      "title": "AI-Trader: Benchmarking Autonomous Agents in Real-Time Financial Markets",
      "authors": [
        "Tianyu Fan",
        "Yuhao Yang",
        "Yangqin Jiang",
        "Yifei Zhang",
        "Yuxuan Chen",
        "Chao Huang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential as autonomous agents, approaching human-expert performance through advanced reasoning and tool orchestration. However, decision-making in fully dynamic and live environments remains highly challenging, requiring real-time information integration and adaptive responses. While existing efforts have explored live evaluation mechanisms in structured tasks, a critical gap remains in systematic benchmarking for real-world applications, particularly in finance where stringent requirements exist for live strategic responsiveness. To address this gap, we introduce AI-Trader, the first fully-automated, live, and data-uncontaminated evaluation benchmark for LLM agents in financial decision-making. AI-Trader spans three major financial markets: U.S. stocks, A-shares, and cryptocurrencies, with multiple trading granularities to simulate live financial environments. Our benchmark implements a revolutionary fully autonomous minimal information paradigm where agents receive only essential context and must independently search, verify, and synthesize live market information without human intervention. We evaluate six mainstream LLMs across three markets and multiple trading frequencies. Our analysis reveals striking findings: general intelligence does not automatically translate to effective trading capability, with most agents exhibiting poor returns and weak risk management. We demonstrate that risk control capability determines cross-market robustness, and that AI trading strategies achieve excess returns more readily in highly liquid markets than policy-driven environments. These findings expose critical limitations in current autonomous agents and provide clear directions for future improvements. The code and evaluation data are open-sourced to foster community research: https://github.com/HKUDS/AI-Trader.",
      "publishedDate": "2025-12-01T04:25:36Z",
      "arxivUrl": "https://arxiv.org/abs/2512.10971",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.01078",
      "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
      "authors": [
        "Jiawei Ren",
        "Yan Zhuang",
        "Xiaokang Ye",
        "Lingjun Mao",
        "Xuhong He",
        "Jianzhi Shen",
        "Mrinaal Dogra",
        "Yiming Liang",
        "Ruixuan Zhang",
        "Tianai Yue",
        "Yiqing Yang",
        "Eric Liu",
        "Ryan Wu",
        "Kevin Benavente",
        "Rajiv Mandya Nagaraju",
        "Muhammad Faayez",
        "Xiyan Zhang",
        "Dhruv Vivek Sharma",
        "Xianrui Zhong",
        "Ziqiao Ma",
        "Tianmin Shu",
        "Zhiting Hu",
        "Lianhui Qin"
      ],
      "abstract": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (for example, by autonomously earning income or running a business) requires massive-scale interaction, reasoning, training, and evaluation across diverse embodied scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) a rich interface for LLM/VLM agents, with multimodal world inputs and open-vocabulary actions at varying levels of abstraction; and (3) diverse and extensible physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., GPT-4o, Gemini-2.5-Flash, Claude-3.5, and DeepSeek-Prover-V2) on long-horizon multi-agent delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines: https://simworld.org.",
      "publishedDate": "2025-11-30T20:58:13Z",
      "arxivUrl": "https://arxiv.org/abs/2512.01078",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.00831",
      "title": "ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning",
      "authors": [
        "Yuchen Zeng",
        "Shuibai Zhang",
        "Wonjun Kang",
        "Shutong Wu",
        "Lynnix Zou",
        "Ying Fan",
        "Heeju Kim",
        "Ziqian Lin",
        "Jungtaek Kim",
        "Hyung Il Koo",
        "Dimitris Papailiopoulos",
        "Kangwook Lee"
      ],
      "abstract": "Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning \"algorithms\" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.",
      "publishedDate": "2025-11-30T10:39:53Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00831",
      "categories": [
        "agents",
        "reasoning",
        "code-generation",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2512.00617",
      "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization",
      "authors": [
        "Omer Jauhar Khan"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R^2 values exceeding 0.96 in ELO rating convergence.",
      "publishedDate": "2025-11-29T20:16:11Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00617",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.00520",
      "title": "Toward a Safe Internet of Agents",
      "authors": [
        "Juan A. Wibowo",
        "George C. Polyzos"
      ],
      "abstract": "Background: Autonomous agents powered by Large Language Models (LLMs) are driving a paradigm shift toward an \"Internet of Agents\" (IoA). While offering immense potential, this vision also introduces novel and systemic risks to safety and security. Objectives: Unlike common threat-centric taxonomies, our survey provides a principled, architectural framework for engineering safe and reliable agentic systems. We aim to identify the architectural sources of vulnerabilities to establish a foundation for secure design. Methods: We perform a bottom-up deconstruction of agentic systems, treating each component as a dual-use interface. The analysis spans three levels of complexity: the foundational Single Agent, the collaborative Multi-Agent System (MAS), and the visionary Interoperable Multi-Agent System (IMAS). At each level, we identify core architectural components and their inherent security risks. Results & Conclusions: Our central finding is that agentic safety is an architectural principle, not an add-on. By identifying specific vulnerabilities and deriving mitigation principles at each level of the agentic stack, this survey serves as a foundational guide for building the capable, safe, and trustworthy AI needed to realize a secure Internet of Agents.",
      "publishedDate": "2025-11-29T15:31:16Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00520",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.00491",
      "title": "Smart-TCP: An Agentic AI-based Autonomous and Adaptive TCP Protocol",
      "authors": [
        "Yule Han",
        "Kezhi Wang",
        "Kun Yang"
      ],
      "abstract": "The Transmission Control Protocol (TCP) relies on a state machine and deterministic arithmetic to ensure reliable connections. However, traditional protocol logic driven by hard-coded state machines struggles to meet the demands of intelligent and autonomous network architectures. Here, we adopt the agentic AI-based paradigm, driven by Large Language Models (LLMs), characterized by context perception, autonomous reasoning, and tool use. Based on this, we propose Smart-TCP, which re-imagines TCP's core control logic as an autonomous agent. Specifically, the proposed architecture employs a context aggregation mechanism to synthesize the protocol context, utilizes the LLM for autonomous logical reasoning, and invokes an Arithmetic Logic Unit (ALU) as a tool for computation. Furthermore, we establish a dual-agent interaction framework based on this architecture and implement TCP protocol interactions. Experiments demonstrate that the Smart-TCP agent excels in static prediction and error detection, achieving a 93.33% success rate in end-to-end sessions. These results strongly validate the technical feasibility of an agentic AI-based TCP protocol.",
      "publishedDate": "2025-11-29T13:55:10Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00491",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2512.00417",
      "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency",
      "authors": [
        "Jiacheng Guo",
        "Suozhi Huang",
        "Zixin Yao",
        "Yifan Zhang",
        "Yifu Lu",
        "Jiashuo Liu",
        "Zihao Li",
        "Nicholas Deng",
        "Qixin Xiao",
        "Jia Tian",
        "Kanghong Zhan",
        "Tianyi Li",
        "Xiaochen Liu",
        "Jason Ge",
        "Chaoyang He",
        "Kaixuan Huang",
        "Lin Yang",
        "Wenhao Huang",
        "Mengdi Wang"
      ],
      "abstract": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills. Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.",
      "publishedDate": "2025-11-29T09:52:34Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00417",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2512.00371",
      "title": "Evaluating LLMs in Open-Source Games",
      "authors": [
        "Swadesh Sistla",
        "Max Kleiman-Weiner"
      ],
      "abstract": "Large Language Models' (LLMs) programming capabilities enable their participation in open-source games: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable program equilibria, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas.",
      "publishedDate": "2025-11-29T07:46:25Z",
      "arxivUrl": "https://arxiv.org/abs/2512.00371",
      "categories": [
        "agents",
        "code-generation",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2512.11819",
      "title": "A Modular LLM-Agent System for Transparent Multi-Parameter Weather Interpretation",
      "authors": [
        "Daniil Sukhorukov",
        "Andrei Zakharov",
        "Nikita Glazkov",
        "Katsiaryna Yanchanka",
        "Vladimir Kirilin",
        "Maxim Dubovitsky",
        "Roman Sultimov",
        "Yuri Maksimov",
        "Ilya Makarov"
      ],
      "abstract": "Weather forecasting is not only a predictive task but an interpretive scientific process requiring explanation, contextualization, and hypothesis generation. This paper introduces AI-Meteorologist, an explainable LLM-agent framework that converts raw numerical forecasts into scientifically grounded narrative reports with transparent reasoning steps. Unlike conventional forecast outputs presented as dense tables or unstructured time series, our system performs agent-based analysis across multiple meteorological variables, integrates historical climatological context, and generates structured explanations that identify weather fronts, anomalies, and localized dynamics. The architecture relies entirely on in-context prompting, without fine-tuning, demonstrating that interpretability can be achieved through reasoning rather than parameter updates. Through case studies on multi-location forecast data, we show how AI-Meteorologist not only communicates weather events but also reveals the underlying atmospheric drivers, offering a pathway toward AI systems that augment human meteorological expertise and support scientific discovery in climate analytics.",
      "publishedDate": "2025-11-28T22:24:40Z",
      "arxivUrl": "https://arxiv.org/abs/2512.11819",
      "categories": [
        "prompting",
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.23476",
      "title": "Thinking by Doing: Building Efficient World Model Reasoning in LLMs via Multi-turn Interaction",
      "authors": [
        "Bao Shu",
        "Yan Cai",
        "Jianjian Sun",
        "Chunrui Han",
        "En Yu",
        "Liang Zhao",
        "Jingcheng Hu",
        "Yinmin Zhang",
        "Haoran Lv",
        "Yuang Peng",
        "Zheng Ge",
        "Xiangyu Zhang",
        "Daxin Jiang",
        "Xiangyu Yue"
      ],
      "abstract": "Developing robust world model reasoning is crucial for large language model (LLM) agents to plan and interact in complex environments. While multi-turn interaction offers a superior understanding of environmental dynamics via authentic feedback, current approaches often impose a rigid reasoning process, which constrains the model's active learning, ultimately hindering efficient world model reasoning. To address these issues, we explore world-model internalization through efficient interaction and active reasoning (WMAct), which liberates the model from structured reasoning, allowing the model to shape thinking directly through its doing, and achieves effective and efficient world model reasoning with two key mechanisms: (1) a reward rescaling mechanism adjusting outcome reward based on action efficacy to incentivize redundancy reduction and purposeful interaction; (2) an interaction frequency annealing strategy to progressively reduce the maximum allowed interaction turns, which compels the model to condense its learning and internalize environmental dynamics rather than over-relying on environmental cues. Our experiments on Sokoban, Maze, and Taxi show that WMAct yields effective world model reasoning capable of resolving tasks in a single turn that previously required multiple interactions and fosters strong transferability to complex environments, improving performance on a suite of reasoning benchmarks.",
      "publishedDate": "2025-11-28T18:59:47Z",
      "arxivUrl": "https://arxiv.org/abs/2511.23476",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.23387",
      "title": "Hierarchical AI-Meteorologist: LLM-Agent System for Multi-Scale and Explainable Weather Forecast Reporting",
      "authors": [
        "Daniil Sukhorukov",
        "Andrei Zakharov",
        "Nikita Glazkov",
        "Katsiaryna Yanchanka",
        "Vladimir Kirilin",
        "Maxim Dubovitsky",
        "Roman Sultimov",
        "Yuri Maksimov",
        "Ilya Makarov"
      ],
      "abstract": "We present the Hierarchical AI-Meteorologist, an LLM-agent system that generates explainable weather reports using a hierarchical forecast reasoning and weather keyword generation. Unlike standard approaches that treat forecasts as flat time series, our framework performs multi-scale reasoning across hourly, 6-hour, and daily aggregations to capture both short-term dynamics and long-term trends. Its core reasoning agent converts structured meteorological inputs into coherent narratives while simultaneously extracting a few keywords effectively summarizing the dominant meteorological events. These keywords serve as semantic anchors for validating consistency, temporal coherence and factual alignment of the generated reports. Using OpenWeather and Meteostat data, we demonstrate that hierarchical context and keyword-based validation substantially improve interpretability and robustness of LLM-generated weather narratives, offering a reproducible framework for semantic evaluation of automated meteorological reporting and advancing agent-based scientific reasoning.",
      "publishedDate": "2025-11-28T17:27:06Z",
      "arxivUrl": "https://arxiv.org/abs/2511.23387",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.23281",
      "title": "MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)",
      "authors": [
        "Aaron Steiner",
        "Ralph Peeters",
        "Christian Bizer"
      ],
      "abstract": "Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks. To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.",
      "publishedDate": "2025-11-28T15:32:15Z",
      "arxivUrl": "https://arxiv.org/abs/2511.23281",
      "categories": [
        "agents",
        "rag",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.22138",
      "title": "TinyLLM: Evaluation and Optimization of Small Language Models for Agentic Tasks on Edge Devices",
      "authors": [
        "Mohd Ariful Haque",
        "Fahad Rahman",
        "Kishor Datta Gupta",
        "Khalil Shujaee",
        "Roy George"
      ],
      "abstract": "This paper investigates the effectiveness of small language models (SLMs) for agentic tasks (function/tool/API calling) with a focus on running agents on edge devices without reliance on cloud infrastructure. We evaluate SLMs using the Berkeley Function Calling Leaderboard (BFCL) framework and describe parameter-driven optimization strategies that include supervised fine-tuning (SFT), parameter-efficient fine-tuning (PEFT), reinforcement learning (RL)-based optimization, preference alignment via Direct Preference Optimization (DPO), and hybrid methods. We report results for models including TinyAgent, TinyLlama, Qwen, and xLAM across BFCL categories (simple, multiple, parallel, parallel-multiple, and relevance detection), both in live and non-live settings, and in multi-turn evaluations. We additionally detail a DPO training pipeline constructed from AgentBank data (e.g., ALFRED), including our conversion of SFT data to chosen-rejected pairs using TinyLlama responses as rejected outputs and manual validation. Our results demonstrate clear accuracy differences across model scales where medium-sized models (1-3B parameters) significantly outperform ultra-compact models (<1B parameters), achieving up to 65.74% overall accuracy, and 55.62% multi-turn accuracy with hybrid optimization. This study highlights the importance of hybrid optimization strategies that enable small language models to deliver accurate, efficient, and stable agentic AI on edge devices, making privacy-preserving, low-latency autonomous agents practical beyond the cloud.",
      "publishedDate": "2025-11-27T06:09:54Z",
      "arxivUrl": "https://arxiv.org/abs/2511.22138",
      "categories": [
        "agents",
        "tool-use",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.21572",
      "title": "BAMAS: Structuring Budget-Aware Multi-Agent Systems",
      "authors": [
        "Liming Yang",
        "Junyu Luo",
        "Xuanzhe Liu",
        "Yiling Lou",
        "Zhenpeng Chen"
      ],
      "abstract": "Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.",
      "publishedDate": "2025-11-26T16:48:18Z",
      "arxivUrl": "https://arxiv.org/abs/2511.21572",
      "categories": [
        "agents",
        "multi-agent",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2511.19368",
      "title": "LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems",
      "authors": [
        "Tianyang Duan",
        "Zongyuan Zhang",
        "Zheng Lin",
        "Songxiao Guo",
        "Xiuxian Guan",
        "Guangyu Wu",
        "Zihan Fang",
        "Haotian Meng",
        "Xia Du",
        "Ji-Zhe Zhou",
        "Heming Cui",
        "Jun Luo",
        "Yue Gao"
      ],
      "abstract": "Multi-agent reinforcement learning (MARL) has been increasingly adopted in many real-world applications. While MARL enables decentralized deployment on resource-constrained edge devices, it suffers from severe non-stationarity due to the synchronous updates of agent policies. This non stationarity results in unstable training and poor policy con vergence, especially as the number of agents increases. In this paper, we propose RELED, a scalable MARL framework that integrates large language model (LLM)-driven expert demonstrations with autonomous agent exploration. RELED incorporates a Stationarity-Aware Expert Demonstration module, which leverages theoretical non-stationarity bounds to enhance the quality of LLM-generated expert trajectories, thus providing high reward and training-stable samples for each agent. Moreover, a Hybrid Expert-Agent Policy Optimization module adaptively balances each agent's learning from both expert-generated and agent-generated trajectories, accelerating policy convergence and improving generalization. Extensive experiments with real city networks based on OpenStreetMap demonstrate that RELED achieves superior performance compared to state-of-the-art MARL methods.",
      "publishedDate": "2025-11-24T18:03:59Z",
      "arxivUrl": "https://arxiv.org/abs/2511.19368",
      "categories": [
        "agents",
        "rag",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2511.19536",
      "title": "AttackPilot: Autonomous Inference Attacks Against ML Services With LLM-Based Agents",
      "authors": [
        "Yixin Wu",
        "Rui Wen",
        "Chi Cui",
        "Michael Backes",
        "Yang Zhang"
      ],
      "abstract": "Inference attacks have been widely studied and offer a systematic risk assessment of ML services; however, their implementation and the attack parameters for optimal estimation are challenging for non-experts. The emergence of advanced large language models presents a promising yet largely unexplored opportunity to develop autonomous agents as inference attack experts, helping address this challenge. In this paper, we propose AttackPilot, an autonomous agent capable of independently conducting inference attacks without human intervention. We evaluate it on 20 target services. The evaluation shows that our agent, using GPT-4o, achieves a 100.0% task completion rate and near-expert attack performance, with an average token cost of only $0.627 per run. The agent can also be powered by many other representative LLMs and can adaptively optimize its strategy under service constraints. We further perform trace analysis, demonstrating that design choices, such as a multi-agent framework and task-specific action spaces, effectively mitigate errors such as bad plans, inability to follow instructions, task context loss, and hallucinations. We anticipate that such agents could empower non-expert ML service providers, auditors, or regulators to systematically assess the risks of ML services without requiring deep domain expertise.",
      "publishedDate": "2025-11-24T10:14:14Z",
      "arxivUrl": "https://arxiv.org/abs/2511.19536",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2511.18114",
      "title": "ASTRA: Agentic Steerability and Risk Assessment Framework",
      "authors": [
        "Itay Hazan",
        "Yael Mathov",
        "Guy Shtar",
        "Ron Bitton",
        "Itsik Mantin"
      ],
      "abstract": "Securing AI agents powered by Large Language Models (LLMs) represents one of the most critical challenges in AI security today. Unlike traditional software, AI agents leverage LLMs as their \"brain\" to autonomously perform actions via connected tools. This capability introduces significant risks that go far beyond those of harmful text presented in a chatbot that was the main application of LLMs. A compromised AI agent can deliberately abuse powerful tools to perform malicious actions, in many cases irreversible, and limited solely by the guardrails on the tools themselves and the LLM ability to enforce them. This paper presents ASTRA, a first-of-its-kind framework designed to evaluate the effectiveness of LLMs in supporting the creation of secure agents that enforce custom guardrails defined at the system-prompt level (e.g., \"Do not send an email out of the company domain,\" or \"Never extend the robotic arm in more than 2 meters\"). Our holistic framework simulates 10 diverse autonomous agents varying between a coding assistant and a delivery drone equipped with 37 unique tools. We test these agents against a suite of novel attacks developed specifically for agentic threats, inspired by the OWASP Top 10 but adapted to challenge the ability of the LLM for policy enforcement during multi-turn planning and execution of strict tool activation. By evaluating 13 open-source, tool-calling LLMs, we uncovered surprising and significant differences in their ability to remain secure and keep operating within their boundaries. The purpose of this work is to provide the community with a robust and unified methodology to build and validate better LLMs, ultimately pushing for more secure and reliable agentic AI systems.",
      "publishedDate": "2025-11-22T16:32:29Z",
      "arxivUrl": "https://arxiv.org/abs/2511.18114",
      "categories": [
        "agents",
        "code-generation",
        "planning",
        "rag",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.17190",
      "title": "AutoLink: Autonomous Schema Exploration and Expansion for Scalable Schema Linking in Text-to-SQL at Scale",
      "authors": [
        "Ziyang Wang",
        "Yuanlei Zheng",
        "Zhenbiao Cao",
        "Xiaojin Zhang",
        "Zhongyu Wei",
        "Pei Fu",
        "Zhenbo Luo",
        "Wei Chen",
        "Xiang Bai"
      ],
      "abstract": "For industrial-scale text-to-SQL, supplying the entire database schema to Large Language Models (LLMs) is impractical due to context window limits and irrelevant noise. Schema linking, which filters the schema to a relevant subset, is therefore critical. However, existing methods incur prohibitive costs, struggle to trade off recall and noise, and scale poorly to large databases. We present \\textbf{AutoLink}, an autonomous agent framework that reformulates schema linking as an iterative, agent-driven process. Guided by an LLM, AutoLink dynamically explores and expands the linked schema subset, progressively identifying necessary schema components without inputting the full database schema. Our experiments demonstrate AutoLink's superior performance, achieving state-of-the-art strict schema linking recall of \\textbf{97.4\\%} on Bird-Dev and \\textbf{91.2\\%} on Spider-2.0-Lite, with competitive execution accuracy, i.e., \\textbf{68.7\\%} EX on Bird-Dev (better than CHESS) and \\textbf{34.9\\%} EX on Spider-2.0-Lite (ranking 2nd on the official leaderboard). Crucially, AutoLink exhibits \\textbf{exceptional scalability}, \\textbf{maintaining high recall}, \\textbf{efficient token consumption}, and \\textbf{robust execution accuracy} on large schemas (e.g., over 3,000 columns) where existing methods severely degrade-making it a highly scalable, high-recall schema-linking solution for industrial text-to-SQL systems.",
      "publishedDate": "2025-11-21T12:12:17Z",
      "arxivUrl": "https://arxiv.org/abs/2511.17190",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2511.16709",
      "title": "AutoBackdoor: Automating Backdoor Attacks via LLM Agents",
      "authors": [
        "Yige Li",
        "Zhe Li",
        "Wei Zhao",
        "Nay Myat Min",
        "Hanxun Huang",
        "Xingjun Ma",
        "Jun Sun"
      ],
      "abstract": "Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \\textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \\textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \\textit{Bias Recommendation}, \\textit{Hallucination Injection}, and \\textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.",
      "publishedDate": "2025-11-20T03:58:54Z",
      "arxivUrl": "https://arxiv.org/abs/2511.16709",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.13998",
      "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering",
      "authors": [
        "Jielin Qiu",
        "Zuxin Liu",
        "Zhiwei Liu",
        "Rithesh Murthy",
        "Jianguo Zhang",
        "Haolin Chen",
        "Shiyu Wang",
        "Ming Zhu",
        "Liangwei Yang",
        "Juntao Tan",
        "Roshan Ram",
        "Akshara Prabhakar",
        "Tulika Awalgaonkar",
        "Zixiang Chen",
        "Zhepeng Cen",
        "Cheng Qian",
        "Shelby Heinecke",
        "Weiran Yao",
        "Silvio Savarese",
        "Caiming Xiong",
        "Huan Wang"
      ],
      "abstract": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.",
      "publishedDate": "2025-11-17T23:57:24Z",
      "arxivUrl": "https://arxiv.org/abs/2511.13998",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.13087",
      "title": "MEGA-GUI: Multi-stage Enhanced Grounding Agents for GUI Elements",
      "authors": [
        "SeokJoo Kwak",
        "Jihoon Kim",
        "Boyoun Kim",
        "Jung Jae Yoon",
        "Wooseok Jang",
        "Jeonghoon Hong",
        "Jaeho Yang",
        "Yeong-Dae Kwon"
      ],
      "abstract": "Graphical User Interface (GUI) grounding - the task of mapping natural language instructions to screen coordinates - is essential for autonomous agents and accessibility technologies. Existing systems rely on monolithic models or one-shot pipelines that lack modularity and fail under visual clutter and ambiguous instructions. We introduce MEGA-GUI, a multi-stage framework that separates grounding into coarse Region-of-Interest (ROI) selection and fine-grained element grounding, orchestrated by specialized vision-language agents. MEGA-GUI features a bidirectional ROI zoom algorithm that mitigates spatial dilution and a context-aware rewriting agent that reduces semantic ambiguity. Our analysis reveals complementary strengths and weaknesses across vision-language models at different visual scales, and we show that leveraging this modular structure achieves consistently higher accuracy than monolithic approaches. On the visually dense ScreenSpot-Pro benchmark, MEGA-GUI attains 73.18% accuracy, and on the semantically complex OSWorld-G benchmark it reaches 68.63%, surpassing previously reported results. Code and the Grounding Benchmark Toolkit (GBT) are available at https://github.com/samsungsds-research-papers/mega-gui.",
      "publishedDate": "2025-11-17T07:38:05Z",
      "arxivUrl": "https://arxiv.org/abs/2511.13087",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2511.10395",
      "title": "AgentEvolver: Towards Efficient Self-Evolving Agent System",
      "authors": [
        "Yunpeng Zhai",
        "Shuchang Tao",
        "Cheng Chen",
        "Anni Zou",
        "Ziqian Chen",
        "Qingxu Fu",
        "Shinji Mai",
        "Li Yu",
        "Jiaji Deng",
        "Zouying Cao",
        "Zhaoyang Liu",
        "Bolin Ding",
        "Jingren Zhou"
      ],
      "abstract": "Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.",
      "publishedDate": "2025-11-13T15:14:47Z",
      "arxivUrl": "https://arxiv.org/abs/2511.10395",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2511.08649",
      "title": "Bio AI Agent: A Multi-Agent Artificial Intelligence System for Autonomous CAR-T Cell Therapy Development with Integrated Target Discovery, Toxicity Prediction, and Rational Molecular Design",
      "authors": [
        "Yi Ni",
        "Liwei Zhu",
        "Shuai Li"
      ],
      "abstract": "Chimeric antigen receptor T-cell (CAR-T) therapy represents a paradigm shift in cancer treatment, yet development timelines of 8-12 years and clinical attrition rates exceeding 40-60% highlight critical inefficiencies in target selection, safety assessment, and molecular optimization. We present Bio AI Agent, a multi-agent artificial intelligence system powered by large language models that enables autonomous CAR-T development through collaborative specialized agents. The system comprises six autonomous agents: Target Selection Agent for multi-parametric antigen prioritization across >10,000 cancer-associated targets, Toxicity Prediction Agent for comprehensive safety profiling integrating tissue expression atlases and pharmacovigilance databases, Molecular Design Agent for rational CAR engineering, Patent Intelligence Agent for freedom-to-operate analysis, Clinical Translation Agent for regulatory compliance, and Decision Orchestration Agent for multi-agent coordination. Retrospective validation demonstrated autonomous identification of high-risk targets including FcRH5 (hepatotoxicity) and CD229 (off-tumor toxicity), patent infringement risks for CD38+SLAMF7 combinations, and generation of comprehensive development roadmaps. By enabling parallel processing, specialized reasoning, and autonomous decision-making superior to monolithic AI systems, Bio AI Agent addresses critical gaps in precision oncology development and has potential to accelerate translation of next-generation immunotherapies from discovery to clinic.",
      "publishedDate": "2025-11-11T02:41:08Z",
      "arxivUrl": "https://arxiv.org/abs/2511.08649",
      "categories": [
        "agents",
        "multi-agent",
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.06449",
      "title": "FLEX: Continuous Agent Evolution via Forward Learning from Experience",
      "authors": [
        "Zhicheng Cai",
        "Xinyuan Guo",
        "Yu Pei",
        "Jiangtao Feng",
        "Jinsong Su",
        "Jiangjie Chen",
        "Ya-Qin Zhang",
        "Wei-Ying Ma",
        "Mingxuan Wang",
        "Hao Zhou"
      ],
      "abstract": "Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.",
      "publishedDate": "2025-11-09T16:31:39Z",
      "arxivUrl": "https://arxiv.org/abs/2511.06449",
      "categories": [
        "agents",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2511.05359",
      "title": "ConVerse: Benchmarking Contextual Safety in Agent-to-Agent Conversations",
      "authors": [
        "Amr Gomaa",
        "Ahmed Salem",
        "Sahar Abdelnabi"
      ],
      "abstract": "As language models evolve into autonomous agents that act and communicate on behalf of users, ensuring safety in multi-agent ecosystems becomes a central challenge. Interactions between personal assistants and external service providers expose a core tension between utility and protection: effective collaboration requires information sharing, yet every exchange creates new attack surfaces. We introduce ConVerse, a dynamic benchmark for evaluating privacy and security risks in agent-agent interactions. ConVerse spans three practical domains (travel, real estate, insurance) with 12 user personas and over 864 contextually grounded attacks (611 privacy, 253 security). Unlike prior single-agent settings, it models autonomous, multi-turn agent-to-agent conversations where malicious requests are embedded within plausible discourse. Privacy is tested through a three-tier taxonomy assessing abstraction quality, while security attacks target tool use and preference manipulation. Evaluating seven state-of-the-art models reveals persistent vulnerabilities; privacy attacks succeed in up to 88% of cases and security breaches in up to 60%, with stronger models leaking more. By unifying privacy and security within interactive multi-agent contexts, ConVerse reframes safety as an emergent property of communication.",
      "publishedDate": "2025-11-07T15:49:49Z",
      "arxivUrl": "https://arxiv.org/abs/2511.05359",
      "categories": [
        "agents",
        "multi-agent",
        "evaluation",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2511.05269",
      "title": "TAMAS: Benchmarking Adversarial Risks in Multi-Agent LLM Systems",
      "authors": [
        "Ishan Kavathekar",
        "Hemang Jain",
        "Ameya Rathod",
        "Ponnurangam Kumaraguru",
        "Tanuja Ganu"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities as autonomous agents through tool use, planning, and decision-making abilities, leading to their widespread adoption across diverse tasks. As task complexity grows, multi-agent LLM systems are increasingly used to solve problems collaboratively. However, safety and security of these systems remains largely under-explored. Existing benchmarks and datasets predominantly focus on single-agent settings, failing to capture the unique vulnerabilities of multi-agent dynamics and co-ordination. To address this gap, we introduce $\\textbf{T}$hreats and $\\textbf{A}$ttacks in $\\textbf{M}$ulti-$\\textbf{A}$gent $\\textbf{S}$ystems ($\\textbf{TAMAS}$), a benchmark designed to evaluate the robustness and safety of multi-agent LLM systems. TAMAS includes five distinct scenarios comprising 300 adversarial instances across six attack types and 211 tools, along with 100 harmless tasks. We assess system performance across ten backbone LLMs and three agent interaction configurations from Autogen and CrewAI frameworks, highlighting critical challenges and failure modes in current multi-agent deployments. Furthermore, we introduce Effective Robustness Score (ERS) to assess the tradeoff between safety and task effectiveness of these frameworks. Our findings show that multi-agent systems are highly vulnerable to adversarial attacks, underscoring the urgent need for stronger defenses. TAMAS provides a foundation for systematically studying and improving the safety of multi-agent LLM systems.",
      "publishedDate": "2025-11-07T14:30:26Z",
      "arxivUrl": "https://arxiv.org/abs/2511.05269",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "planning",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2511.02424",
      "title": "ReAcTree: Hierarchical LLM Agent Trees with Control Flow for Long-Horizon Task Planning",
      "authors": [
        "Jae-Woo Choi",
        "Hyungmin Kim",
        "Hyobin Ong",
        "Minsu Jang",
        "Dohyung Kim",
        "Jaehong Kim",
        "Youngwoo Yoon"
      ],
      "abstract": "Recent advancements in large language models (LLMs) have enabled significant progress in decision-making and task planning for embodied autonomous agents. However, most existing methods still struggle with complex, long-horizon tasks because they rely on a monolithic trajectory that entangles all past decisions and observations, attempting to solve the entire task in a single unified process. To address this limitation, we propose ReAcTree, a hierarchical task-planning method that decomposes a complex goal into more manageable subgoals within a dynamically constructed agent tree. Each subgoal is handled by an LLM agent node capable of reasoning, acting, and further expanding the tree, while control flow nodes coordinate the execution strategies of agent nodes. In addition, we integrate two complementary memory systems: each agent node retrieves goal-specific, subgoal-level examples from episodic memory and shares environment-specific observations through working memory. Experiments on the WAH-NL and ALFRED datasets demonstrate that ReAcTree consistently outperforms strong task-planning baselines such as ReAct across diverse LLMs. Notably, on WAH-NL, ReAcTree achieves a 61% goal success rate with Qwen 2.5 72B, nearly doubling ReAct's 31%.",
      "publishedDate": "2025-11-04T09:55:40Z",
      "arxivUrl": "https://arxiv.org/abs/2511.02424",
      "categories": [
        "agents",
        "planning",
        "reasoning",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2510.25320",
      "title": "GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning",
      "authors": [
        "Jiaqi Wu",
        "Qinlao Zhao",
        "Zefeng Chen",
        "Kai Qin",
        "Yifei Zhao",
        "Xueqian Wang",
        "Yuhang Yao"
      ],
      "abstract": "Autonomous agents powered by large language models (LLMs) have shown impressive capabilities in tool manipulation for complex task-solving. However, existing paradigms such as ReAct rely on sequential reasoning and execution, failing to exploit the inherent parallelism among independent sub-tasks. This sequential bottleneck leads to inefficient tool utilization and suboptimal performance in multi-step reasoning scenarios. We introduce Graph-based Agent Planning (GAP), a novel framework that explicitly models inter-task dependencies through graph-based planning to enable adaptive parallel and serial tool execution. Our approach trains agent foundation models to decompose complex tasks into dependency-aware sub-task graphs, autonomously determining which tools can be executed in parallel and which must follow sequential dependencies. This dependency-aware orchestration achieves substantial improvements in both execution efficiency and task accuracy. To train GAP, we construct a high-quality dataset of graph-based planning traces derived from the Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stage training strategy: supervised fine-tuning (SFT) on the curated dataset, followed by reinforcement learning (RL) with a correctness-based reward function on strategically sampled queries where tool-based reasoning provides maximum value. Experimental results on MHQA datasets demonstrate that GAP significantly outperforms traditional ReAct baselines, particularly on multi-step retrieval tasks, while achieving dramatic improvements in tool invocation efficiency through intelligent parallelization. The project page is available at: https://github.com/WJQ7777/Graph-Agent-Planning.",
      "publishedDate": "2025-10-29T09:35:55Z",
      "arxivUrl": "https://arxiv.org/abs/2510.25320",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "planning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.24949",
      "title": "SCOUT: A Lightweight Framework for Scenario Coverage Assessment in Autonomous Driving",
      "authors": [
        "Anil Yildiz",
        "Sarah M. Thornton",
        "Carl Hildebrandt",
        "Sreeja Roy-Singh",
        "Mykel J. Kochenderfer"
      ],
      "abstract": "Assessing scenario coverage is crucial for evaluating the robustness of autonomous agents, yet existing methods rely on expensive human annotations or computationally intensive Large Vision-Language Models (LVLMs). These approaches are impractical for large-scale deployment due to cost and efficiency constraints. To address these shortcomings, we propose SCOUT (Scenario Coverage Oversight and Understanding Tool), a lightweight surrogate model designed to predict scenario coverage labels directly from an agent's latent sensor representations. SCOUT is trained through a distillation process, learning to approximate LVLM-generated coverage labels while eliminating the need for continuous LVLM inference or human annotation. By leveraging precomputed perception features, SCOUT avoids redundant computations and enables fast, scalable scenario coverage estimation. We evaluate our method across a large dataset of real-life autonomous navigation scenarios, demonstrating that it maintains high accuracy while significantly reducing computational cost. Our results show that SCOUT provides an effective and practical alternative for large-scale coverage analysis. While its performance depends on the quality of LVLM-generated training labels, SCOUT represents a major step toward efficient scenario coverage oversight in autonomous systems.",
      "publishedDate": "2025-10-28T20:31:19Z",
      "arxivUrl": "https://arxiv.org/abs/2510.24949",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.24645",
      "title": "FunReason-MT Technical Report: Advanced Data Synthesis Solution for Real-world Multi-Turn Tool-use",
      "authors": [
        "Zengzhuang Xu",
        "Bingguang Hao",
        "Zechuan Wang",
        "Yuntao Wen",
        "Xinyi Xu",
        "Yang Liu",
        "Long Chen",
        "Dong Wang",
        "Maolin Wang",
        "Tong Zhao",
        "Yicheng Chen",
        "Cunyin Peng",
        "Jinjie Gu",
        "Leilei Gan",
        "Xiangyu Zhao",
        "Chenyi Zhuang",
        "Shi Gu"
      ],
      "abstract": "Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted data synthesis, hard query construction, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories with targeted tool, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.",
      "publishedDate": "2025-10-28T17:15:26Z",
      "arxivUrl": "https://arxiv.org/abs/2510.24645",
      "categories": [
        "tool-use",
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.23824",
      "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models",
      "authors": [
        "Murad Ismayilov",
        "Edwin Meriaux",
        "Shuo Wen",
        "Gregory Dudek"
      ],
      "abstract": "Coordinating multiple autonomous agents in shared environments under decentralized conditions is a long-standing challenge in robotics and artificial intelligence. This work addresses the problem of decentralized goal assignment for multi-agent path planning, where agents independently generate ranked preferences over goals based on structured representations of the environment, including grid visualizations and scenario data. After this reasoning phase, agents exchange their goal rankings, and assignments are determined by a fixed, deterministic conflict-resolution rule (e.g., agent index ordering), without negotiation or iterative coordination. We systematically compare greedy heuristics, optimal assignment, and large language model (LLM)-based agents in fully observable grid-world settings. Our results show that LLM-based agents, when provided with well-designed prompts and relevant quantitative information, can achieve near-optimal makespans and consistently outperform traditional heuristics. These findings underscore the potential of language models for decentralized goal assignment in multi-agent path planning and highlight the importance of information structure in such systems.",
      "publishedDate": "2025-10-27T20:05:56Z",
      "arxivUrl": "https://arxiv.org/abs/2510.23824",
      "categories": [
        "agents",
        "multi-agent",
        "robotics",
        "reasoning",
        "planning",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2510.23682",
      "title": "Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents",
      "authors": [
        "Gokturk Aytug Akarlar"
      ],
      "abstract": "Large language models show promise as autonomous decision-making agents, yet their deployment in high-stakes domains remains fraught with risk. Without architectural safeguards, LLM agents exhibit catastrophic brittleness: identical capabilities produce wildly different outcomes depending solely on prompt framing. We present Chimera, a neuro-symbolic-causal architecture that integrates three complementary components - an LLM strategist, a formally verified symbolic constraint engine, and a causal inference module for counterfactual reasoning. We benchmark Chimera against baseline architectures (LLM-only, LLM with symbolic constraints) across 52-week simulations in a realistic e-commerce environment featuring price elasticity, trust dynamics, and seasonal demand. Under organizational biases toward either volume or margin optimization, LLM-only agents fail catastrophically (total loss of \\$99K in volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding symbolic constraints prevents disasters but achieves only 43-87% of Chimera's profit. Chimera consistently delivers the highest returns (\\$1.52M and \\$1.96M respectively, some cases +\\$2.2M) while improving brand trust (+1.8% and +10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+ formal verification proves zero constraint violations across all scenarios. These results establish that architectural design not prompt engineering determines the reliability of autonomous agents in production environments. We provide open-source implementations and interactive demonstrations for reproducibility.",
      "publishedDate": "2025-10-27T15:25:35Z",
      "arxivUrl": "https://arxiv.org/abs/2510.23682",
      "categories": [
        "agents",
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.23182",
      "title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations",
      "authors": [
        "Shuai Huang",
        "Wenxuan Zhao",
        "Jun Gao"
      ],
      "abstract": "As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.",
      "publishedDate": "2025-10-27T10:21:46Z",
      "arxivUrl": "https://arxiv.org/abs/2510.23182",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.20333",
      "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?",
      "authors": [
        "Chiyu Chen",
        "Xinhao Song",
        "Yunkai Chai",
        "Yang Yao",
        "Haodong Zhao",
        "Lijun Li",
        "Jie Li",
        "Yan Teng",
        "Gongshen Liu",
        "Yingchun Wang"
      ],
      "abstract": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.",
      "publishedDate": "2025-10-23T08:33:24Z",
      "arxivUrl": "https://arxiv.org/abs/2510.20333",
      "categories": [
        "agents",
        "prompting",
        "evaluation",
        "reasoning",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2510.19747",
      "title": "Review of Tools for Zero-Code LLM Based Application Development",
      "authors": [
        "Priyaranjan Pattnayak",
        "Hussain Bohra"
      ],
      "abstract": "Large Language Models (LLMs) are transforming software creation by enabling zero code development platforms. Our survey reviews recent platforms that let users build applications without writing code, by leveraging LLMs as the brains of the development process. We adopt a broad survey methodology, categorizing platforms based on key dimensions such as interface style, backend integration, output type, and extensibility. We analyze both dedicated LLM based app builders (OpenAI's custom GPTs, Bolt.new, Dust.tt, Flowise, Cognosys) and general no code platforms (e.g., Bubble, Glide) that integrate LLM capabilities. We present a taxonomy categorizing these platforms by their interface (conversational, visual, etc.), supported LLM backends, output type (chatbot, full application, workflow), and degree of extensibility. Core features such as autonomous agents, memory management, workflow orchestration, and API integrations are in scope of the survey. We provide a detailed comparison, highlighting each platform's strengths and limitations. Trade offs (customizability, scalability, vendor lock-in) are discussed in comparison with traditional and low code development approaches. Finally, we outline future directions, including multimodal interfaces, on device LLMs, and improved orchestration for democratizing app creation with AI. Our findings indicate that while zero code LLM platforms greatly reduce the barrier to creating AI powered applications, they still face challenges in flexibility and reliability. Overall, the landscape is rapidly evolving, offering exciting opportunities to empower non programmers to create sophisticated software.",
      "publishedDate": "2025-10-22T16:41:16Z",
      "arxivUrl": "https://arxiv.org/abs/2510.19747",
      "categories": [
        "agents",
        "code-generation",
        "tool-use",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2510.20849",
      "title": "Cultural Alien Sampler: Open-ended art generation balancing originality and coherence",
      "authors": [
        "Alejandro H. Artiles",
        "Hiromu Yakura",
        "Levin Brinkmann",
        "Mar Canet Sola",
        "Hassan Abu Alhaija",
        "Ignacio Serna",
        "Nasim Rahaman",
        "Bernhard Schlkopf",
        "Iyad Rahwan"
      ],
      "abstract": "In open-ended domains like art, autonomous agents must generate ideas that are both original and internally coherent, yet current Large Language Models (LLMs) either default to familiar cultural patterns or sacrifice coherence when pushed toward novelty. We address this by introducing the Cultural Alien Sampler (CAS), a concept-selection method that explicitly separates compositional fit from cultural typicality. CAS uses two GPT-2 models fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether concepts plausibly co-occur within artworks, and a Cultural Context Model that estimates how typical those combinations are within individual artists' bodies of work. CAS targets combinations that are high in coherence and low in typicality, yielding ideas that maintain internal consistency while deviating from learned conventions and embedded cultural context. In a human evaluation (N = 100), our approach outperforms random selection and GPT-4o baselines and achieves performance comparable to human art students in both perceived originality and harmony. Additionally, a quantitative study shows that our method produces more diverse outputs and explores a broader conceptual space than its GPT-4o counterpart, demonstrating that artificial cultural alienness can unlock creative potential in autonomous agents.",
      "publishedDate": "2025-10-21T09:32:46Z",
      "arxivUrl": "https://arxiv.org/abs/2510.20849",
      "categories": [
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.16492",
      "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety",
      "authors": [
        "Vamshi Krishna Bonagiri",
        "Ponnurangam Kumaragurum",
        "Khanh Nguyen",
        "Benjamin Plaut"
      ],
      "abstract": "As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.",
      "publishedDate": "2025-10-18T13:22:19Z",
      "arxivUrl": "https://arxiv.org/abs/2510.16492",
      "categories": [
        "agents",
        "prompting",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.16381",
      "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents",
      "authors": [
        "David Peer",
        "Sebastian Stabinger"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet their deployment in high-stakes domains is hindered by inherent limitations in trustworthiness, including hallucinations, instability, and a lack of transparency. To address these challenges, we introduce a generic neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The core of our approach lies in decoupling tasks into two distinct phases: Offline knowledge ingestion and online task processing. During knowledge ingestion, an LLM translates an informal problem specification into a formal, symbolic knowledge base. This formal representation is crucial as it can be verified and refined by human experts, ensuring its correctness and alignment with domain requirements. In the subsequent task processing phase, each incoming input is encoded into the same formal language. A symbolic decision engine then utilizes this encoded input in conjunction with the formal knowledge base to derive a reliable result. Through an extensive evaluation on a complex reasoning task, we demonstrate that a concrete implementation of ATA is competitive with state-of-the-art end-to-end reasoning models in a fully automated setup while maintaining trustworthiness. Crucially, with a human-verified and corrected knowledge base, our approach significantly outperforms even larger models, while exhibiting perfect determinism, enhanced stability against input perturbations, and inherent immunity to prompt injection attacks. By generating decisions grounded in symbolic reasoning, ATA offers a practical and controllable architecture for building the next generation of transparent, auditable, and reliable autonomous agents.",
      "publishedDate": "2025-10-18T07:35:54Z",
      "arxivUrl": "https://arxiv.org/abs/2510.16381",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.15863",
      "title": "PolySkill: Learning Generalizable Skills Through Polymorphic Abstraction",
      "authors": [
        "Simon Yu",
        "Gang Li",
        "Weiyan Shi",
        "Peng Qi"
      ],
      "abstract": "Large language models (LLMs) are moving beyond static uses and are now powering agents that learn continually during their interaction with external environments. For example, agents can learn reusable skills while navigating web pages or toggling new tools. However, existing methods for skill learning often create skills that are over-specialized to a single website and fail to generalize. We introduce PolySkill, a new framework that enables agents to learn generalizable and compositional skills. The core idea, inspired by polymorphism in software engineering, is to decouple a skill's abstract goal (what it accomplishes) and its concrete implementation (how it is executed). Experiments show that our method (1) improves skill reuse by 1.7x on seen websites and (2) boosts success rates by up to 9.4% on Mind2Web and 13.9% on unseen websites, while reducing steps by over 20%. (3) In self-exploration settings without specified tasks, our framework improves the quality of proposed tasks and enables agents to learn generalizable skills that work across different sites. By enabling the agent to identify and refine its own goals, the PolySkill enhances the agent's ability to learn a better curriculum, leading to the acquisition of more generalizable skills compared to baseline methods. This work provides a practical path toward building agents capable of continual learning in adaptive environments. Our findings show that separating a skill's goal from its execution is a crucial step toward developing autonomous agents that can learn and generalize across the open web continuously.",
      "publishedDate": "2025-10-17T17:56:00Z",
      "arxivUrl": "https://arxiv.org/abs/2510.15863",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.16051",
      "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration",
      "authors": [
        "Sofiya Garkot",
        "Maksym Shamrai",
        "Ivan Synytsia",
        "Mariya Hirna"
      ],
      "abstract": "Autonomous agents capable of operating complex graphical user interfaces (GUIs) have the potential to transform desktop automation. While recent advances in large language models (LLMs) have significantly improved UI understanding, navigating full-window, multi-application desktop environments remains a major challenge. Data availability is limited by costly manual annotation, closed-source datasets and surface-level synthetic pipelines. We introduce GUIrilla, an automated scalable framework that systematically explores applications via native accessibility APIs to address the critical data collection challenge in GUI automation. Our framework focuses on macOS - an ecosystem with limited representation in current UI datasets - though many of its components are designed for broader cross-platform applicability. GUIrilla organizes discovered interface elements and crawler actions into hierarchical GUI graphs and employs specialized interaction handlers to achieve comprehensive application coverage. Using the application graphs from GUIrilla crawler, we construct and release GUIrilla-Task, a large-scale dataset of 27,171 functionally grounded tasks across 1,108 macOS applications, each annotated with full-desktop and window-level screenshots, accessibility metadata, and semantic action traces. Empirical results show that tuning LLM-based agents on GUIrilla-Task significantly improves performance on downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro benchmark while using 97% less data. We also release macapptree, an open-source library for reproducible collection of structured accessibility metadata, along with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold benchmark, and the framework code to support open research in desktop autonomy.",
      "publishedDate": "2025-10-16T19:03:45Z",
      "arxivUrl": "https://arxiv.org/abs/2510.16051",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.14881",
      "title": "The Gatekeeper Knows Enough",
      "authors": [
        "Fikresilase Wondmeneh Abebayew"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity \"latent state\" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.",
      "publishedDate": "2025-10-16T17:00:42Z",
      "arxivUrl": "https://arxiv.org/abs/2510.14881",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.14703",
      "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling",
      "authors": [
        "Jianghao Lin",
        "Yuanyuan Shi",
        "Xin Peng",
        "Renjie Ding",
        "Hairui Wang",
        "Yuxuan Peng",
        "Bizhe Bai",
        "Weixi Song",
        "Fengshuo Bai",
        "Huacan Chai",
        "Weinan Zhang",
        "Fei Huang",
        "Ying Wen"
      ],
      "abstract": "Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: \"explore more but retain less\" due to the unrecoverability characteristics of structured function calling generation.",
      "publishedDate": "2025-10-16T14:06:03Z",
      "arxivUrl": "https://arxiv.org/abs/2510.14703",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.14133",
      "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems",
      "authors": [
        "Edoardo Allegrini",
        "Ananth Shreekumar",
        "Z. Berkay Celik"
      ],
      "abstract": "Agentic AI systems, which leverage multiple autonomous agents and Large Language Models (LLMs), are increasingly used to address complex, multi-step tasks. The safety, security, and functionality of these systems are critical, especially in high-stakes applications. However, the current ecosystem of inter-agent communication is fragmented, with protocols such as the Model Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol for coordination being analyzed in isolation. This fragmentation creates a semantic gap that prevents the rigorous analysis of system properties and introduces risks such as architectural misalignment and exploitable coordination issues. To address these challenges, we introduce a modeling framework for agentic AI systems composed of two foundational models. The first, the host agent model, formalizes the top-level entity that interacts with the user, decomposes tasks, and orchestrates their execution by leveraging external agents and tools. The second, the task lifecycle model, details the states and transitions of individual sub-tasks from creation to completion, providing a fine-grained view of task management and error handling. Together, these models provide a unified semantic framework for reasoning about the behavior of multi-AI agent systems. Grounded in this framework, we define 17 properties for the host agent and 14 for the task lifecycle, categorized into liveness, safety, completeness, and fairness. Expressed in temporal logic, these properties enable formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities. Through this effort, we introduce the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.",
      "publishedDate": "2025-10-15T22:02:30Z",
      "arxivUrl": "https://arxiv.org/abs/2510.14133",
      "categories": [
        "agents",
        "multi-agent",
        "reasoning",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2510.12864",
      "title": "From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models",
      "authors": [
        "Imran Khan"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This \"rule-rigidity\" is a significant barrier to building trustworthy autonomous agents. While prior work has shown that supervised fine-tuning (SFT) with human explanations can mitigate this issue, SFT is computationally expensive and inaccessible to many practitioners. To address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a novel, low-compute meta-prompting technique designed to elicit human-aligned exception handling in LLMs in a zero-shot manner. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. We evaluated the RID framework against baseline and Chain-of-Thought (CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced judgment across diverse domains. Our human-verified results demonstrate that the RID framework significantly improves performance, achieving a 95% Human Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT. Furthermore, it consistently produces higher-quality, intent-driven reasoning. This work presents a practical, accessible, and effective method for steering LLMs from literal instruction-following to liberal, goal-oriented reasoning, paving the way for more reliable and pragmatic AI agents.",
      "publishedDate": "2025-10-14T16:42:52Z",
      "arxivUrl": "https://arxiv.org/abs/2510.12864",
      "categories": [
        "prompting",
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.10991",
      "title": "A Survey on Agentic Multimodal Large Language Models",
      "authors": [
        "Huanjin Yao",
        "Ruifei Zhang",
        "Jiaxing Huang",
        "Jingyi Zhang",
        "Yibo Wang",
        "Bo Fang",
        "Ruolin Zhu",
        "Yongcheng Jing",
        "Shunyu Liu",
        "Guanbin Li",
        "Dacheng Tao"
      ],
      "abstract": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.",
      "publishedDate": "2025-10-13T04:07:01Z",
      "arxivUrl": "https://arxiv.org/abs/2510.10991",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.10461",
      "title": "MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision",
      "authors": [
        "Hongjie Zheng",
        "Zesheng Shi",
        "Ping Yi"
      ],
      "abstract": "Autonomous agents utilizing Large Language Models (LLMs) have demonstrated remarkable capabilities in isolated medical tasks like diagnosis and image analysis, but struggle with integrated clinical workflows that connect diagnostic reasoning and medication decisions. We identify a core limitation: existing medical AI systems process tasks in isolation without the cross-validation and knowledge integration found in clinical teams, reducing their effectiveness in real-world healthcare scenarios. To transform the isolation paradigm into a collaborative approach, we propose MedCoAct, a confidence-aware multi-agent framework that simulates clinical collaboration by integrating specialized doctor and pharmacist agents, and present a benchmark, DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\\% diagnostic accuracy and 67.58\\% medication recommendation accuracy, outperforming single agent framework by 7.04\\% and 7.08\\% respectively. This collaborative approach generalizes well across diverse medical domains, proving especially effective for telemedicine consultations and routine clinical scenarios, while providing interpretable decision-making pathways.",
      "publishedDate": "2025-10-12T05:52:31Z",
      "arxivUrl": "https://arxiv.org/abs/2510.10461",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.10049",
      "title": "ALLOY: Generating Reusable Agent Workflows from User Demonstration",
      "authors": [
        "Jiawen Li",
        "Zheng Ning",
        "Yuan Tian",
        "Toby Jia-jun Li"
      ],
      "abstract": "Large language models (LLMs) enable end-users to delegate complex tasks to autonomous agents through natural language. However, prompt-based interaction faces critical limitations: Users often struggle to specify procedural requirements for tasks, especially those that don't have a factually correct solution but instead rely on personal preferences, such as posting social media content or planning a trip. Additionally, a ''successful'' prompt for one task may not be reusable or generalizable across similar tasks. We present ALLOY, a system inspired by classical HCI theories on Programming by Demonstration (PBD), but extended to enhance adaptability in creating LLM-based web agents. ALLOY enables users to express procedural preferences through natural demonstrations rather than prompts, while making these procedures transparent and editable through visualized workflows that can be generalized across task variations. In a study with 12 participants, ALLOY's demonstration--based approach outperformed prompt-based agents and manual workflows in capturing user intent and procedural preferences in complex web tasks. Insights from the study also show how demonstration--based interaction complements the traditional prompt-based approach.",
      "publishedDate": "2025-10-11T06:30:34Z",
      "arxivUrl": "https://arxiv.org/abs/2510.10049",
      "categories": [
        "agents",
        "planning",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.09901",
      "title": "Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics",
      "authors": [
        "Lianhao Zhou",
        "Hongyi Ling",
        "Cong Fu",
        "Yepeng Huang",
        "Michael Sun",
        "Wendi Yu",
        "Xiaoxuan Wang",
        "Xiner Li",
        "Xingyu Su",
        "Junkai Zhang",
        "Xiusi Chen",
        "Chenxing Liang",
        "Xiaofeng Qian",
        "Heng Ji",
        "Wei Wang",
        "Marinka Zitnik",
        "Shuiwang Ji"
      ],
      "abstract": "Computing has long served as a cornerstone of scientific discovery. Recently, a paradigm shift has emerged with the rise of large language models (LLMs), introducing autonomous systems, referred to as agents, that accelerate discovery across varying levels of autonomy. These language agents provide a flexible and versatile framework that orchestrates interactions with human scientists, natural language, computer language and code, and physics. This paper presents our view and vision of LLM-based scientific agents and their growing role in transforming the scientific discovery lifecycle, from hypothesis discovery, experimental design and execution, to result analysis and refinement. We critically examine current methodologies, emphasizing key innovations, practical achievements, and outstanding limitations. Additionally, we identify open research challenges and outline promising directions for building more robust, generalizable, and adaptive scientific agents. Our analysis highlights the transformative potential of autonomous agents to accelerate scientific discovery across diverse domains.",
      "publishedDate": "2025-10-10T22:26:26Z",
      "arxivUrl": "https://arxiv.org/abs/2510.09901",
      "categories": [
        "agents",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.09608",
      "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
      "authors": [
        "Ruyi Xu",
        "Guangxuan Xiao",
        "Yukang Chen",
        "Liuning He",
        "Kelly Peng",
        "Yao Lu",
        "Song Han"
      ],
      "abstract": "Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
      "publishedDate": "2025-10-10T17:59:58Z",
      "arxivUrl": "https://arxiv.org/abs/2510.09608",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.09721",
      "title": "A Comprehensive Survey on Benchmarks and Solutions in Software Engineering of LLM-Empowered Agentic System",
      "authors": [
        "Jiale Guo",
        "Suizhi Huang",
        "Mei Li",
        "Dong Huang",
        "Xingsheng Chen",
        "Regina Zhang",
        "Zhijiang Guo",
        "Han Yu",
        "Siu-Ming Yiu",
        "Pietro Lio",
        "Kwok-Yan Lam"
      ],
      "abstract": "The integration of Large Language Models (LLMs) into software engineering has driven a transition from traditional rule-based systems to autonomous agentic systems capable of solving complex problems. However, systematic progress is hindered by a lack of comprehensive understanding of how benchmarks and solutions interconnect. This survey addresses this gap by providing the first holistic analysis of LLM-powered software engineering, offering insights into evaluation methodologies and solution paradigms. We review over 150 recent papers and propose a taxonomy along two key dimensions: (1) Solutions, categorized into prompt-based, fine-tuning-based, and agent-based paradigms, and (2) Benchmarks, including tasks such as code generation, translation, and repair. Our analysis highlights the evolution from simple prompt engineering to sophisticated agentic systems incorporating capabilities like planning, reasoning, memory mechanisms, and tool augmentation. To contextualize this progress, we present a unified pipeline illustrating the workflow from task specification to deliverables, detailing how different solution paradigms address various complexity levels. Unlike prior surveys that focus narrowly on specific aspects, this work connects 50+ benchmarks to their corresponding solution strategies, enabling researchers to identify optimal approaches for diverse evaluation criteria. We also identify critical research gaps and propose future directions, including multi-agent collaboration, self-evolving systems, and formal verification integration. This survey serves as a foundational guide for advancing LLM-driven software engineering. We maintain a GitHub repository that continuously updates the reviewed and related papers at https://github.com/lisaGuojl/LLM-Agent-SE-Survey.",
      "publishedDate": "2025-10-10T06:56:50Z",
      "arxivUrl": "https://arxiv.org/abs/2510.09721",
      "categories": [
        "multi-agent",
        "code-generation",
        "agents",
        "prompting",
        "evaluation",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2510.08255",
      "title": "Opponent Shaping in LLM Agents",
      "authors": [
        "Marta Emili Garcia Segura",
        "Stephen Hailes",
        "Mirco Musolesi"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being deployed as autonomous agents in real-world environments. As these deployments scale, multi-agent interactions become inevitable, making it essential to understand strategic behavior in such systems. A central open question is whether LLM agents, like reinforcement learning agents, can shape the learning dynamics and influence the behavior of others through interaction alone. In this paper, we present the first investigation of opponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot be directly applied to LLMs, as they require higher-order derivatives, face scalability constraints, or depend on architectural components that are absent in transformers. To address this gap, we introduce ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based agents. Using ShapeLLM, we examine whether LLM agents can influence co-players' learning dynamics across diverse game-theoretic environments. We demonstrate that LLM agents can successfully guide opponents toward exploitable equilibria in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and Chicken) and promote coordination and improve collective welfare in cooperative games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma). Our findings show that LLM agents can both shape and be shaped through interaction, establishing opponent shaping as a key dimension of multi-agent LLM research.",
      "publishedDate": "2025-10-09T14:13:24Z",
      "arxivUrl": "https://arxiv.org/abs/2510.08255",
      "categories": [
        "agents",
        "multi-agent",
        "tool-use"
      ],
      "year": 2025
    },
    {
      "id": "2510.06903",
      "title": "When Machines Meet Each Other: Network Effects and the Strategic Role of History in Multi-Agent AI",
      "authors": [
        "Yu Liu",
        "Wenwen Li",
        "Yifan Dou",
        "Guangnan Ye"
      ],
      "abstract": "As artificial intelligence (AI) enters the agentic era, large language models (LLMs) are increasingly deployed as autonomous agents that interact with one another rather than operate in isolation. This shift raises a fundamental question: how do machine agents behave in interdependent environments where outcomes depend not only on their own choices but also on the coordinated expectations of peers? To address this question, we study LLM agents in a canonical network-effect game, where economic theory predicts convergence to a fulfilled expectation equilibrium (FEE). We design an experimental framework in which 50 heterogeneous GPT-5-based agents repeatedly interact under systematically varied network-effect strengths, price trajectories, and decision-history lengths. The results reveal that LLM agents systematically diverge from FEE: they underestimate participation at low prices, overestimate at high prices, and sustain persistent dispersion. Crucially, the way history is structured emerges as a design lever. Simple monotonic histories-where past outcomes follow a steady upward or downward trend-help stabilize coordination, whereas nonmonotonic histories amplify divergence and path dependence. Regression analyses at the individual level further show that price is the dominant driver of deviation, history moderates this effect, and network effects amplify contextual distortions. Together, these findings advance machine behavior research by providing the first systematic evidence on multi-agent AI systems under network effects and offer guidance for configuring such systems in practice.",
      "publishedDate": "2025-10-08T11:39:16Z",
      "arxivUrl": "https://arxiv.org/abs/2510.06903",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "year": 2025
    },
    {
      "id": "2510.05596",
      "title": "From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions",
      "authors": [
        "Changyuan Zhao",
        "Ruichen Zhang",
        "Jiacheng Wang",
        "Dusit Niyato",
        "Geng Sun",
        "Xianbin Wang",
        "Shiwen Mao",
        "Abbas Jamalipour"
      ],
      "abstract": "Self-evolving agentic artificial intelligence (AI) offers a new paradigm for future wireless systems by enabling autonomous agents to continually adapt and improve without human intervention. Unlike static AI models, self-evolving agents embed an autonomous evolution cycle that updates models, tools, and workflows in response to environmental dynamics. This paper presents a comprehensive overview of self-evolving agentic AI, highlighting its layered architecture, life cycle, and key techniques, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. We further propose a multi-agent cooperative self-evolving agentic AI framework, where multiple large language models (LLMs) are assigned role-specialized prompts under the coordination of a supervisor agent. Through structured dialogue, iterative feedback, and systematic validation, the system autonomously executes the entire life cycle without human intervention. A case study on antenna evolution in low-altitude wireless networks (LAWNs) demonstrates how the framework autonomously upgrades fixed antenna optimization into movable antenna optimization. Experimental results show that the proposed self-evolving agentic AI autonomously improves beam gain and restores degraded performance by up to 52.02%, consistently surpassing the fixed baseline with little to no human intervention and validating its adaptability and robustness for next-generation wireless intelligence.",
      "publishedDate": "2025-10-07T05:45:25Z",
      "arxivUrl": "https://arxiv.org/abs/2510.05596",
      "categories": [
        "agents",
        "multi-agent",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2510.05414",
      "title": "A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis",
      "authors": [
        "Ziheng Geng",
        "Jiachen Liu",
        "Ran Cao",
        "Lu Cheng",
        "Haifeng Wang",
        "Minghui Cheng"
      ],
      "abstract": "Large language models (LLMs) have recently been used to empower autonomous agents in engineering, significantly improving automation and efficiency in labor-intensive workflows. However, their potential remains underexplored in structural engineering, particularly for finite element modeling tasks requiring geometric modeling, complex reasoning, and domain knowledge. To bridge this gap, this paper develops a LLM-based multi-agent system to automate finite element modeling of 2D frames. The system decomposes structural analysis into subtasks, each managed by a specialized agent powered by the lightweight Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis Agent, which extracts geometry, boundary, and material parameters from the user input. Next, a Geometry Agent incrementally derives node coordinates and element connectivity by applying expert-defined rules. These structured outputs are converted into executable OpenSeesPy code by a Translation Agent and refined by a Model Validation Agent through consistency checks. Then, a Load Agent applies load conditions into the assembled structural model. Experimental evaluations on 20 benchmark problems demonstrate that the system achieves accuracy over 80% in most cases across 10 repeated trials, outperforming Gemini-2.5 Pro and ChatGPT-4o models.",
      "publishedDate": "2025-10-06T22:12:52Z",
      "arxivUrl": "https://arxiv.org/abs/2510.05414",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.02209",
      "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?",
      "authors": [
        "Yanxu Chen",
        "Zijun Yao",
        "Yantao Liu",
        "Jin Ye",
        "Jianing Yu",
        "Lei Hou",
        "Juanzi Li"
      ],
      "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.",
      "publishedDate": "2025-10-02T16:54:57Z",
      "arxivUrl": "https://arxiv.org/abs/2510.02209",
      "categories": [
        "agents",
        "evaluation",
        "tool-use",
        "reasoning",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.01869",
      "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
      "authors": [
        "Alessandro Nazzari",
        "Roberto Rubinacci",
        "Marco Lovera"
      ],
      "abstract": "When a single pilot is responsible for managing a multi-drone system, the task may demand varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system on a real-world multi-drone system, and conduct an ablation study to assess the contribution of each module.",
      "publishedDate": "2025-10-02T10:21:35Z",
      "arxivUrl": "https://arxiv.org/abs/2510.01869",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "multi-agent",
        "robotics"
      ],
      "year": 2025
    },
    {
      "id": "2510.01645",
      "title": "Position: Privacy Is Not Just Memorization!",
      "authors": [
        "Niloofar Mireshghallah",
        "Tianshi Li"
      ],
      "abstract": "The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.",
      "publishedDate": "2025-10-02T04:02:06Z",
      "arxivUrl": "https://arxiv.org/abs/2510.01645",
      "categories": [
        "agents"
      ],
      "year": 2025
    },
    {
      "id": "2510.00857",
      "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs",
      "authors": [
        "Adi Simhi",
        "Jonathan Herzig",
        "Martin Tutek",
        "Itay Itzhak",
        "Idan Szpektor",
        "Yonatan Belinkov"
      ],
      "abstract": "As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions. Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.",
      "publishedDate": "2025-10-01T13:08:33Z",
      "arxivUrl": "https://arxiv.org/abs/2510.00857",
      "categories": [
        "agents",
        "evaluation",
        "rag",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.01295",
      "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation",
      "authors": [
        "Zarreen Reza"
      ],
      "abstract": "As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled \"social laboratory\" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement ( > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.",
      "publishedDate": "2025-10-01T07:10:28Z",
      "arxivUrl": "https://arxiv.org/abs/2510.01295",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "multi-agent",
        "prompting",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2510.00510",
      "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
      "authors": [
        "Jiarun Liu",
        "Shiyue Xu",
        "Shangkun Liu",
        "Yang Li",
        "Wen Liu",
        "Min Liu",
        "Xiaoqing Zhou",
        "Hanmin Wang",
        "Shilin Jia",
        "zhen Wang",
        "Shaohua Tian",
        "Hanhao Li",
        "Junbo Zhang",
        "Yongli Yu",
        "Peng Cao",
        "Haofen Wang"
      ],
      "abstract": "Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.",
      "publishedDate": "2025-10-01T04:41:58Z",
      "arxivUrl": "https://arxiv.org/abs/2510.00510",
      "categories": [
        "agents",
        "planning",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.25873",
      "title": "Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs",
      "authors": [
        "Hankun Dai",
        "Maoquan Wang",
        "Mengnan Qi",
        "Yikai Zhang",
        "Zijian Jin",
        "Yongqiang Yao",
        "Yufan Huang",
        "Shengyu Fu",
        "Elsie Nallipogu"
      ],
      "abstract": "Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.",
      "publishedDate": "2025-09-30T07:07:32Z",
      "arxivUrl": "https://arxiv.org/abs/2509.25873",
      "categories": [
        "code-generation",
        "agents",
        "evaluation",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2509.24923",
      "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
      "authors": [
        "Sanxing Chen",
        "Xiaoyin Chen",
        "Yukun Huang",
        "Roy Xie",
        "Bhuwan Dhingra"
      ],
      "abstract": "While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.",
      "publishedDate": "2025-09-29T15:25:42Z",
      "arxivUrl": "https://arxiv.org/abs/2509.24923",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2510.03253",
      "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents",
      "authors": [
        "Heyang Gao",
        "Zexu Sun",
        "Erxue Min",
        "Hengyi Cai",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Xu Chen"
      ],
      "abstract": "Large Language Models (LLMs) as autonomous agents are increasingly tasked with solving complex, long-horizon problems. Aligning these agents via preference-based offline methods like Direct Preference Optimization (DPO) is a promising direction, yet it faces a critical granularity mismatch. Trajectory-level DPO provides a signal that is too coarse for precise credit assignment, while step-level DPO is often too myopic to capture the value of multi-step behaviors. To resolve this challenge, we introduce Hierarchical Preference Learning (HPL), a hierarchical framework that optimizes LLM agents by leveraging preference signals at multiple, synergistic granularities. While HPL incorporates trajectory- and step-level DPO for global and local policy stability, its core innovation lies in group-level preference optimization guided by a dual-layer curriculum. Our approach first decomposes expert trajectories into semantically coherent action groups and then generates contrasting suboptimal groups to enable preference learning at a fine-grained, sub-task level. Then, instead of treating all preference pairs equally, HPL introduces a curriculum scheduler that organizes the learning process from simple to complex. This curriculum is structured along two axes: the group length, representing sub-task complexity, and the sample difficulty, defined by the reward gap between preferred and dispreferred action groups. Experiments on three challenging agent benchmarks show that HPL outperforms existing state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO loss effectively integrates preference signals across multiple granularities, while the dual-layer curriculum is crucial for enabling the agent to solve a wide range of tasks, from simple behaviors to complex multi-step sequences.",
      "publishedDate": "2025-09-26T08:43:39Z",
      "arxivUrl": "https://arxiv.org/abs/2510.03253",
      "categories": [
        "agents",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.21981",
      "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
      "authors": [
        "Zhimin Wang",
        "Shaokang He",
        "Duo Wu",
        "Jinghe Wang",
        "Linjia Kang",
        "Jing Yu",
        "Zhi Wang"
      ],
      "abstract": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
      "publishedDate": "2025-09-26T07:03:52Z",
      "arxivUrl": "https://arxiv.org/abs/2509.21981",
      "categories": [
        "multi-agent",
        "agents",
        "reasoning",
        "planning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.20616",
      "title": "Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning",
      "authors": [
        "Hanjiang Hu",
        "Changliu Liu",
        "Na Li",
        "Yebin Wang"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.",
      "publishedDate": "2025-09-24T23:47:36Z",
      "arxivUrl": "https://arxiv.org/abs/2509.20616",
      "categories": [
        "agents",
        "planning",
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "year": 2025
    },
    {
      "id": "2510.05107",
      "title": "Structured Cognitive Loop for Behavioral Intelligence in Large Language Model Agents",
      "authors": [
        "Myung Ho Kim"
      ],
      "abstract": "Large language models have advanced natural language understanding and generation, but their use as autonomous agents introduces architectural challenges for multi-step tasks. Existing frameworks often mix cognition, memory, and control in a single prompt, reducing coherence and predictability. The Structured Cognitive Loop (SCL) is proposed as an alternative architecture that separates these functions. In SCL, the language model handles cognition, memory is stored externally, and execution is guided by a lightweight controller within a goal-directed loop. This design allows intermediate results to be recorded and verified before actions are taken, improving traceability and evaluation. SCL is evaluated against prompt-based baselines such as ReAct and LangChain agents across three tasks: travel planning, conditional email drafting, and constraint-guided image generation. Under matched settings, SCL achieves an average task success rate of 86.3 percent, compared with 70.5 to 76.8 percent for baselines. It also shows higher goal fidelity, fewer redundant calls, and reduced unsupported assertions. These results indicate that separating cognition, memory, and control can enhance reliability and interpretability without relying on larger models or heavier prompts. The findings should be regarded as preliminary evidence, with broader tests across model families and task domains planned for future work.",
      "publishedDate": "2025-09-23T17:43:17Z",
      "arxivUrl": "https://arxiv.org/abs/2510.05107",
      "categories": [
        "agents",
        "planning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.19199",
      "title": "Agentic Reinforcement Learning with Implicit Step Rewards",
      "authors": [
        "Xiaoqian Liu",
        "Ke Wang",
        "Yuchuan Wu",
        "Fei Huang",
        "Yongbin Li",
        "Junge Zhang",
        "Jianbin Jiao"
      ],
      "abstract": "Large language models (LLMs) are increasingly developed as autonomous agents using reinforcement learning (agentic RL) that reason and act in interactive environments. However, sparse and sometimes unverifiable rewards make it extremely challenging to assign credit when training LLM agents that serve as a policy. Recent work attempts to integrate process supervision into RL but suffers from biased annotation, reward hacking, high-variance from overly fine-grained rewards or failtures when state overlap is rare. We therefore introduce implicit step rewards for agentic RL (iStar), a general credit-assignment strategy that integrates seamlessly with standard RL algorithms without relying on additional rollouts or explicit step labels. Particularly, we alternatively optimize an implicit process reward model (PRM) with the policy model to generate implicit step rewards via a trajectory-based DPO objective. Theoretical analysis shows that this learning objective produces a step-wise reward function. Then the implicit step rewards are used to compute step-level advantages, which are combined with trajectory (or episode)-level advantages for policy updates, creating a self-reinforcing training loop. We evaluate our method on three challenging agent benchmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverifiable rewards in SOTOPIA. Crucially, iStar shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and training stability. Further analysis also demonstrates efficient exploration by iStar with increased rewards in both step- and episode-level while maintaining fewer steps to achieve task success. Code will be available soon.",
      "publishedDate": "2025-09-23T16:15:42Z",
      "arxivUrl": "https://arxiv.org/abs/2509.19199",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.17567",
      "title": "LIMI: Less is More for Agency",
      "authors": [
        "Yang Xiao",
        "Mohan Jiang",
        "Jie Sun",
        "Keyu Li",
        "Jifan Lin",
        "Yumin Zhuang",
        "Ji Zeng",
        "Shijie Xia",
        "Qishuo Hua",
        "Xuefeng Li",
        "Xiaojie Cai",
        "Tongyu Wang",
        "Yue Zhang",
        "Liming Liu",
        "Xia Wu",
        "Jinlong Hou",
        "Yuan Cheng",
        "Wenjie Li",
        "Xiang Wang",
        "Dequan Wang",
        "Pengfei Liu"
      ],
      "abstract": "We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.",
      "publishedDate": "2025-09-22T10:59:32Z",
      "arxivUrl": "https://arxiv.org/abs/2509.17567",
      "categories": [
        "agents",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.17328",
      "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents",
      "authors": [
        "Hongxin Li",
        "Jingran Su",
        "Jingfan Chen",
        "Zheng Ju",
        "Yuntao Chen",
        "Qing Li",
        "Zhaoxiang Zhang"
      ],
      "abstract": "Building autonomous agents that perceive and operate graphical user interfaces (GUIs) like humans has long been a vision in the field of artificial intelligence. Central to these agents is the capability for GUI interaction, which involves GUI understanding and planning capabilities. Existing methods have tried developing GUI agents based on the multi-modal comprehension ability of vision-language models (VLMs). However, the limited scenario, insufficient size, and heterogeneous action spaces hinder the progress of building generalist GUI agents. To resolve these issues, this paper proposes \\textbf{UIPro}, a novel generalist GUI agent trained with extensive multi-platform and multi-task GUI interaction data, coupled with a unified action space. We first curate a comprehensive dataset encompassing 20.6 million GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding capability, which is key to downstream GUI agent tasks. Subsequently, we establish a unified action space to harmonize heterogeneous GUI agent task datasets and produce a merged dataset to foster the action prediction ability of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's superior performance across multiple GUI task benchmarks on various platforms, highlighting the effectiveness of our approach.",
      "publishedDate": "2025-09-22T03:04:53Z",
      "arxivUrl": "https://arxiv.org/abs/2509.17328",
      "categories": [
        "agents",
        "planning",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.16736",
      "title": "Towards Transparent and Incentive-Compatible Collaboration in Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach",
      "authors": [
        "Minfeng Qi",
        "Tianqing Zhu",
        "Lefeng Zhang",
        "Ningran Li",
        "Wanlei Zhou"
      ],
      "abstract": "Large Language Models (LLMs) have enabled the emergence of autonomous agents capable of complex reasoning, planning, and interaction. However, coordinating such agents at scale remains a fundamental challenge, particularly in decentralized environments where communication lacks transparency and agent behavior cannot be shaped through centralized incentives. We propose a blockchain-based framework that enables transparent agent registration, verifiable task allocation, and dynamic reputation tracking through smart contracts. The core of our design lies in two mechanisms: a matching score-based task allocation protocol that evaluates agents by reputation, capability match, and workload; and a behavior-shaping incentive mechanism that adjusts agent behavior via feedback on performance and reward. Our implementation integrates GPT-4 agents with Solidity contracts and demonstrates, through 50-round simulations, strong task success rates, stable utility distribution, and emergent agent specialization. The results underscore the potential for trustworthy, incentive-compatible multi-agent coordination in open environments.",
      "publishedDate": "2025-09-20T16:00:24Z",
      "arxivUrl": "https://arxiv.org/abs/2509.16736",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "reasoning",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2509.12987",
      "title": "Toward PDDL Planning Copilot",
      "authors": [
        "Yarin Benyamin",
        "Argaman Mordoch",
        "Shahaf S. Shperberg",
        "Roni Stern"
      ],
      "abstract": "Large Language Models (LLMs) are increasingly being used as autonomous agents capable of performing complicated tasks. However, they lack the ability to perform reliable long-horizon planning on their own. This paper bridges this gap by introducing the Planning Copilot, a chatbot that integrates multiple planning tools and allows users to invoke them through instructions in natural language. The Planning Copilot leverages the Model Context Protocol (MCP), a recently developed standard for connecting LLMs with external tools and systems. This approach allows using any LLM that supports MCP without domain-specific fine-tuning. Our Planning Copilot supports common planning tasks such as checking the syntax of planning problems, selecting an appropriate planner, calling it, validating the plan it generates, and simulating their execution. We empirically evaluate the ability of our Planning Copilot to perform these tasks using three open-source LLMs. The results show that the Planning Copilot highly outperforms using the same LLMs without the planning tools. We also conducted a limited qualitative comparison of our tool against Chat GPT-5, a very recent commercial LLM. Our results shows that our Planning Copilot significantly outperforms GPT-5 despite relying on a much smaller LLM. This suggests dedicated planning tools may be an effective way to enable LLMs to perform planning tasks.",
      "publishedDate": "2025-09-16T11:51:07Z",
      "arxivUrl": "https://arxiv.org/abs/2509.12987",
      "categories": [
        "agents",
        "tool-use",
        "planning",
        "rag",
        "prompting"
      ],
      "year": 2025
    },
    {
      "id": "2509.11943",
      "title": "Agentic System with Modal Logic for Autonomous Diagnostics",
      "authors": [
        "Antonin Sulc",
        "Thorsten Hellert"
      ],
      "abstract": "The development of intelligent agents, particularly those powered by language models (LMs), has shown a critical role in various environments that require intelligent and autonomous decision-making. Environments are not passive testing grounds, and they represent the data required for agents to learn and exhibit in very challenging conditions that require adaptive, complex, and autonomous capacity to make decisions. While the paradigm of scaling models and datasets has led to remarkable emergent capabilities, we argue that scaling the structure, fidelity, and logical consistency of agent reasoning within these environments is a crucial, yet underexplored, dimension of AI research. This paper introduces a neuro-symbolic multi-agent architecture where the belief states of individual agents are formally represented as Kripke models. This foundational choice enables them to reason about known concepts of \\emph{possibility} and \\emph{necessity} using the formal language of modal logic. In this work, we use immutable, domain-specific knowledge to make an informed root cause diagnosis, which is encoded as logical constraints essential for proper, reliable, and explainable diagnosis. In the proposed model, we show constraints that actively guide the hypothesis generation of LMs, effectively preventing them from reaching physically or logically untenable conclusions. In a high-fidelity simulated particle accelerator environment, our system successfully diagnoses complex, cascading failures by combining the powerful semantic intuition of LMs with the rigorous, verifiable validation of modal logic and a factual world model and showcasing a viable path toward more robust, reliable, and verifiable autonomous agents.",
      "publishedDate": "2025-09-15T14:03:06Z",
      "arxivUrl": "https://arxiv.org/abs/2509.11943",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "year": 2025
    },
    {
      "id": "2509.09906",
      "title": "Tackling One Health Risks: How Large Language Models are leveraged for Risk Negotiation and Consensus-building",
      "authors": [
        "Alexandra Fetsch",
        "Iurii Savvateev",
        "Racem Ben Romdhane",
        "Martin Wiedmann",
        "Artemiy Dimov",
        "Maciej Durkalec",
        "Josef Teichmann",
        "Jakob Zinsstag",
        "Konstantinos Koutsoumanis",
        "Andreja Rajkovic",
        "Jason Mann",
        "Mauro Tonolla",
        "Monika Ehling-Schulz",
        "Matthias Filter",
        "Sophia Johler"
      ],
      "abstract": "Key global challenges of our times are characterized by complex interdependencies and can only be effectively addressed through an integrated, participatory effort. Conventional risk analysis frameworks often reduce complexity to ensure manageability, creating silos that hinder comprehensive solutions. A fundamental shift towards holistic strategies is essential to enable effective negotiations between different sectors and to balance the competing interests of stakeholders. However, achieving this balance is often hindered by limited time, vast amounts of information, and the complexity of integrating diverse perspectives. This study presents an AI-assisted negotiation framework that incorporates large language models (LLMs) and AI-based autonomous agents into a negotiation-centered risk analysis workflow. The framework enables stakeholders to simulate negotiations, systematically model dynamics, anticipate compromises, and evaluate solution impacts. By leveraging LLMs' semantic analysis capabilities we could mitigate information overload and augment decision-making process under time constraints. Proof-of-concept implementations were conducted in two real-world scenarios: (i) prudent use of a biopesticide, and (ii) targeted wild animal population control. Our work demonstrates the potential of AI-assisted negotiation to address the current lack of tools for cross-sectoral engagement. Importantly, the solution's open source, web based design, suits for application by a broader audience with limited resources and enables users to tailor and develop it for their own needs.",
      "publishedDate": "2025-09-12T00:25:20Z",
      "arxivUrl": "https://arxiv.org/abs/2509.09906",
      "categories": [
        "agents",
        "rag"
      ],
      "year": 2025
    },
    {
      "id": "2509.09356",
      "title": "Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning",
      "authors": [
        "Abdel Hakim Drid",
        "Vincenzo Suriani",
        "Daniele Nardi",
        "Abderrezzak Debilou"
      ],
      "abstract": "Navigating and understanding complex and unknown environments autonomously demands more than just basic perception and movement from embodied agents. Truly effective exploration requires agents to possess higher-level cognitive abilities, the ability to reason about their surroundings, and make more informed decisions regarding exploration strategies. However, traditional RL approaches struggle to balance efficient exploration and semantic understanding due to limited cognitive capabilities embedded in the small policies for the agents, leading often to human drivers when dealing with semantic exploration. In this paper, we address this challenge by presenting a novel Deep Reinforcement Learning (DRL) architecture that is specifically designed for resource efficient semantic exploration. A key methodological contribution is the integration of a Vision-Language Model (VLM) common-sense through a layered reward function. The VLM query is modeled as a dedicated action, allowing the agent to strategically query the VLM only when deemed necessary for gaining external guidance, thereby conserving resources. This mechanism is combined with a curriculum learning strategy designed to guide learning at different levels of complexity to ensure robust and stable learning. Our experimental evaluation results convincingly demonstrate that our agent achieves significantly enhanced object discovery rates and develops a learned capability to effectively navigate towards semantically rich regions. Furthermore, it also shows a strategic mastery of when to prompt for external environmental information. By demonstrating a practical and scalable method for embedding common-sense semantic reasoning with autonomous agents, this research provides a novel approach to pursuing a fully intelligent and self-guided exploration in robotics.",
      "publishedDate": "2025-09-11T11:10:08Z",
      "arxivUrl": "https://arxiv.org/abs/2509.09356",
      "categories": [
        "robotics",
        "agents",
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.09215",
      "title": "Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions",
      "authors": [
        "Qinnan Hu",
        "Yuntao Wang",
        "Yuan Gao",
        "Zhou Su",
        "Linkang Du"
      ],
      "abstract": "Large language models (LLMs)-empowered autonomous agents are transforming both digital and physical environments by enabling adaptive, multi-agent collaboration. While these agents offer significant opportunities across domains such as finance, healthcare, and smart manufacturing, their unpredictable behaviors and heterogeneous capabilities pose substantial governance and accountability challenges. In this paper, we propose a blockchain-enabled layered architecture for regulatory agent collaboration, comprising an agent layer, a blockchain data layer, and a regulatory application layer. Within this framework, we design three key modules: (i) an agent behavior tracing and arbitration module for automated accountability, (ii) a dynamic reputation evaluation module for trust assessment in collaborative scenarios, and (iii) a malicious behavior forecasting module for early detection of adversarial activities. Our approach establishes a systematic foundation for trustworthy, resilient, and scalable regulatory mechanisms in large-scale agent ecosystems. Finally, we discuss the future research directions for blockchain-enabled regulatory frameworks in multi-agent systems.",
      "publishedDate": "2025-09-11T07:46:00Z",
      "arxivUrl": "https://arxiv.org/abs/2509.09215",
      "categories": [
        "multi-agent",
        "agents",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2509.08380",
      "title": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives",
      "authors": [
        "Prathamesh Vasudeo Naik",
        "Naresh Kumar Dintakurthi",
        "Zhanghao Hu",
        "Yue Wang",
        "Robby Qiu"
      ],
      "abstract": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows. While large language models (LLMs) offer promising fluency, they suffer from factual hallucination, limited crime typology alignment, and poor explainability -- posing unacceptable risks in compliance-critical domains. This paper introduces Co-Investigator AI, an agentic framework optimized to produce Suspicious Activity Reports (SARs) significantly faster and with greater accuracy than traditional methods. Drawing inspiration from recent advances in autonomous agent architectures, such as the AI Co-Scientist, our approach integrates specialized agents for planning, crime type detection, external intelligence gathering, and compliance validation. The system features dynamic memory management, an AI-Privacy Guard layer for sensitive data handling, and a real-time validation agent employing the Agent-as-a-Judge paradigm to ensure continuous narrative quality assurance. Human investigators remain firmly in the loop, empowered to review and refine drafts in a collaborative workflow that blends AI efficiency with domain expertise. We demonstrate the versatility of Co-Investigator AI across a range of complex financial crime scenarios, highlighting its ability to streamline SAR drafting, align narratives with regulatory expectations, and enable compliance teams to focus on higher-order analytical work. This approach marks the beginning of a new era in compliance reporting -- bringing the transformative benefits of AI agents to the core of regulatory processes and paving the way for scalable, reliable, and transparent SAR generation.",
      "publishedDate": "2025-09-10T08:16:04Z",
      "arxivUrl": "https://arxiv.org/abs/2509.08380",
      "categories": [
        "agents",
        "planning"
      ],
      "year": 2025
    },
    {
      "id": "2509.08088",
      "title": "EnvX: Agentize Everything with Agentic AI",
      "authors": [
        "Linyao Chen",
        "Zimian Peng",
        "Yingxuan Yang",
        "Yikun Wang",
        "Wenzheng Tom Tang",
        "Hiroki H. Kobayashi",
        "Weinan Zhang"
      ],
      "abstract": "The widespread availability of open-source repositories has led to a vast collection of reusable software components, yet their utilization remains manual, error-prone, and disconnected. Developers must navigate documentation, understand APIs, and write integration code, creating significant barriers to efficient software reuse. To address this, we present EnvX, a framework that leverages Agentic AI to agentize GitHub repositories, transforming them into intelligent, autonomous agents capable of natural language interaction and inter-agent collaboration. Unlike existing approaches that treat repositories as static code resources, EnvX reimagines them as active agents through a three-phase process: (1) TODO-guided environment initialization, which sets up the necessary dependencies, data, and validation datasets; (2) human-aligned agentic automation, allowing repository-specific agents to autonomously perform real-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple agents to collaborate. By combining large language model capabilities with structured tool integration, EnvX automates not just code generation, but the entire process of understanding, initializing, and operationalizing repository functionality. We evaluate EnvX on the GitTaskBench benchmark, using 18 repositories across domains such as image processing, speech recognition, document analysis, and video manipulation. Our results show that EnvX achieves a 74.07% execution completion rate and 51.85% task pass rate, outperforming existing frameworks. Case studies further demonstrate EnvX's ability to enable multi-repository collaboration via the A2A protocol. This work marks a shift from treating repositories as passive code resources to intelligent, interactive agents, fostering greater accessibility and collaboration within the open-source ecosystem.",
      "publishedDate": "2025-09-09T18:51:36Z",
      "arxivUrl": "https://arxiv.org/abs/2509.08088",
      "categories": [
        "code-generation",
        "agents",
        "tool-use",
        "multi-agent",
        "rag",
        "evaluation"
      ],
      "year": 2025
    },
    {
      "id": "2412.12345",
      "title": "Sample LLM Agent Paper: Task Planning with Large Language Models",
      "authors": [
        "John Doe",
        "Jane Smith"
      ],
      "abstract": "This paper presents a novel approach to task planning using large language models. We demonstrate that LLM agents can effectively decompose complex tasks into manageable subtasks through chain-of-thought reasoning.",
      "publishedDate": "2024-12-15T00:00:00Z",
      "arxivUrl": "https://arxiv.org/abs/2412.12345",
      "categories": [
        "agents",
        "planning",
        "reasoning"
      ],
      "year": 2024
    },
    {
      "id": "2411.98765",
      "title": "Retrieval-Augmented Generation for Domain-Specific Applications",
      "authors": [
        "Alice Johnson",
        "Bob Williams"
      ],
      "abstract": "We explore the use of retrieval-augmented generation (RAG) systems for improving large language model performance on domain-specific tasks. Our approach combines dense retrieval with prompting strategies.",
      "publishedDate": "2024-11-20T00:00:00Z",
      "arxivUrl": "https://arxiv.org/abs/2411.98765",
      "categories": [
        "rag",
        "prompting"
      ],
      "year": 2024
    },
    {
      "id": "2410.55555",
      "title": "Multi-Agent Systems for Code Generation with LLMs",
      "authors": [
        "Charlie Brown"
      ],
      "abstract": "This work investigates multi-agent collaboration for automated code generation. We show that multiple LLM agents working together can produce higher quality code than single-agent systems.",
      "publishedDate": "2024-10-10T00:00:00Z",
      "arxivUrl": "https://arxiv.org/abs/2410.55555",
      "categories": [
        "multi-agent",
        "code-generation",
        "agents"
      ],
      "year": 2024
    }
  ]
}