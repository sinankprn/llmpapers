{
  "year": "2026",
  "count": 362,
  "papers": [
    {
      "id": "2601.00791",
      "title": "Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning",
      "authors": [
        {
          "name": "Valentin Noël",
          "affiliation": null
        }
      ],
      "abstract": "We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\\text{MW}} = 1.16 \\times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.",
      "publishedDate": "2026-01-02T18:49:37Z",
      "updatedDate": "2026-01-02T18:49:37Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00791v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00791",
      "comment": "58 pages, 19 figures, Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00756",
      "title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
      "authors": [
        {
          "name": "Thomas Katraouras",
          "affiliation": null
        },
        {
          "name": "Dimitrios Rafailidis",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.",
      "publishedDate": "2026-01-02T17:22:34Z",
      "updatedDate": "2026-01-02T17:22:34Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00756v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00756",
      "comment": "Accepted to the 41st ACM/SIGAPP Symposium on Applied Computing (SAC '26)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00694",
      "title": "A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference",
      "authors": [
        {
          "name": "Qingwen Pu",
          "affiliation": null
        },
        {
          "name": "Kun Xie",
          "affiliation": null
        },
        {
          "name": "Hong Yang",
          "affiliation": null
        },
        {
          "name": "Guocong Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.",
      "publishedDate": "2026-01-02T14:13:28Z",
      "updatedDate": "2026-01-02T14:13:28Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00694v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00694",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00635",
      "title": "SEMODS: A Validated Dataset of Open-Source Software Engineering Models",
      "authors": [
        {
          "name": "Alexandra González",
          "affiliation": null
        },
        {
          "name": "Xavier Franch",
          "affiliation": null
        },
        {
          "name": "Silverio Martínez-Fernández",
          "affiliation": null
        }
      ],
      "abstract": "Integrating Artificial Intelligence into Software Engineering (SE) requires having a curated collection of models suited to SE tasks. With millions of models hosted on Hugging Face (HF) and new ones continuously being created, it is infeasible to identify SE models without a dedicated catalogue. To address this gap, we present SEMODS: an SE-focused dataset of 3,427 models extracted from HF, combining automated collection with rigorous validation through manual annotation and large language model assistance. Our dataset links models to SE tasks and activities from the software development lifecycle, offering a standardized representation of their evaluation results, and supporting multiple applications such as data analysis, model discovery, benchmarking, and model adaptation.",
      "publishedDate": "2026-01-02T10:38:24Z",
      "updatedDate": "2026-01-02T10:38:24Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00635v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00635",
      "comment": "Accepted at the 3rd ACM international conference on AI Foundation Models and Software Engineering (FORGE 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00497",
      "title": "STELLAR: A Search-Based Testing Framework for Large Language Model Applications",
      "authors": [
        {
          "name": "Lev Sorokin",
          "affiliation": null
        },
        {
          "name": "Ivan Vasilev",
          "affiliation": null
        },
        {
          "name": "Ken E. Friedl",
          "affiliation": null
        },
        {
          "name": "Andrea Stocco",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based applications are increasingly deployed across various domains, including customer service, education, and mobility. However, these systems are prone to inaccurate, fictitious, or harmful responses, and their vast, high-dimensional input space makes systematic testing particularly challenging. To address this, we present STELLAR, an automated search-based testing framework for LLM-based applications that systematically uncovers text inputs leading to inappropriate system responses. Our framework models test generation as an optimization problem and discretizes the input space into stylistic, content-related, and perturbation features. Unlike prior work that focuses on prompt optimization or coverage heuristics, our work employs evolutionary optimization to dynamically explore feature combinations that are more likely to expose failures. We evaluate STELLAR on three LLM-based conversational question-answering systems. The first focuses on safety, benchmarking both public and proprietary LLMs against malicious or unsafe prompts. The second and third target navigation, using an open-source and an industrial retrieval-augmented system for in-vehicle venue recommendations. Overall, STELLAR exposes up to 4.3 times (average 2.5 times) more failures than the existing baseline approaches.",
      "publishedDate": "2026-01-01T22:30:15Z",
      "updatedDate": "2026-01-05T18:03:57Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00497v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00497",
      "comment": "Accepted for publication at the 33th International Conference on Software Analysis, Evolution and Reengineering (SANER 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00348",
      "title": "Robust Uncertainty Quantification for Factual Generation of Large Language Models",
      "authors": [
        {
          "name": "Yuhao Zhang",
          "affiliation": null
        },
        {
          "name": "Zhongliang Yang",
          "affiliation": null
        },
        {
          "name": "Linna Zhou",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.",
      "publishedDate": "2026-01-01T14:06:58Z",
      "updatedDate": "2026-01-01T14:06:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00348v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00348",
      "comment": "9 pages, 5 tables, 5 figures, accepted to IJCNN 2025",
      "journalRef": "2025 International Joint Conference on Neural Networks (IJCNN), Rome, Italy, 2025, pp. 1-9",
      "doi": "10.1109/IJCNN64981.2025.11227634",
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00282",
      "title": "Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations",
      "authors": [
        {
          "name": "Qianli Wang",
          "affiliation": null
        },
        {
          "name": "Nils Feldhus",
          "affiliation": null
        },
        {
          "name": "Pepa Atanasova",
          "affiliation": null
        },
        {
          "name": "Fedor Splitt",
          "affiliation": null
        },
        {
          "name": "Simon Ostermann",
          "affiliation": null
        },
        {
          "name": "Sebastian Möller",
          "affiliation": null
        },
        {
          "name": "Vera Schmitt",
          "affiliation": null
        }
      ],
      "abstract": "Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\\%) and faithfulness (up to 2.38\\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.",
      "publishedDate": "2026-01-01T09:50:01Z",
      "updatedDate": "2026-01-01T09:50:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00282v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00282",
      "comment": "In submission",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00274",
      "title": "Making Theft Useless: Adulteration-Based Protection of Proprietary Knowledge Graphs in GraphRAG Systems",
      "authors": [
        {
          "name": "Weijie Wang",
          "affiliation": null
        },
        {
          "name": "Peizhuo Lv",
          "affiliation": null
        },
        {
          "name": "Yan Wang",
          "affiliation": null
        },
        {
          "name": "Rujie Dai",
          "affiliation": null
        },
        {
          "name": "Guokun Xu",
          "affiliation": null
        },
        {
          "name": "Qiujian Lv",
          "affiliation": null
        },
        {
          "name": "Hangcheng Liu",
          "affiliation": null
        },
        {
          "name": "Weiqing Huang",
          "affiliation": null
        },
        {
          "name": "Wei Dong",
          "affiliation": null
        },
        {
          "name": "Jiaheng Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) has emerged as a key technique for enhancing Large Language Models (LLMs) with proprietary Knowledge Graphs (KGs) in knowledge-intensive applications. As these KGs often represent an organization's highly valuable intellectual property (IP), they face a significant risk of theft for private use. In this scenario, attackers operate in isolated environments. This private-use threat renders passive defenses like watermarking ineffective, as they require output access for detection. Simultaneously, the low-latency demands of GraphRAG make strong encryption which incurs prohibitive overhead impractical. To address these challenges, we propose AURA, a novel framework based on Data Adulteration designed to make any stolen KG unusable to an adversary. Our framework pre-emptively injects plausible but false adulterants into the KG. For an attacker, these adulterants deteriorate the retrieved context and lead to factually incorrect responses. Conversely, for authorized users, a secret key enables the efficient filtering of all adulterants via encrypted metadata tags before they are passed to the LLM, ensuring query results remain completely accurate. Our evaluation demonstrates the effectiveness of this approach: AURA degrades the performance of unauthorized systems to an accuracy of just 5.3%, while maintaining 100% fidelity for authorized users with negligible overhead. Furthermore, AURA proves robust against various sanitization attempts, retaining 80.2% of its adulterants.",
      "publishedDate": "2026-01-01T09:27:24Z",
      "updatedDate": "2026-01-01T09:27:24Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00274v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00274",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00254",
      "title": "An Empirical Evaluation of LLM-Based Approaches for Code Vulnerability Detection: RAG, SFT, and Dual-Agent Systems",
      "authors": [
        {
          "name": "Md Hasan Saju",
          "affiliation": null
        },
        {
          "name": "Maher Muhtadi",
          "affiliation": null
        },
        {
          "name": "Akramul Azim",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancement of Large Language Models (LLMs) presents new opportunities for automated software vulnerability detection, a crucial task in securing modern codebases. This paper presents a comparative study on the effectiveness of LLM-based techniques for detecting software vulnerabilities. The study evaluates three approaches, Retrieval-Augmented Generation (RAG), Supervised Fine-Tuning (SFT), and a Dual-Agent LLM framework, against a baseline LLM model. A curated dataset was compiled from Big-Vul and real-world code repositories from GitHub, focusing on five critical Common Weakness Enumeration (CWE) categories: CWE-119, CWE-399, CWE-264, CWE-20, and CWE-200. Our RAG approach, which integrated external domain knowledge from the internet and the MITRE CWE database, achieved the highest overall accuracy (0.86) and F1 score (0.85), highlighting the value of contextual augmentation. Our SFT approach, implemented using parameter-efficient QLoRA adapters, also demonstrated strong performance. Our Dual-Agent system, an architecture in which a secondary agent audits and refines the output of the first, showed promise in improving reasoning transparency and error mitigation, with reduced resource overhead. These results emphasize that incorporating a domain expertise mechanism significantly strengthens the practical applicability of LLMs in real-world vulnerability detection tasks.",
      "publishedDate": "2026-01-01T08:05:51Z",
      "updatedDate": "2026-01-01T08:05:51Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00254v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00254",
      "comment": null,
      "journalRef": "https://conf.researchr.org/home/cascon-2025",
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "code-generation",
        "agents",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "agents",
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00213",
      "title": "Overlooked Safety Vulnerability in LLMs: Malicious Intelligent Optimization Algorithm Request and its Jailbreak",
      "authors": [
        {
          "name": "Haoran Gu",
          "affiliation": null
        },
        {
          "name": "Handing Wang",
          "affiliation": null
        },
        {
          "name": "Yi Mei",
          "affiliation": null
        },
        {
          "name": "Mengjie Zhang",
          "affiliation": null
        },
        {
          "name": "Yaochu Jin",
          "affiliation": null
        }
      ],
      "abstract": "The widespread deployment of large language models (LLMs) has raised growing concerns about their misuse risks and associated safety issues. While prior studies have examined the safety of LLMs in general usage, code generation, and agent-based applications, their vulnerabilities in automated algorithm design remain underexplored. To fill this gap, this study investigates this overlooked safety vulnerability, with a particular focus on intelligent optimization algorithm design, given its prevalent use in complex decision-making scenarios. We introduce MalOptBench, a benchmark consisting of 60 malicious optimization algorithm requests, and propose MOBjailbreak, a jailbreak method tailored for this scenario. Through extensive evaluation of 13 mainstream LLMs including the latest GPT-5 and DeepSeek-V3.1, we reveal that most models remain highly susceptible to such attacks, with an average attack success rate of 83.59% and an average harmfulness score of 4.28 out of 5 on original harmful prompts, and near-complete failure under MOBjailbreak. Furthermore, we assess state-of-the-art plug-and-play defenses that can be applied to closed-source models, and find that they are only marginally effective against MOBjailbreak and prone to exaggerated safety behaviors. These findings highlight the urgent need for stronger alignment techniques to safeguard LLMs against misuse in algorithm design.",
      "publishedDate": "2026-01-01T05:14:32Z",
      "updatedDate": "2026-01-01T05:14:32Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00213v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00213",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00202",
      "title": "Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models",
      "authors": [
        {
          "name": "Wang Xing",
          "affiliation": null
        },
        {
          "name": "Wei Song",
          "affiliation": null
        },
        {
          "name": "Siyu Lin",
          "affiliation": null
        },
        {
          "name": "Chen Wu",
          "affiliation": null
        },
        {
          "name": "Zhesi Li",
          "affiliation": null
        },
        {
          "name": "Man Wang",
          "affiliation": null
        }
      ],
      "abstract": "Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.",
      "publishedDate": "2026-01-01T04:38:00Z",
      "updatedDate": "2026-01-01T04:38:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00202v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00202",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00770",
      "title": "LLM Agents for Combinatorial Efficient Frontiers: Investment Portfolio Optimization",
      "authors": [
        {
          "name": "Simon Paquette-Greenbaum",
          "affiliation": null
        },
        {
          "name": "Jiangbo Yu",
          "affiliation": null
        }
      ],
      "abstract": "Investment portfolio optimization is a task conducted in all major financial institutions. The Cardinality Constrained Mean-Variance Portfolio Optimization (CCPO) problem formulation is ubiquitous for portfolio optimization. The challenge of this type of portfolio optimization, a mixed-integer quadratic programming (MIQP) problem, arises from the intractability of solutions from exact solvers, where heuristic algorithms are used to find approximate portfolio solutions. CCPO entails many laborious and complex workflows and also requires extensive effort pertaining to heuristic algorithm development, where the combination of pooled heuristic solutions results in improved efficient frontiers. Hence, common approaches are to develop many heuristic algorithms. Agentic frameworks emerge as a promising candidate for many problems within combinatorial optimization, as they have been shown to be equally efficient with regard to automating large workflows and have been shown to be excellent in terms of algorithm development, sometimes surpassing human-level performance. This study implements a novel agentic framework for the CCPO and explores several concrete architectures. In benchmark problems, the implemented agentic framework matches state-of-the-art algorithms. Furthermore, complex workflows and algorithm development efforts are alleviated, while in the worst case, lower but acceptable error is reported.",
      "publishedDate": "2026-01-02T18:02:13Z",
      "updatedDate": "2026-01-02T18:02:13Z",
      "primaryCategory": "cs.CE",
      "arxivCategories": [
        "cs.CE",
        "cs.AI",
        "econ.GN"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00770v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00770",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00596",
      "title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence",
      "authors": [
        {
          "name": "Sumanth Balaji",
          "affiliation": null
        },
        {
          "name": "Piyush Mishra",
          "affiliation": null
        },
        {
          "name": "Aashraya Sachdeva",
          "affiliation": null
        },
        {
          "name": "Suraj Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.",
      "publishedDate": "2026-01-02T07:21:23Z",
      "updatedDate": "2026-01-02T07:21:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00596v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00596",
      "comment": "17 pages, 3 figures, preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00516",
      "title": "Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI",
      "authors": [
        {
          "name": "Laksh Advani",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both \"wrong plan for this task\" and \"malformed plan structure.\" On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.",
      "publishedDate": "2026-01-02T00:27:11Z",
      "updatedDate": "2026-01-02T00:27:11Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00516v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00516",
      "comment": "Accepted to AAAI Trustagent 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00268",
      "title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity",
      "authors": [
        {
          "name": "Doyoung Kim",
          "affiliation": null
        },
        {
          "name": "Zhiwei Ren",
          "affiliation": null
        },
        {
          "name": "Jie Hao",
          "affiliation": null
        },
        {
          "name": "Zhongkai Sun",
          "affiliation": null
        },
        {
          "name": "Lichao Wang",
          "affiliation": null
        },
        {
          "name": "Xiyao Ma",
          "affiliation": null
        },
        {
          "name": "Zack Ye",
          "affiliation": null
        },
        {
          "name": "Xu Han",
          "affiliation": null
        },
        {
          "name": "Jun Yin",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        },
        {
          "name": "Wei Shen",
          "affiliation": null
        },
        {
          "name": "Xing Fan",
          "affiliation": null
        },
        {
          "name": "Benjamin Yao",
          "affiliation": null
        },
        {
          "name": "Chenlei Guo",
          "affiliation": null
        }
      ],
      "abstract": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.",
      "publishedDate": "2026-01-01T09:19:20Z",
      "updatedDate": "2026-01-01T09:19:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00268v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00268",
      "comment": "26 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00227",
      "title": "FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems",
      "authors": [
        {
          "name": "Shanli Xing",
          "affiliation": null
        },
        {
          "name": "Yiyan Zhai",
          "affiliation": null
        },
        {
          "name": "Alexander Jiang",
          "affiliation": null
        },
        {
          "name": "Yixin Dong",
          "affiliation": null
        },
        {
          "name": "Yong Wu",
          "affiliation": null
        },
        {
          "name": "Zihao Ye",
          "affiliation": null
        },
        {
          "name": "Charlie Ruan",
          "affiliation": null
        },
        {
          "name": "Yingyi Huang",
          "affiliation": null
        },
        {
          "name": "Yineng Zhang",
          "affiliation": null
        },
        {
          "name": "Liangsheng Yin",
          "affiliation": null
        },
        {
          "name": "Aksara Bayyapu",
          "affiliation": null
        },
        {
          "name": "Luis Ceze",
          "affiliation": null
        },
        {
          "name": "Tianqi Chen",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances show that large language models (LLMs) can act as autonomous agents capable of generating GPU kernels, but integrating these AI-generated kernels into real-world inference systems remains challenging. FlashInfer-Bench addresses this gap by establishing a standardized, closed-loop framework that connects kernel generation, benchmarking, and deployment. At its core, FlashInfer Trace provides a unified schema describing kernel definitions, workloads, implementations, and evaluations, enabling consistent communication between agents and systems. Built on real serving traces, FlashInfer-Bench includes a curated dataset, a robust correctness- and performance-aware benchmarking framework, a public leaderboard to track LLM agents' GPU programming capabilities, and a dynamic substitution mechanism (apply()) that seamlessly injects the best-performing kernels into production LLM engines such as SGLang and vLLM. Using FlashInfer-Bench, we further evaluate the performance and limitations of LLM agents, compare the trade-offs among different GPU programming languages, and provide insights for future agent design. FlashInfer-Bench thus establishes a practical, reproducible pathway for continuously improving AI-generated kernels and deploying them into large-scale LLM inference.",
      "publishedDate": "2026-01-01T06:18:53Z",
      "updatedDate": "2026-01-01T06:18:53Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00227v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00227",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00513",
      "title": "When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents",
      "authors": [
        {
          "name": "Laksh Advani",
          "affiliation": null
        }
      ],
      "abstract": "Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.",
      "publishedDate": "2026-01-01T23:54:15Z",
      "updatedDate": "2026-01-01T23:54:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00513v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00513",
      "comment": "Accepted to Trustagent workshop AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00777",
      "title": "Investigating the Viability of Employing Multi-modal Large Language Models in the Context of Audio Deepfake Detection",
      "authors": [
        {
          "name": "Akanksha Chuchra",
          "affiliation": null
        },
        {
          "name": "Shukesh Reddy",
          "affiliation": null
        },
        {
          "name": "Sudeepta Mishra",
          "affiliation": null
        },
        {
          "name": "Abhijit Das",
          "affiliation": null
        },
        {
          "name": "Abhinav Dhall",
          "affiliation": null
        }
      ],
      "abstract": "While Vision-Language Models (VLMs) and Multimodal Large Language Models (MLLMs) have shown strong generalisation in detecting image and video deepfakes, their use for audio deepfake detection remains largely unexplored. In this work, we aim to explore the potential of MLLMs for audio deepfake detection. Combining audio inputs with a range of text prompts as queries to find out the viability of MLLMs to learn robust representations across modalities for audio deepfake detection. Therefore, we attempt to explore text-aware and context-rich, question-answer based prompts with binary decisions. We hypothesise that such a feature-guided reasoning will help in facilitating deeper multimodal understanding and enable robust feature learning for audio deepfake detection. We evaluate the performance of two MLLMs, Qwen2-Audio-7B-Instruct and SALMONN, in two evaluation modes: (a) zero-shot and (b) fine-tuned. Our experiments demonstrate that combining audio with a multi-prompt approach could be a viable way forward for audio deepfake detection. Our experiments show that the models perform poorly without task-specific training and struggle to generalise to out-of-domain data. However, they achieve good performance on in-domain data with minimal supervision, indicating promising potential for audio deepfake detection.",
      "publishedDate": "2026-01-02T18:17:22Z",
      "updatedDate": "2026-01-02T18:17:22Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00777v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00777",
      "comment": "Accepted at IJCB 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00747",
      "title": "The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving",
      "authors": [
        {
          "name": "Max Ruiz Luyten",
          "affiliation": null
        },
        {
          "name": "Mihaela van der Schaar",
          "affiliation": null
        }
      ],
      "abstract": "State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.",
      "publishedDate": "2026-01-02T17:10:31Z",
      "updatedDate": "2026-01-02T17:10:31Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00747v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00747",
      "comment": "56 pages, 9 figures, submitted to Twenty-Ninth Annual Conference on Artificial Intelligence and Statistics",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00736",
      "title": "Exploring the Performance of Large Language Models on Subjective Span Identification Tasks",
      "authors": [
        {
          "name": "Alphaeus Dmonte",
          "affiliation": null
        },
        {
          "name": "Roland Oruche",
          "affiliation": null
        },
        {
          "name": "Tharindu Ranasinghe",
          "affiliation": null
        },
        {
          "name": "Marcos Zampieri",
          "affiliation": null
        },
        {
          "name": "Prasad Calyam",
          "affiliation": null
        }
      ],
      "abstract": "Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.",
      "publishedDate": "2026-01-02T16:30:14Z",
      "updatedDate": "2026-01-02T16:30:14Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00736v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00736",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00730",
      "title": "Grading Handwritten Engineering Exams with Multimodal Large Language Models",
      "authors": [
        {
          "name": "Janez Perš",
          "affiliation": null
        },
        {
          "name": "Jon Muhovič",
          "affiliation": null
        },
        {
          "name": "Andrej Košir",
          "affiliation": null
        },
        {
          "name": "Boštjan Murovec",
          "affiliation": null
        }
      ],
      "abstract": "Handwritten STEM exams capture open-ended reasoning and diagrams, but manual grading is slow and difficult to scale. We present an end-to-end workflow for grading scanned handwritten engineering quizzes with multimodal large language models (LLMs) that preserves the standard exam process (A4 paper, unconstrained student handwriting). The lecturer provides only a handwritten reference solution (100%) and a short set of grading rules; the reference is converted into a text-only summary that conditions grading without exposing the reference scan. Reliability is achieved through a multi-stage design with a format/presence check to prevent grading blank answers, an ensemble of independent graders, supervisor aggregation, and rigid templates with deterministic validation to produce auditable, machine-parseable reports. We evaluate the frozen pipeline in a clean-room protocol on a held-out real course quiz in Slovenian, including hand-drawn circuit schematics. With state-of-the-art backends (GPT-5.2 and Gemini-3 Pro), the full pipeline achieves $\\approx$8-point mean absolute difference to lecturer grades with low bias and an estimated manual-review trigger rate of $\\approx$17% at $D_{\\max}=40$. Ablations show that trivial prompting and removing the reference solution substantially degrade accuracy and introduce systematic over-grading, confirming that structured prompting and reference grounding are essential.",
      "publishedDate": "2026-01-02T16:10:08Z",
      "updatedDate": "2026-01-02T16:10:08Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00730v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00730",
      "comment": "10 pages, 5 figures, 2 tables. Supplementary material available at https://lmi.fe.uni-lj.si/en/janez-pers-2/supplementary-material/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00680",
      "title": "Sigmoid Head for Quality Estimation under Language Ambiguity",
      "authors": [
        {
          "name": "Tu Anh Dinh",
          "affiliation": null
        },
        {
          "name": "Jan Niehues",
          "affiliation": null
        }
      ],
      "abstract": "Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.",
      "publishedDate": "2026-01-02T13:12:28Z",
      "updatedDate": "2026-01-02T13:12:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00680v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00680",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00675",
      "title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics",
      "authors": [
        {
          "name": "Tony Lee",
          "affiliation": null
        },
        {
          "name": "Andrew Wagenmaker",
          "affiliation": null
        },
        {
          "name": "Karl Pertsch",
          "affiliation": null
        },
        {
          "name": "Percy Liang",
          "affiliation": null
        },
        {
          "name": "Sergey Levine",
          "affiliation": null
        },
        {
          "name": "Chelsea Finn",
          "affiliation": null
        }
      ],
      "abstract": "A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \\textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \\emph{negative examples data augmentation} pipeline that generates calibrated \\emph{negatives} and \\emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.",
      "publishedDate": "2026-01-02T12:47:34Z",
      "updatedDate": "2026-01-02T12:47:34Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00675v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00675",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "robotics",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00575",
      "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs",
      "authors": [
        {
          "name": "Ishir Garg",
          "affiliation": null
        },
        {
          "name": "Neel Kolhe",
          "affiliation": null
        },
        {
          "name": "Xuandong Zhao",
          "affiliation": null
        },
        {
          "name": "Dawn Song",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/",
      "publishedDate": "2026-01-02T05:26:27Z",
      "updatedDate": "2026-01-02T05:26:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00575v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00575",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00559",
      "title": "Cracking IoT Security: Can LLMs Outsmart Static Analysis Tools?",
      "authors": [
        {
          "name": "Jason Quantrill",
          "affiliation": null
        },
        {
          "name": "Noura Khajehnouri",
          "affiliation": null
        },
        {
          "name": "Zihan Guo",
          "affiliation": null
        },
        {
          "name": "Manar H. Alalfi",
          "affiliation": null
        }
      ],
      "abstract": "Smart home IoT platforms such as openHAB rely on Trigger Action Condition (TAC) rules to automate device behavior, but the interplay among these rules can give rise to interaction threats, unintended or unsafe behaviors emerging from implicit dependencies, conflicting triggers, or overlapping conditions. Identifying these threats requires semantic understanding and structural reasoning that traditionally depend on symbolic, constraint-driven static analysis. This work presents the first comprehensive evaluation of Large Language Models (LLMs) across a multi-category interaction threat taxonomy, assessing their performance on both the original openHAB (oHC/IoTB) dataset and a structurally challenging Mutation dataset designed to test robustness under rule transformations. We benchmark Llama 3.1 8B, Llama 70B, GPT-4o, Gemini-2.5-Pro, and DeepSeek-R1 across zero-, one-, and two-shot settings, comparing their results against oHIT's manually validated ground truth. Our findings show that while LLMs exhibit promising semantic understanding, particularly on action- and condition-related threats, their accuracy degrades significantly for threats requiring cross-rule structural reasoning, especially under mutated rule forms. Model performance varies widely across threat categories and prompt settings, with no model providing consistent reliability. In contrast, the symbolic reasoning baseline maintains stable detection across both datasets, unaffected by rule rewrites or structural perturbations. These results underscore that LLMs alone are not yet dependable for safety critical interaction-threat detection in IoT environments. We discuss the implications for tool design and highlight the potential of hybrid architectures that combine symbolic analysis with LLM-based semantic interpretation to reduce false positives while maintaining structural rigor.",
      "publishedDate": "2026-01-02T04:17:36Z",
      "updatedDate": "2026-01-02T04:17:36Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00559",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00506",
      "title": "Rule-Based Approaches to Atomic Sentence Extraction",
      "authors": [
        {
          "name": "Lineesha Kamana",
          "affiliation": null
        },
        {
          "name": "Akshita Ananda Subramanian",
          "affiliation": null
        },
        {
          "name": "Mehuli Ghosh",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the \"split-and-rephrase\" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.",
      "publishedDate": "2026-01-01T23:19:51Z",
      "updatedDate": "2026-01-01T23:19:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00506v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00506",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00501",
      "title": "CPPO: Contrastive Perception for Vision Language Policy Optimization",
      "authors": [
        {
          "name": "Ahmad Rezaei",
          "affiliation": null
        },
        {
          "name": "Mohsen Gholami",
          "affiliation": null
        },
        {
          "name": "Saeed Ranjbar Alvar",
          "affiliation": null
        },
        {
          "name": "Kevin Cannons",
          "affiliation": null
        },
        {
          "name": "Mohammad Asiful Hossain",
          "affiliation": null
        },
        {
          "name": "Zhou Weimin",
          "affiliation": null
        },
        {
          "name": "Shunbo Zhou",
          "affiliation": null
        },
        {
          "name": "Yong Zhang",
          "affiliation": null
        },
        {
          "name": "Mohammad Akbari",
          "affiliation": null
        }
      ],
      "abstract": "We introduce CPPO, a Contrastive Perception Policy Optimization method for finetuning vision-language models (VLMs). While reinforcement learning (RL) has advanced reasoning in language models, extending it to multimodal reasoning requires improving both the perception and reasoning aspects. Prior works tackle this challenge mainly with explicit perception rewards, but disentangling perception tokens from reasoning tokens is difficult, requiring extra LLMs, ground-truth data, forced separation of perception from reasoning by policy model, or applying rewards indiscriminately to all output tokens. CPPO addresses this problem by detecting perception tokens via entropy shifts in the model outputs under perturbed input images. CPPO then extends the RL objective function with a Contrastive Perception Loss (CPL) that enforces consistency under information-preserving perturbations and sensitivity under information-removing ones. Experiments show that CPPO surpasses previous perception-rewarding methods, while avoiding extra models, making training more efficient and scalable.",
      "publishedDate": "2026-01-01T22:48:26Z",
      "updatedDate": "2026-01-01T22:48:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00501v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00501",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00482",
      "title": "Multi-Agent Coordinated Rename Refactoring",
      "authors": [
        {
          "name": "Abhiram Bellur",
          "affiliation": null
        },
        {
          "name": "Mohammed Raihan Ullah",
          "affiliation": null
        },
        {
          "name": "Fraol Batole",
          "affiliation": null
        },
        {
          "name": "Mohit Kansara",
          "affiliation": null
        },
        {
          "name": "Masaharu Morimoto",
          "affiliation": null
        },
        {
          "name": "Kai Ishikawa",
          "affiliation": null
        },
        {
          "name": "Haifeng Chen",
          "affiliation": null
        },
        {
          "name": "Yaroslav Zharov",
          "affiliation": null
        },
        {
          "name": "Timofey Bryksin",
          "affiliation": null
        },
        {
          "name": "Tien N. Nguyen",
          "affiliation": null
        },
        {
          "name": "Hridesh Rajan",
          "affiliation": null
        },
        {
          "name": "Danny Dig",
          "affiliation": null
        }
      ],
      "abstract": "The primary value of AI agents in software development lies in their ability to extend the developer's capacity for reasoning and action, not to supplant human involvement. To showcase how to use agents working in tandem with developers, we designed a novel approach for carrying out coordinated renaming. Coordinated renaming, where a single rename refactoring triggers refactorings in multiple, related identifiers, is a frequent yet challenging task. Developers must manually propagate these rename refactorings across numerous files and contexts, a process that is both tedious and highly error-prone. State-of-the-art heuristic-based approaches produce an overwhelming number of false positives, while vanilla Large Language Models (LLMs) provide incomplete suggestions due to their limited context and inability to interact with refactoring tools. This leaves developers with incomplete refactorings or burdens them with filtering too many false positives. Coordinated renaming is exactly the kind of repetitive task that agents can significantly reduce the developers' burden while keeping them in the driver's seat. We designed, implemented, and evaluated the first multi-agent framework that automates coordinated renaming. It operates on a key insight: a developer's initial refactoring is a clue to infer the scope of related refactorings. Our Scope Inference Agent first transforms this clue into an explicit, natural-language Declared Scope. The Planned Execution Agent then uses this as a strict plan to identify program elements that should undergo refactoring and safely executes the changes by invoking the IDE's own trusted refactoring APIs. Finally, the Replication Agent uses it to guide the project-wide search. We first conducted a formative study on the practice of coordinated renaming in 609K commits in 100 open-source projects and surveyed 205 developers ...",
      "publishedDate": "2026-01-01T21:29:43Z",
      "updatedDate": "2026-01-01T21:29:43Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00482v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00482",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "multi-agent",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00448",
      "title": "Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games",
      "authors": [
        {
          "name": "Dimitris Vartziotis",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.",
      "publishedDate": "2026-01-01T19:15:17Z",
      "updatedDate": "2026-01-01T19:15:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00448v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00448",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00388",
      "title": "Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach",
      "authors": [
        {
          "name": "Biao Wu",
          "affiliation": null
        },
        {
          "name": "Meng Fang",
          "affiliation": null
        },
        {
          "name": "Ling Chen",
          "affiliation": null
        },
        {
          "name": "Ke Xu",
          "affiliation": null
        },
        {
          "name": "Tao Cheng",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.",
      "publishedDate": "2026-01-01T16:51:41Z",
      "updatedDate": "2026-01-05T18:27:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00388v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00388",
      "comment": "Accepted to AAAI 2026. Project Page: https://github.com/aialt/geo-r",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00364",
      "title": "The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining",
      "authors": [
        {
          "name": "Jiandong Shao",
          "affiliation": null
        },
        {
          "name": "Raphael Tang",
          "affiliation": null
        },
        {
          "name": "Crystina Zhang",
          "affiliation": null
        },
        {
          "name": "Karin Sevegnani",
          "affiliation": null
        },
        {
          "name": "Pontus Stenetorp",
          "affiliation": null
        },
        {
          "name": "Jianfei Yang",
          "affiliation": null
        },
        {
          "name": "Yao Lu",
          "affiliation": null
        }
      ],
      "abstract": "Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.",
      "publishedDate": "2026-01-01T14:52:06Z",
      "updatedDate": "2026-01-01T14:52:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00364v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00364",
      "comment": "under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00339",
      "title": "Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems",
      "authors": [
        {
          "name": "Alaa Saleh",
          "affiliation": null
        },
        {
          "name": "Praveen Kumar Donta",
          "affiliation": null
        },
        {
          "name": "Roberto Morabito",
          "affiliation": null
        },
        {
          "name": "Sasu Tarkoma",
          "affiliation": null
        },
        {
          "name": "Anders Lindgren",
          "affiliation": null
        },
        {
          "name": "Qiyang Zhang",
          "affiliation": null
        },
        {
          "name": "Schahram Dustdar Susanna Pirttikangas",
          "affiliation": null
        },
        {
          "name": "Lauri Lovén",
          "affiliation": null
        }
      ],
      "abstract": "Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.",
      "publishedDate": "2026-01-01T13:30:38Z",
      "updatedDate": "2026-01-01T13:30:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.DC",
        "cs.ET",
        "cs.MA",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00339v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00339",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00269",
      "title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering",
      "authors": [
        {
          "name": "Chaodong Tong",
          "affiliation": null
        },
        {
          "name": "Qi Zhang",
          "affiliation": null
        },
        {
          "name": "Chen Li",
          "affiliation": null
        },
        {
          "name": "Lei Jiang",
          "affiliation": null
        },
        {
          "name": "Yanbing Liu",
          "affiliation": null
        }
      ],
      "abstract": "Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.",
      "publishedDate": "2026-01-01T09:19:39Z",
      "updatedDate": "2026-01-01T09:19:39Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00269v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00269",
      "comment": "14 pages, 9 figures, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00215",
      "title": "From Sight to Insight: Improving Visual Reasoning Capabilities of Multimodal Models via Reinforcement Learning",
      "authors": [
        {
          "name": "Omar Sharif",
          "affiliation": null
        },
        {
          "name": "Eftekhar Hossain",
          "affiliation": null
        },
        {
          "name": "Patrick Ng",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning (RL) has emerged as a promising approach for eliciting reasoning chains before generating final answers. However, multimodal large language models (MLLMs) generate reasoning that lacks integration of visual information. This limits their ability to solve problems that demand accurate visual perception, such as visual puzzles. We show that visual perception is the key bottleneck in such tasks: converting images into textual descriptions significantly improves performance, yielding gains of 26.7% for Claude 3.5 and 23.6% for Claude 3.7. To address this, we investigate reward-driven RL as a mechanism to unlock long visual reasoning in open-source MLLMs without requiring costly supervision. We design and evaluate six reward functions targeting different reasoning aspects, including image understanding, thinking steps, and answer accuracy. Using group relative policy optimization (GRPO), our approach explicitly incentivizes longer, structured reasoning and mitigates bypassing of visual information. Experiments on Qwen-2.5-VL-7B achieve 5.56% improvements over the base model, with consistent gains across both in-domain and out-of-domain settings.",
      "publishedDate": "2026-01-01T05:19:28Z",
      "updatedDate": "2026-01-01T05:19:28Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00215v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00215",
      "comment": "23 pages, 15 Figures, 10 Tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00166",
      "title": "Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description",
      "authors": [
        {
          "name": "Yongmin Yoo",
          "affiliation": null
        },
        {
          "name": "Kris W Pan",
          "affiliation": null
        }
      ],
      "abstract": "Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.",
      "publishedDate": "2026-01-01T02:10:26Z",
      "updatedDate": "2026-01-01T02:10:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00166v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00166",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00150",
      "title": "FCMBench: A Comprehensive Financial Credit Multimodal Benchmark for Real-world Applications",
      "authors": [
        {
          "name": "Yehui Yang",
          "affiliation": null
        },
        {
          "name": "Dalu Yang",
          "affiliation": null
        },
        {
          "name": "Wenshuo Zhou",
          "affiliation": null
        },
        {
          "name": "Fangxin Shang",
          "affiliation": null
        },
        {
          "name": "Yifan Liu",
          "affiliation": null
        },
        {
          "name": "Jie Ren",
          "affiliation": null
        },
        {
          "name": "Haojun Fei",
          "affiliation": null
        },
        {
          "name": "Qing Yang",
          "affiliation": null
        },
        {
          "name": "Yanwu Xu",
          "affiliation": null
        },
        {
          "name": "Tao Chen",
          "affiliation": null
        }
      ],
      "abstract": "As multimodal AI becomes widely used for credit risk assessment and document review, a domain-specific benchmark is urgently needed that (1) reflects documents and workflows specific to financial credit applications, (2) includes credit-specific understanding and real-world robustness, and (3) preserves privacy compliance without sacrificing practical utility. Here, we introduce FCMBench-V1.0 -- a large-scale financial credit multimodal benchmark for real-world applications, covering 18 core certificate types, with 4,043 privacy-compliant images and 8,446 QA samples. The FCMBench evaluation framework consists of three dimensions: Perception, Reasoning, and Robustness, including 3 foundational perception tasks, 4 credit-specific reasoning tasks that require decision-oriented understanding of visual evidence, and 10 real-world acquisition artifact types for robustness stress testing. To reconcile compliance with realism, we construct all samples via a closed synthesis-capture pipeline: we manually synthesize document templates with virtual content and capture scenario-aware images in-house. This design also mitigates pre-training data leakage by avoiding web-sourced or publicly released images. FCMBench can effectively discriminate performance disparities and robustness across modern vision-language models. Extensive experiments were conducted on 23 state-of-the-art vision-language models (VLMs) from 14 top AI companies and research institutes. Among them, Gemini 3 Pro achieves the best F1(\\%) score as a commercial model (64.61), Qwen3-VL-235B achieves the best score as an open-source baseline (57.27), and our financial credit-specific model, Qfin-VL-Instruct, achieves the top overall score (64.92). Robustness evaluations show that even top-performing models suffer noticeable performance drops under acquisition artifacts.",
      "publishedDate": "2026-01-01T00:42:54Z",
      "updatedDate": "2026-01-06T08:08:49Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CE",
        "cs.MM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00150v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00150",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00685",
      "title": "Human-like AI-based Auto-Field-in-Field Whole-Brain Radiotherapy Treatment Planning With Conversation Large Language Model Feedback",
      "authors": [
        {
          "name": "Adnan Jafar",
          "affiliation": null
        },
        {
          "name": "An Qin",
          "affiliation": null
        },
        {
          "name": "Gavin Atkins",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Hu",
          "affiliation": null
        },
        {
          "name": "Yin Gao",
          "affiliation": null
        },
        {
          "name": "Xun Jia",
          "affiliation": null
        }
      ],
      "abstract": "Whole-brain radiotherapy (WBRT) is a common treatment due to its simplicity and effectiveness. While automated Field-in-Field (Auto-FiF) functions assist WBRT planning in modern treatment planning systems, it still requires manual approaches for optimal plan generation including patient-specific hyperparameters definition and plan refinement based on quality feedback. This study introduces an automated WBRT planning pipeline that integrates a deep learning (DL) Hyperparameter Prediction model for patient-specific parameter generation and a large-language model (LLM)-based conversational interface for interactive plan refinement. The Hyperparameter Prediction module was trained on 55 WBRT cases using geometric features of clinical target volume (CTV) and organs at risk (OARs) to determine optimal Auto-FiF settings in RayStation treatment planning system. Plans were generated under predicted hyperparameters. For cases in which the generated plan was suboptimal, quality feedback via voice input was captured by a Conversation module, transcribed using Whisper, and interpreted by GPT-4o to adjust planning settings. Plan quality was evaluated in 15 independent cases using clinical metrics and expert review, and model explainability was supported through analysis of feature importance. Fourteen of 15 DL-generated plans were clinically acceptable. Normalized to identical CTV D95% as the clinical plans, the DL-generated and clinical plans showed no statistically significant differences in doses to the eyes, lenses, or CTV dose metrics D1% and D99%. The DL-based planning required under 1 minute of computation and achieved total workflow execution in approximately 7 minutes with a single mouse click, compared to 15 minutes for manual planning. In cases requiring adjustment, the Conversational module successfully improved dose conformity and hotspot reduction.",
      "publishedDate": "2026-01-02T13:23:12Z",
      "updatedDate": "2026-01-02T13:23:12Z",
      "primaryCategory": "physics.med-ph",
      "arxivCategories": [
        "physics.med-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00685v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00685",
      "comment": "22 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00742",
      "title": "Materials Informatics: Emergence To Autonomous Discovery In The Age Of AI",
      "authors": [
        {
          "name": "Turab Lookman",
          "affiliation": null
        },
        {
          "name": "YuJie Liu",
          "affiliation": null
        },
        {
          "name": "Zhibin Gao",
          "affiliation": null
        }
      ],
      "abstract": "This perspective explores the evolution of materials informatics, from its foundational roots in physics and information theory to its maturation through artificial intelligence (AI). We trace the field's trajectory from early milestones to the transformative impact of the Materials Genome Initiative and the recent advent of large language models (LLMs). Rather than a mere toolkit, we present materials informatics as an evolving ecosystem, reviewing key methodologies such as Bayesian Optimization, Reinforcement Learning, and Transformers that drive inverse design and autonomous self-driving laboratories. We specifically address the practical challenges of LLM integration, comparing specialist versus generalist models and discussing solutions for uncertainty quantification. Looking forward, we assess the transition of AI from a predictive tool to a collaborative research partner. By leveraging active learning and retrieval-augmented generation (RAG), the field is moving toward a new era of autonomous materials science, increasingly characterized by \"human-out-of-the-loop\" discovery processes.",
      "publishedDate": "2026-01-02T16:53:19Z",
      "updatedDate": "2026-01-02T16:53:19Z",
      "primaryCategory": "physics.comp-ph",
      "arxivCategories": [
        "physics.comp-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00742v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00742",
      "comment": "44 pages, 14 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "agents"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00536",
      "title": "Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends",
      "authors": [
        {
          "name": "Yuelyu Ji",
          "affiliation": null
        },
        {
          "name": "Zhuochun Li",
          "affiliation": null
        },
        {
          "name": "Rui Meng",
          "affiliation": null
        },
        {
          "name": "Daqing He",
          "affiliation": null
        }
      ],
      "abstract": "Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \\emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.",
      "publishedDate": "2026-01-02T02:38:01Z",
      "updatedDate": "2026-01-02T02:38:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00536v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00536",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00526",
      "title": "Federated Customization of Large Models: Approaches, Experiments, and Insights",
      "authors": [
        {
          "name": "Yuchuan Ye",
          "affiliation": null
        },
        {
          "name": "Ming Ding",
          "affiliation": null
        },
        {
          "name": "Youjia Chen",
          "affiliation": null
        },
        {
          "name": "Peng Cheng",
          "affiliation": null
        },
        {
          "name": "Dusit Niyato",
          "affiliation": null
        }
      ],
      "abstract": "In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.",
      "publishedDate": "2026-01-02T01:45:52Z",
      "updatedDate": "2026-01-02T01:45:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.DC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00526v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00526",
      "comment": "8 pages, 1 figure",
      "journalRef": null,
      "doi": "10.1109/MNET.2025.3648812",
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00509",
      "title": "Improving LLM-Assisted Secure Code Generation through Retrieval-Augmented-Generation and Multi-Tool Feedback",
      "authors": [
        {
          "name": "Vidyut Sriram",
          "affiliation": null
        },
        {
          "name": "Sawan Pandita",
          "affiliation": null
        },
        {
          "name": "Achintya Lakshmanan",
          "affiliation": null
        },
        {
          "name": "Aneesh Shamraj",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can generate code but often introduce security vulnerabilities, logical inconsistencies, and compilation errors. Prior work demonstrates that LLMs benefit substantially from structured feedback, static analysis, retrieval augmentation, and execution-based refinement. We propose a retrieval-augmented, multi-tool repair workflow in which a single code-generating LLM iteratively refines its outputs using compiler diagnostics, CodeQL security scanning, and KLEE symbolic execution. A lightweight embedding model is used for semantic retrieval of previously successful repairs, providing security-focused examples that guide generation. Evaluated on a combined dataset of 3,242 programs generated by DeepSeek-Coder-1.3B and CodeLlama-7B, the system demonstrates significant improvements in robustness. For DeepSeek, security vulnerabilities were reduced by 96%. For the larger CodeLlama model, the critical security defect rate was decreased from 58.55% to 22.19%, highlighting the efficacy of tool-assisted self-repair even on \"stubborn\" models.",
      "publishedDate": "2026-01-01T23:34:00Z",
      "updatedDate": "2026-01-01T23:34:00Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00509v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00509",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00376",
      "title": "In Line with Context: Repository-Level Code Generation via Context Inlining",
      "authors": [
        {
          "name": "Chao Hu",
          "affiliation": null
        },
        {
          "name": "Wenhao Zeng",
          "affiliation": null
        },
        {
          "name": "Yuling Shi",
          "affiliation": null
        },
        {
          "name": "Beijun Shen",
          "affiliation": null
        },
        {
          "name": "Xiaodong Gu",
          "affiliation": null
        }
      ],
      "abstract": "Repository-level code generation has attracted growing attention in recent years. Unlike function-level code generation, it requires the model to understand the entire repository, reasoning over complex dependencies across functions, classes, and modules. However, existing approaches such as retrieval-augmented generation (RAG) or context-based function selection often fall short: they primarily rely on surface-level similarity and struggle to capture the rich dependencies that govern repository-level semantics. In this paper, we introduce InlineCoder, a novel framework for repository-level code generation. InlineCoder enhances the understanding of repository context by inlining the unfinished function into its call graph, thereby reframing the challenging repository understanding as an easier function-level coding task. Given a function signature, InlineCoder first generates a draft completion, termed an anchor, which approximates downstream dependencies and enables perplexity-based confidence estimation. This anchor drives a bidirectional inlining process: (i) Upstream Inlining, which embeds the anchor into its callers to capture diverse usage scenarios; and (ii) Downstream Retrieval, which integrates the anchor's callees into the prompt to provide precise dependency context. The enriched context, combining draft completion with upstream and downstream perspectives, equips the LLM with a comprehensive repository view.",
      "publishedDate": "2026-01-01T15:56:24Z",
      "updatedDate": "2026-01-01T15:56:24Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00376v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00376",
      "comment": "Accepted to FSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "code-generation",
        "rag",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00216",
      "title": "From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark",
      "authors": [
        {
          "name": "Jinning Zhang",
          "affiliation": null
        },
        {
          "name": "Jie Song",
          "affiliation": null
        },
        {
          "name": "Wenhui Tu",
          "affiliation": null
        },
        {
          "name": "Zecheng Li",
          "affiliation": null
        },
        {
          "name": "Jingxuan Li",
          "affiliation": null
        },
        {
          "name": "Jin Li",
          "affiliation": null
        },
        {
          "name": "Xuan Liu",
          "affiliation": null
        },
        {
          "name": "Taole Sha",
          "affiliation": null
        },
        {
          "name": "Zichen Wei",
          "affiliation": null
        },
        {
          "name": "Yan Li",
          "affiliation": null
        }
      ],
      "abstract": "In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.",
      "publishedDate": "2026-01-01T05:20:54Z",
      "updatedDate": "2026-01-01T05:20:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00216v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00216",
      "comment": "35 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00641",
      "title": "Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs",
      "authors": [
        {
          "name": "Nils Rautenberg",
          "affiliation": null
        },
        {
          "name": "Sven Schippkus",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting. We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer. Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.",
      "publishedDate": "2026-01-02T10:52:33Z",
      "updatedDate": "2026-01-02T10:52:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00641v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00641",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00624",
      "title": "Do Chatbot LLMs Talk Too Much? The YapBench Benchmark",
      "authors": [
        {
          "name": "Vadim Borisov",
          "affiliation": null
        },
        {
          "name": "Michael Gröger",
          "affiliation": null
        },
        {
          "name": "Mina Mikhael",
          "affiliation": null
        },
        {
          "name": "Richard H. Schreiber",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality. We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores. YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.",
      "publishedDate": "2026-01-02T09:43:52Z",
      "updatedDate": "2026-01-02T09:43:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00624v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00624",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "tool-use",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00504",
      "title": "MotionPhysics: Learnable Motion Distillation for Text-Guided Simulation",
      "authors": [
        {
          "name": "Miaowei Wang",
          "affiliation": null
        },
        {
          "name": "Jakub Zadrożny",
          "affiliation": null
        },
        {
          "name": "Oisin Mac Aodha",
          "affiliation": null
        },
        {
          "name": "Amir Vaxman",
          "affiliation": null
        }
      ],
      "abstract": "Accurately simulating existing 3D objects and a wide variety of materials often demands expert knowledge and time-consuming physical parameter tuning to achieve the desired dynamic behavior. We introduce MotionPhysics, an end-to-end differentiable framework that infers plausible physical parameters from a user-provided natural language prompt for a chosen 3D scene of interest, removing the need for guidance from ground-truth trajectories or annotated videos. Our approach first utilizes a multimodal large language model to estimate material parameter values, which are constrained to lie within plausible ranges. We further propose a learnable motion distillation loss that extracts robust motion priors from pretrained video diffusion models while minimizing appearance and geometry inductive biases to guide the simulation. We evaluate MotionPhysics across more than thirty scenarios, including real-world, human-designed, and AI-generated 3D objects, spanning a wide range of materials such as elastic solids, metals, foams, sand, and both Newtonian and non-Newtonian fluids. We demonstrate that MotionPhysics produces visually realistic dynamic simulations guided by natural language, surpassing the state of the art while automatically determining physically plausible parameters. The code and project page are available at: https://wangmiaowei.github.io/MotionPhysics.github.io/.",
      "publishedDate": "2026-01-01T22:56:37Z",
      "updatedDate": "2026-01-01T22:56:37Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00504v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00504",
      "comment": "AAAI2026 Accepted",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00469",
      "title": "DSL or Code? Evaluating the Quality of LLM-Generated Algebraic Specifications: A Case Study in Optimization at Kinaxis",
      "authors": [
        {
          "name": "Negin Ayoughi",
          "affiliation": null
        },
        {
          "name": "David Dewar",
          "affiliation": null
        },
        {
          "name": "Shiva Nejati",
          "affiliation": null
        },
        {
          "name": "Mehrdad Sabetzadeh",
          "affiliation": null
        }
      ],
      "abstract": "Model-driven engineering (MDE) provides abstraction and analytical rigour, but industrial adoption in many domains has been limited by the cost of developing and maintaining models. Large language models (LLMs) can help shift this cost balance by supporting direct generation of models from natural-language (NL) descriptions. For domain-specific languages (DSLs), however, LLM-generated models may be less accurate than LLM-generated code in mainstream languages such as Python, due to the latter's dominance in LLM training corpora. We investigate this issue in mathematical optimization, with AMPL, a DSL with established industrial use. We introduce EXEOS, an LLM-based approach that derives AMPL models and Python code from NL problem descriptions and iteratively refines them with solver feedback. Using a public optimization dataset and real-world supply-chain cases from our industrial partner Kinaxis, we evaluate generated AMPL models against Python code in terms of executability and correctness. An ablation study with two LLM families shows that AMPL is competitive with, and sometimes better than, Python, and that our design choices in EXEOS improve the quality of generated specifications.",
      "publishedDate": "2026-01-01T20:48:15Z",
      "updatedDate": "2026-01-05T17:09:37Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00469v2",
      "arxivUrl": "https://arxiv.org/abs/2601.00469",
      "comment": "Accepted for publication in ICSE-SEIP 2026",
      "journalRef": null,
      "doi": "10.1145/3786583.3786879",
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00224",
      "title": "Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback",
      "authors": [
        {
          "name": "Yan Sun",
          "affiliation": null
        },
        {
          "name": "Ming Cai",
          "affiliation": null
        },
        {
          "name": "Stanley Kok",
          "affiliation": null
        }
      ],
      "abstract": "As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.",
      "publishedDate": "2026-01-01T06:10:06Z",
      "updatedDate": "2026-01-01T06:10:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00224v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00224",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-05T03:50:02.658Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02298",
      "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)",
      "authors": [
        {
          "name": "Mahmoud Elgenedy",
          "affiliation": null
        }
      ],
      "abstract": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.",
      "publishedDate": "2026-01-05T17:33:16Z",
      "updatedDate": "2026-01-05T17:33:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "eess.SP"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.02186",
      "title": "Toward Global Large Language Models in Medicine",
      "authors": [
        {
          "name": "Rui Yang",
          "affiliation": null
        },
        {
          "name": "Huitao Li",
          "affiliation": null
        },
        {
          "name": "Weihao Xuan",
          "affiliation": null
        },
        {
          "name": "Heli Qi",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Kunyu Yu",
          "affiliation": null
        },
        {
          "name": "Yingjian Chen",
          "affiliation": null
        },
        {
          "name": "Rongrong Wang",
          "affiliation": null
        },
        {
          "name": "Jacques Behmoaras",
          "affiliation": null
        },
        {
          "name": "Tianxi Cai",
          "affiliation": null
        },
        {
          "name": "Bibhas Chakraborty",
          "affiliation": null
        },
        {
          "name": "Qingyu Chen",
          "affiliation": null
        },
        {
          "name": "Lionel Tim-Ee Cheng",
          "affiliation": null
        },
        {
          "name": "Marie-Louise Damwanza",
          "affiliation": null
        },
        {
          "name": "Chido Dzinotyiwei",
          "affiliation": null
        },
        {
          "name": "Aosong Feng",
          "affiliation": null
        },
        {
          "name": "Chuan Hong",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yuhe Ke",
          "affiliation": null
        },
        {
          "name": "Linah Kitala",
          "affiliation": null
        },
        {
          "name": "Taehoon Ko",
          "affiliation": null
        },
        {
          "name": "Jisan Lee",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        },
        {
          "name": "Jonathan Chong Kai Liew",
          "affiliation": null
        },
        {
          "name": "Hongfang Liu",
          "affiliation": null
        },
        {
          "name": "Lian Leng Low",
          "affiliation": null
        },
        {
          "name": "Edison Marrese-Taylor",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Isheanesu Misi",
          "affiliation": null
        },
        {
          "name": "Yilin Ning",
          "affiliation": null
        },
        {
          "name": "Jasmine Chiat Ling Ong",
          "affiliation": null
        },
        {
          "name": "Marcus Eng Hock Ong",
          "affiliation": null
        },
        {
          "name": "Enrico Petretto",
          "affiliation": null
        },
        {
          "name": "Hossein Rouhizadeh",
          "affiliation": null
        },
        {
          "name": "Abiram Sandralegar",
          "affiliation": null
        },
        {
          "name": "Oren Schreier",
          "affiliation": null
        },
        {
          "name": "Iain Bee Huat Tan",
          "affiliation": null
        },
        {
          "name": "Patrick Tan",
          "affiliation": null
        },
        {
          "name": "Daniel Shu Wei Ting",
          "affiliation": null
        },
        {
          "name": "Junjue Wang",
          "affiliation": null
        },
        {
          "name": "Chunhua Weng",
          "affiliation": null
        },
        {
          "name": "Matthew Yu Heng Wong",
          "affiliation": null
        },
        {
          "name": "Fang Wu",
          "affiliation": null
        },
        {
          "name": "Yunze Xiao",
          "affiliation": null
        },
        {
          "name": "Xuhai Xu",
          "affiliation": null
        },
        {
          "name": "Qingcheng Zeng",
          "affiliation": null
        },
        {
          "name": "Zhuo Zheng",
          "affiliation": null
        },
        {
          "name": "Yifan Peng",
          "affiliation": null
        },
        {
          "name": "Douglas Teodoro",
          "affiliation": null
        },
        {
          "name": "Nan Liu",
          "affiliation": null
        }
      ],
      "abstract": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.",
      "publishedDate": "2026-01-05T15:05:49Z",
      "updatedDate": "2026-01-05T15:05:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02186v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02186",
      "comment": "182 pages, 65 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02179",
      "title": "Confidence Estimation for LLMs in Multi-turn Interactions",
      "authors": [
        {
          "name": "Caiqi Zhang",
          "affiliation": null
        },
        {
          "name": "Ruihan Yang",
          "affiliation": null
        },
        {
          "name": "Xiaochen Zhu",
          "affiliation": null
        },
        {
          "name": "Chengzu Li",
          "affiliation": null
        },
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Yijiang River Dong",
          "affiliation": null
        },
        {
          "name": "Deqing Yang",
          "affiliation": null
        },
        {
          "name": "Nigel Collier",
          "affiliation": null
        }
      ],
      "abstract": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.",
      "publishedDate": "2026-01-05T14:58:04Z",
      "updatedDate": "2026-01-05T14:58:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02179v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02179",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02078",
      "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot",
      "authors": [
        {
          "name": "Chenghao Yin",
          "affiliation": null
        },
        {
          "name": "Da Huang",
          "affiliation": null
        },
        {
          "name": "Di Yang",
          "affiliation": null
        },
        {
          "name": "Jichao Wang",
          "affiliation": null
        },
        {
          "name": "Nanshu Zhao",
          "affiliation": null
        },
        {
          "name": "Chen Xu",
          "affiliation": null
        },
        {
          "name": "Wenjun Sun",
          "affiliation": null
        },
        {
          "name": "Linjie Hou",
          "affiliation": null
        },
        {
          "name": "Zhijun Li",
          "affiliation": null
        },
        {
          "name": "Junhui Wu",
          "affiliation": null
        },
        {
          "name": "Zhaobo Liu",
          "affiliation": null
        },
        {
          "name": "Zhen Xiao",
          "affiliation": null
        },
        {
          "name": "Sheng Zhang",
          "affiliation": null
        },
        {
          "name": "Lei Bao",
          "affiliation": null
        },
        {
          "name": "Rui Feng",
          "affiliation": null
        },
        {
          "name": "Zhenquan Pang",
          "affiliation": null
        },
        {
          "name": "Jiayu Li",
          "affiliation": null
        },
        {
          "name": "Qian Wang",
          "affiliation": null
        },
        {
          "name": "Maoqing Yao",
          "affiliation": null
        }
      ],
      "abstract": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.",
      "publishedDate": "2026-01-05T12:59:39Z",
      "updatedDate": "2026-01-05T12:59:39Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02078v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02078",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "prompting",
        "tool-use",
        "rag",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "tool-use",
          "rag",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02071",
      "title": "FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations",
      "authors": [
        {
          "name": "Adeshola Okubena",
          "affiliation": null
        },
        {
          "name": "Yusuf Ali Mohammed",
          "affiliation": null
        },
        {
          "name": "Moe Elbadawi",
          "affiliation": null
        }
      ],
      "abstract": "Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.",
      "publishedDate": "2026-01-05T12:50:50Z",
      "updatedDate": "2026-01-07T10:02:54Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02071v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02071",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01925",
      "title": "AR-MOT: Autoregressive Multi-object Tracking",
      "authors": [
        {
          "name": "Lianjie Jia",
          "affiliation": null
        },
        {
          "name": "Yuhan Wu",
          "affiliation": null
        },
        {
          "name": "Binghao Ran",
          "affiliation": null
        },
        {
          "name": "Yifan Wang",
          "affiliation": null
        },
        {
          "name": "Lijun Wang",
          "affiliation": null
        },
        {
          "name": "Huchuan Lu",
          "affiliation": null
        }
      ],
      "abstract": "As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.",
      "publishedDate": "2026-01-05T09:17:28Z",
      "updatedDate": "2026-01-05T09:17:28Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01925v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01925",
      "comment": "12 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01868",
      "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs",
      "authors": [
        {
          "name": "Jinghan Ru",
          "affiliation": null
        },
        {
          "name": "Siyuan Yan",
          "affiliation": null
        },
        {
          "name": "Yuguo Yin",
          "affiliation": null
        },
        {
          "name": "Yuexian Zou",
          "affiliation": null
        },
        {
          "name": "Zongyuan Ge",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.",
      "publishedDate": "2026-01-05T07:55:36Z",
      "updatedDate": "2026-01-05T07:55:36Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01868v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01868",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01836",
      "title": "COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs",
      "authors": [
        {
          "name": "Dasol Choi",
          "affiliation": null
        },
        {
          "name": "DongGeon Lee",
          "affiliation": null
        },
        {
          "name": "Brigitta Jesica Kartono",
          "affiliation": null
        },
        {
          "name": "Helena Berndt",
          "affiliation": null
        },
        {
          "name": "Taeyoun Kwon",
          "affiliation": null
        },
        {
          "name": "Joonwon Jang",
          "affiliation": null
        },
        {
          "name": "Haon Park",
          "affiliation": null
        },
        {
          "name": "Hwanjo Yu",
          "affiliation": null
        },
        {
          "name": "Minsuk Kahng",
          "affiliation": null
        }
      ],
      "abstract": "As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.",
      "publishedDate": "2026-01-05T06:57:45Z",
      "updatedDate": "2026-01-05T06:57:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01836v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01836",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01768",
      "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation",
      "authors": [
        {
          "name": "Meiman Xiao",
          "affiliation": null
        },
        {
          "name": "Ante Wang",
          "affiliation": null
        },
        {
          "name": "Qingguo Hu",
          "affiliation": null
        },
        {
          "name": "Zhongjian Miao",
          "affiliation": null
        },
        {
          "name": "Huangjun Shen",
          "affiliation": null
        },
        {
          "name": "Longyue Wang",
          "affiliation": null
        },
        {
          "name": "Weihua Luo",
          "affiliation": null
        },
        {
          "name": "Jinsong Su",
          "affiliation": null
        }
      ],
      "abstract": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure their response lengths, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.",
      "publishedDate": "2026-01-05T03:49:14Z",
      "updatedDate": "2026-01-07T11:47:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01768v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01768",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01718",
      "title": "Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications",
      "authors": [
        {
          "name": "YuanLab. ai",
          "affiliation": null
        },
        {
          "name": ":",
          "affiliation": null
        },
        {
          "name": "Shawn Wu",
          "affiliation": null
        },
        {
          "name": "Sean Wang",
          "affiliation": null
        },
        {
          "name": "Louie Li",
          "affiliation": null
        },
        {
          "name": "Darcy Chen",
          "affiliation": null
        },
        {
          "name": "Allen Wang",
          "affiliation": null
        },
        {
          "name": "Jiangang Luo",
          "affiliation": null
        },
        {
          "name": "Xudong Zhao",
          "affiliation": null
        },
        {
          "name": "Joseph Shen",
          "affiliation": null
        },
        {
          "name": "Gawain Ma",
          "affiliation": null
        },
        {
          "name": "Jasper Jia",
          "affiliation": null
        },
        {
          "name": "Marcus Mao",
          "affiliation": null
        },
        {
          "name": "Claire Wang",
          "affiliation": null
        },
        {
          "name": "Hunter He",
          "affiliation": null
        },
        {
          "name": "Carol Wang",
          "affiliation": null
        },
        {
          "name": "Zera Zhang",
          "affiliation": null
        },
        {
          "name": "Jason Wang",
          "affiliation": null
        },
        {
          "name": "Chonly Shen",
          "affiliation": null
        },
        {
          "name": "Leo Zhang",
          "affiliation": null
        },
        {
          "name": "Logan Chen",
          "affiliation": null
        },
        {
          "name": "Qasim Meng",
          "affiliation": null
        },
        {
          "name": "James Gong",
          "affiliation": null
        },
        {
          "name": "Danied Zhao",
          "affiliation": null
        },
        {
          "name": "Penn Zheng",
          "affiliation": null
        },
        {
          "name": "Owen Zhu",
          "affiliation": null
        },
        {
          "name": "Tong Yu",
          "affiliation": null
        }
      ],
      "abstract": "We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.",
      "publishedDate": "2026-01-05T01:44:09Z",
      "updatedDate": "2026-01-05T01:44:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01718v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01718",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01609",
      "title": "Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration",
      "authors": [
        {
          "name": "Albert Sadowski",
          "affiliation": null
        },
        {
          "name": "Jarosław A. Chudziak",
          "affiliation": null
        }
      ],
      "abstract": "Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.",
      "publishedDate": "2026-01-04T17:19:20Z",
      "updatedDate": "2026-01-04T17:19:20Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01609v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01609",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01592",
      "title": "OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs",
      "authors": [
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Yunhao Chen",
          "affiliation": null
        },
        {
          "name": "Juncheng Li",
          "affiliation": null
        },
        {
          "name": "Yixu Wang",
          "affiliation": null
        },
        {
          "name": "Yang Yao",
          "affiliation": null
        },
        {
          "name": "Tianle Gu",
          "affiliation": null
        },
        {
          "name": "Jie Li",
          "affiliation": null
        },
        {
          "name": "Yan Teng",
          "affiliation": null
        },
        {
          "name": "Xingjun Ma",
          "affiliation": null
        },
        {
          "name": "Yingchun Wang",
          "affiliation": null
        },
        {
          "name": "Xia Hu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.",
      "publishedDate": "2026-01-04T16:41:33Z",
      "updatedDate": "2026-01-04T16:41:33Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01592v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01592",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01477",
      "title": "Can Legislation Be Made Machine-Readable in PROLEG?",
      "authors": [
        {
          "name": "May-Myo Zin",
          "affiliation": null
        },
        {
          "name": "Sabine Wehnert",
          "affiliation": null
        },
        {
          "name": "Yuntao Kong",
          "affiliation": null
        },
        {
          "name": "Ha-Thanh Nguyen",
          "affiliation": null
        },
        {
          "name": "Wachara Fungwacharakorn",
          "affiliation": null
        },
        {
          "name": "Jieying Xue",
          "affiliation": null
        },
        {
          "name": "Michał Araszkiewicz",
          "affiliation": null
        },
        {
          "name": "Randy Goebel",
          "affiliation": null
        },
        {
          "name": "Ken Satoh",
          "affiliation": null
        },
        {
          "name": "Le-Minh Nguyen",
          "affiliation": null
        }
      ],
      "abstract": "The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to \"compile\" natural language text to if-then rules, then to further \"compile\" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.",
      "publishedDate": "2026-01-04T10:53:16Z",
      "updatedDate": "2026-01-04T10:53:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01477v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01477",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01432",
      "title": "Personalizing black-box models for nonparametric regression with minimax optimality",
      "authors": [
        {
          "name": "Sai Li",
          "affiliation": null
        },
        {
          "name": "Linjun Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.",
      "publishedDate": "2026-01-04T08:32:28Z",
      "updatedDate": "2026-01-04T08:32:28Z",
      "primaryCategory": "stat.ME",
      "arxivCategories": [
        "stat.ME",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01432v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01432",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01378",
      "title": "Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification",
      "authors": [
        {
          "name": "Han Yuan",
          "affiliation": null
        },
        {
          "name": "Yilin Wu",
          "affiliation": null
        },
        {
          "name": "Li Zhang",
          "affiliation": null
        },
        {
          "name": "Zheng Ma",
          "affiliation": null
        }
      ],
      "abstract": "Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.",
      "publishedDate": "2026-01-04T05:09:11Z",
      "updatedDate": "2026-01-04T05:09:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01378v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01378",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01321",
      "title": "Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models",
      "authors": [
        {
          "name": "Rong Zhou",
          "affiliation": null
        },
        {
          "name": "Dongping Chen",
          "affiliation": null
        },
        {
          "name": "Zihan Jia",
          "affiliation": null
        },
        {
          "name": "Yao Su",
          "affiliation": null
        },
        {
          "name": "Yixin Liu",
          "affiliation": null
        },
        {
          "name": "Yiwen Lu",
          "affiliation": null
        },
        {
          "name": "Dongwei Shi",
          "affiliation": null
        },
        {
          "name": "Yue Huang",
          "affiliation": null
        },
        {
          "name": "Tianyang Xu",
          "affiliation": null
        },
        {
          "name": "Yi Pan",
          "affiliation": null
        },
        {
          "name": "Xinliang Li",
          "affiliation": null
        },
        {
          "name": "Yohannes Abate",
          "affiliation": null
        },
        {
          "name": "Qingyu Chen",
          "affiliation": null
        },
        {
          "name": "Zhengzhong Tu",
          "affiliation": null
        },
        {
          "name": "Yu Yang",
          "affiliation": null
        },
        {
          "name": "Yu Zhang",
          "affiliation": null
        },
        {
          "name": "Qingsong Wen",
          "affiliation": null
        },
        {
          "name": "Gengchen Mai",
          "affiliation": null
        },
        {
          "name": "Sunyang Fu",
          "affiliation": null
        },
        {
          "name": "Jiachen Li",
          "affiliation": null
        },
        {
          "name": "Xuyu Wang",
          "affiliation": null
        },
        {
          "name": "Ziran Wang",
          "affiliation": null
        },
        {
          "name": "Jing Huang",
          "affiliation": null
        },
        {
          "name": "Tianming Liu",
          "affiliation": null
        },
        {
          "name": "Yong Chen",
          "affiliation": null
        },
        {
          "name": "Lichao Sun",
          "affiliation": null
        },
        {
          "name": "Lifang He",
          "affiliation": null
        }
      ],
      "abstract": "Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.",
      "publishedDate": "2026-01-04T01:17:09Z",
      "updatedDate": "2026-01-04T01:17:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01321v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01321",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "robotics",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "robotics",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01260",
      "title": "MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance",
      "authors": [
        {
          "name": "Hamad Khan",
          "affiliation": null
        },
        {
          "name": "Saddam Hussain Khan",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.",
      "publishedDate": "2026-01-03T19:01:33Z",
      "updatedDate": "2026-01-03T19:01:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01260v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01260",
      "comment": "28 Pages, Tables 12, Figure 09",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.01239",
      "title": "IO-RAE: Information-Obfuscation Reversible Adversarial Example for Audio Privacy Protection",
      "authors": [
        {
          "name": "Jiajie Zhu",
          "affiliation": null
        },
        {
          "name": "Xia Du",
          "affiliation": null
        },
        {
          "name": "Xiaoyuan Liu",
          "affiliation": null
        },
        {
          "name": "Jizhe Zhou",
          "affiliation": null
        },
        {
          "name": "Qizhen Xu",
          "affiliation": null
        },
        {
          "name": "Zheng Lin",
          "affiliation": null
        },
        {
          "name": "Chi-Man Pun",
          "affiliation": null
        }
      ],
      "abstract": "The rapid advancements in artificial intelligence have significantly accelerated the adoption of speech recognition technology, leading to its widespread integration across various applications. However, this surge in usage also highlights a critical issue: audio data is highly vulnerable to unauthorized exposure and analysis, posing significant privacy risks for businesses and individuals. This paper introduces an Information-Obfuscation Reversible Adversarial Example (IO-RAE) framework, the pioneering method designed to safeguard audio privacy using reversible adversarial examples. IO-RAE leverages large language models to generate misleading yet contextually coherent content, effectively preventing unauthorized eavesdropping by humans and Automatic Speech Recognition (ASR) systems. Additionally, we propose the Cumulative Signal Attack technique, which mitigates high-frequency noise and enhances attack efficacy by targeting low-frequency signals. Our approach ensures the protection of audio data without degrading its quality or our ability. Experimental evaluations demonstrate the superiority of our method, achieving a targeted misguidance rate of 96.5% and a remarkable 100% untargeted misguidance rate in obfuscating target keywords across multiple ASR models, including a commercial black-box system from Google. Furthermore, the quality of the recovered audio, measured by the Perceptual Evaluation of Speech Quality score, reached 4.45, comparable to high-quality original recordings. Notably, the recovered audio processed by ASR systems exhibited an error rate of 0%, indicating nearly lossless recovery. These results highlight the practical applicability and effectiveness of our IO-RAE framework in protecting sensitive audio privacy.",
      "publishedDate": "2026-01-03T17:08:35Z",
      "updatedDate": "2026-01-03T17:08:35Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.CR",
        "cs.MM",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01239v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01239",
      "comment": "10 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "tool-use",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01153",
      "title": "SongSage: A Large Musical Language Model with Lyric Generative Pre-training",
      "authors": [
        {
          "name": "Jiani Guo",
          "affiliation": null
        },
        {
          "name": "Jiajia Li",
          "affiliation": null
        },
        {
          "name": "Jie Wu",
          "affiliation": null
        },
        {
          "name": "Zuchao Li",
          "affiliation": null
        },
        {
          "name": "Yujiu Yang",
          "affiliation": null
        },
        {
          "name": "Ping Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.",
      "publishedDate": "2026-01-03T10:54:37Z",
      "updatedDate": "2026-01-03T10:54:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01153v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01153",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00995",
      "title": "Grain-Aware Data Transformations: Type-Level Formal Verification at Zero Computational Cost",
      "authors": [
        {
          "name": "Nikos Karayannidis",
          "affiliation": null
        }
      ],
      "abstract": "Data transformation correctness is a major challenge in data engineering: how to verify pipeline accuracy before deployment. Traditional methods involve costly iterative testing, data materialization, and manual error detection, due to the lack of formal approaches to reasoning about data granularity (grain), which can shift during transformations, causing issues like fan traps (metrics duplication) and chasm traps (data loss). We introduce the first formal, mathematical definition of grain, extending it from an informal concept in dimensional modeling to a universal, type-theoretic framework applicable to any data type. Encoding grain into the type system allows compile-time verification of transformation correctness, shifting validation from runtime. We define three core grain relations-equality, ordering, and incomparability-and prove a general grain inference theorem that computes the output grain of equi-joins from input grains using type-level operations. This covers all join scenarios, including comparable and incomparable keys. Together with inference rules for relational operations, this enables verification through schema analysis alone, at zero cost. Our approach allows engineers to verify that entire pipeline DAGs maintain correctness properties, detecting grain-related errors such as fan traps, chasm traps, and aggregation issues before data processing. It emphasizes the importance of grain, focusing on critical characteristics rather than all data details. We provide machine-checked formal proofs in Lean 4, reducing verification costs by 98-99%. Additionally, large language models can automatically generate correctness proofs, shifting human effort from proof writing to proof verification, thus democratizing formal methods in data engineering and supporting confident deployment of AI-generated pipelines with machine-checkable guarantees.",
      "publishedDate": "2026-01-02T22:26:31Z",
      "updatedDate": "2026-01-02T22:26:31Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00995v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00995",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00942",
      "title": "Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures",
      "authors": [
        {
          "name": "Kabir Grover",
          "affiliation": null
        }
      ],
      "abstract": "The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.",
      "publishedDate": "2026-01-02T18:10:10Z",
      "updatedDate": "2026-01-02T18:10:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00942v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00942",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00936",
      "title": "Emoji-Based Jailbreaking of Large Language Models",
      "authors": [
        {
          "name": "M P V S Gopinadh",
          "affiliation": null
        },
        {
          "name": "S Mahaboob Hussain",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.",
      "publishedDate": "2026-01-02T10:49:06Z",
      "updatedDate": "2026-01-02T10:49:06Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00936v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00936",
      "comment": "7 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02314",
      "title": "Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents",
      "authors": [
        {
          "name": "Sourena Khanzadeh",
          "affiliation": null
        }
      ],
      "abstract": "As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \\textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \\textbf{faithful} generative drivers of the model's output or merely \\textbf{post-hoc rationalizations}. We introduce \\textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \\textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \\textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \\textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \\textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as \"Reasoning Theater\" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.",
      "publishedDate": "2026-01-05T18:05:29Z",
      "updatedDate": "2026-01-05T18:05:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02314v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02314",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01885",
      "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents",
      "authors": [
        {
          "name": "Yi Yu",
          "affiliation": null
        },
        {
          "name": "Liuyi Yao",
          "affiliation": null
        },
        {
          "name": "Yuexiang Xie",
          "affiliation": null
        },
        {
          "name": "Qingquan Tan",
          "affiliation": null
        },
        {
          "name": "Jiaqi Feng",
          "affiliation": null
        },
        {
          "name": "Yaliang Li",
          "affiliation": null
        },
        {
          "name": "Libing Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.",
      "publishedDate": "2026-01-05T08:24:16Z",
      "updatedDate": "2026-01-05T08:24:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01885v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01885",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01522",
      "title": "Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making",
      "authors": [
        {
          "name": "Danial Amin",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds \"confidence,\" and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.",
      "publishedDate": "2026-01-04T13:19:27Z",
      "updatedDate": "2026-01-04T13:19:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.ET"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01522v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01522",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01498",
      "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents",
      "authors": [
        {
          "name": "Bingguang Hao",
          "affiliation": null
        },
        {
          "name": "Zengzhuang Xu",
          "affiliation": null
        },
        {
          "name": "Yuntao Wen",
          "affiliation": null
        },
        {
          "name": "Xinyi Xu",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        },
        {
          "name": "Tong Zhao",
          "affiliation": null
        },
        {
          "name": "Maolin Wang",
          "affiliation": null
        },
        {
          "name": "Long Chen",
          "affiliation": null
        },
        {
          "name": "Dong Wang",
          "affiliation": null
        },
        {
          "name": "Yicheng Chen",
          "affiliation": null
        },
        {
          "name": "Cunyin Peng",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhao",
          "affiliation": null
        },
        {
          "name": "Chenyi Zhuang",
          "affiliation": null
        },
        {
          "name": "Ji Zhang",
          "affiliation": null
        }
      ],
      "abstract": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.",
      "publishedDate": "2026-01-04T11:56:33Z",
      "updatedDate": "2026-01-04T11:56:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01498v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01498",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "tool-use",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01357",
      "title": "Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows",
      "authors": [
        {
          "name": "Ke Xiao",
          "affiliation": null
        },
        {
          "name": "Haoze Zhang",
          "affiliation": null
        },
        {
          "name": "Runze Mao",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Zhi X. Chen",
          "affiliation": null
        }
      ],
      "abstract": "The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.",
      "publishedDate": "2026-01-04T04:00:28Z",
      "updatedDate": "2026-01-04T04:00:28Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "physics.flu-dyn"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01357v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01357",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01241",
      "title": "MCP-SandboxScan: WASM-based Secure Execution and Runtime Analysis for MCP Tools",
      "authors": [
        {
          "name": "Zhuoran Tan",
          "affiliation": null
        },
        {
          "name": "Run Hao",
          "affiliation": null
        },
        {
          "name": "Jeremy Singer",
          "affiliation": null
        },
        {
          "name": "Yutian Tang",
          "affiliation": null
        },
        {
          "name": "Christos Anagnostopoulos",
          "affiliation": null
        }
      ],
      "abstract": "Tool-augmented LLM agents raise new security risks: tool executions can introduce runtime-only behaviors, including prompt injection and unintended exposure of external inputs (e.g., environment secrets or local files). While existing scanners often focus on static artifacts, analyzing runtime behavior is challenging because directly executing untrusted tools can itself be dangerous. We present MCP-SandboxScan, a lightweight framework motivated by the Model Context Protocol (MCP) that safely executes untrusted tools inside a WebAssembly/WASI sandbox and produces auditable reports of external-to-sink exposures. Our prototype (i) extracts LLM-relevant sinks from runtime outputs (prompt/messages and structured tool-return fields), (ii) instantiates external-input candidates from environment values, mounted file contents, and output-surfaced HTTP fetch intents, and (iii) links sources to sinks via snippet-based substring matching. Case studies on three representative tools show that MCP-SandboxScan can surface provenance evidence when external inputs appear in prompt/messages or tool-return payloads, and can expose filesystem capability violations as runtime evidence. We further compare against a lightweight static string-signature baseline and use a micro-benchmark to characterize false negatives under transformations and false positives from short-token collisions.",
      "publishedDate": "2026-01-03T17:25:38Z",
      "updatedDate": "2026-01-03T17:25:38Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01241v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01241",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01090",
      "title": "Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai",
      "authors": [
        {
          "name": "Erica Coppolillo",
          "affiliation": null
        },
        {
          "name": "Luca Luceri",
          "affiliation": null
        },
        {
          "name": "Emilio Ferrara",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms. We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content. These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.",
      "publishedDate": "2026-01-03T06:33:08Z",
      "updatedDate": "2026-01-03T06:33:08Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01090v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01090",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00994",
      "title": "ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems",
      "authors": [
        {
          "name": "Michael Bao",
          "affiliation": null
        }
      ],
      "abstract": "This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as \"kernel of truth\" messages and spontaneous developments with an \"ink\" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.",
      "publishedDate": "2026-01-02T22:10:09Z",
      "updatedDate": "2026-01-02T22:10:09Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00994v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00994",
      "comment": "In proceedings of 2025 IEEE International Conference on Agentic AI (ICA)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00930",
      "title": "AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation",
      "authors": [
        {
          "name": "Nicolas Bougie",
          "affiliation": null
        },
        {
          "name": "Gian Maria Marconi",
          "affiliation": null
        },
        {
          "name": "Tony Yip",
          "affiliation": null
        },
        {
          "name": "Narimasa Watanabe",
          "affiliation": null
        }
      ],
      "abstract": "Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.",
      "publishedDate": "2026-01-02T03:01:33Z",
      "updatedDate": "2026-01-02T03:01:33Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00930v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00930",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01891",
      "title": "Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems",
      "authors": [
        {
          "name": "Niloufar Alipour Talemi",
          "affiliation": null
        },
        {
          "name": "Julia Boone",
          "affiliation": null
        },
        {
          "name": "Fatemeh Afghah",
          "affiliation": null
        }
      ],
      "abstract": "The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.",
      "publishedDate": "2026-01-05T08:34:17Z",
      "updatedDate": "2026-01-05T08:34:17Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01891v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01891",
      "comment": "Accepted to the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026, GeoCV Workshop",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation",
        "reasoning",
        "planning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation",
          "reasoning",
          "planning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01857",
      "title": "Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios",
      "authors": [
        {
          "name": "Defei Xia",
          "affiliation": null
        },
        {
          "name": "Bingfeng Pi",
          "affiliation": null
        },
        {
          "name": "Shenbin Zhang",
          "affiliation": null
        },
        {
          "name": "Song Hua",
          "affiliation": null
        },
        {
          "name": "Yunfei Wei",
          "affiliation": null
        },
        {
          "name": "Lei Zuo",
          "affiliation": null
        }
      ],
      "abstract": "As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.",
      "publishedDate": "2026-01-05T07:35:12Z",
      "updatedDate": "2026-01-07T01:48:24Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01857v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01857",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01685",
      "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage",
      "authors": [
        {
          "name": "Jinwei Hu",
          "affiliation": null
        },
        {
          "name": "Xinmiao Huang",
          "affiliation": null
        },
        {
          "name": "Youcheng Sun",
          "affiliation": null
        },
        {
          "name": "Yi Dong",
          "affiliation": null
        },
        {
          "name": "Xiaowei Huang",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.",
      "publishedDate": "2026-01-04T22:50:23Z",
      "updatedDate": "2026-01-04T22:50:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01685v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01685",
      "comment": "Under Review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01366",
      "title": "KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models",
      "authors": [
        {
          "name": "Zixian Liu",
          "affiliation": null
        },
        {
          "name": "Sihao Liu",
          "affiliation": null
        },
        {
          "name": "Yuqi Zhao",
          "affiliation": null
        }
      ],
      "abstract": "With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.",
      "publishedDate": "2026-01-04T04:39:39Z",
      "updatedDate": "2026-01-04T04:39:39Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01366v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01366",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02346",
      "title": "Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling",
      "authors": [
        {
          "name": "Falcon LLM Team",
          "affiliation": null
        },
        {
          "name": "Iheb Chaabane",
          "affiliation": null
        },
        {
          "name": "Puneesh Khanna",
          "affiliation": null
        },
        {
          "name": "Suhail Mohmad",
          "affiliation": null
        },
        {
          "name": "Slim Frikha",
          "affiliation": null
        },
        {
          "name": "Shi Hu",
          "affiliation": null
        },
        {
          "name": "Abdalgader Abubaker",
          "affiliation": null
        },
        {
          "name": "Reda Alami",
          "affiliation": null
        },
        {
          "name": "Mikhail Lubinets",
          "affiliation": null
        },
        {
          "name": "Mohamed El Amine Seddik",
          "affiliation": null
        },
        {
          "name": "Hakim Hacid",
          "affiliation": null
        }
      ],
      "abstract": "This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\\times$ to $7\\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.",
      "publishedDate": "2026-01-05T18:44:27Z",
      "updatedDate": "2026-01-05T18:44:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02346v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02346",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02224",
      "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality",
      "authors": [
        {
          "name": "Fabian Lukassen",
          "affiliation": null
        },
        {
          "name": "Jan Herrmann",
          "affiliation": null
        },
        {
          "name": "Christoph Weisser",
          "affiliation": null
        },
        {
          "name": "Benjamin Saefken",
          "affiliation": null
        },
        {
          "name": "Thomas Kneib",
          "affiliation": null
        }
      ],
      "abstract": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.",
      "publishedDate": "2026-01-05T15:52:20Z",
      "updatedDate": "2026-01-05T15:52:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02224v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02224",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02170",
      "title": "Streaming Hallucination Detection in Long Chain-of-Thought Reasoning",
      "authors": [
        {
          "name": "Haolang Lu",
          "affiliation": null
        },
        {
          "name": "Minghui Pan",
          "affiliation": null
        },
        {
          "name": "Ripeng Li",
          "affiliation": null
        },
        {
          "name": "Guoshun Nan",
          "affiliation": null
        },
        {
          "name": "Jialin Zhuang",
          "affiliation": null
        },
        {
          "name": "Zijie Zhao",
          "affiliation": null
        },
        {
          "name": "Zhongxiang Sun",
          "affiliation": null
        },
        {
          "name": "Kun Wang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.",
      "publishedDate": "2026-01-05T14:47:41Z",
      "updatedDate": "2026-01-05T14:47:41Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02170v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02170",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02163",
      "title": "EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning",
      "authors": [
        {
          "name": "Chuanrui Hu",
          "affiliation": null
        },
        {
          "name": "Xingze Gao",
          "affiliation": null
        },
        {
          "name": "Zuyi Zhou",
          "affiliation": null
        },
        {
          "name": "Dannong Xu",
          "affiliation": null
        },
        {
          "name": "Yi Bai",
          "affiliation": null
        },
        {
          "name": "Xintong Li",
          "affiliation": null
        },
        {
          "name": "Hui Zhang",
          "affiliation": null
        },
        {
          "name": "Tong Li",
          "affiliation": null
        },
        {
          "name": "Chong Zhang",
          "affiliation": null
        },
        {
          "name": "Lidong Bing",
          "affiliation": null
        },
        {
          "name": "Yafeng Deng",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.",
      "publishedDate": "2026-01-05T14:39:43Z",
      "updatedDate": "2026-01-05T14:39:43Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02163v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02163",
      "comment": "16 pages, 6 figures, 12 tables. Code available at https://github.com/EverMind-AI/EverMemOS",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02158",
      "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience",
      "authors": [
        {
          "name": "Almaz Ermilov",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.",
      "publishedDate": "2026-01-05T14:36:02Z",
      "updatedDate": "2026-01-05T14:36:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG",
        "physics.geo-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02158v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02158",
      "comment": "24 pages, 8 figures, 10 tables; benchmark and code at https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02147",
      "title": "BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models",
      "authors": [
        {
          "name": "Sunny Gupta",
          "affiliation": null
        },
        {
          "name": "Shounak Das",
          "affiliation": null
        },
        {
          "name": "Amit Sethi",
          "affiliation": null
        }
      ],
      "abstract": "Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.",
      "publishedDate": "2026-01-05T14:22:20Z",
      "updatedDate": "2026-01-05T14:22:20Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02147v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02147",
      "comment": "Accepted at the AAAI 2026 Workshop AIR-FM, Assessing and Improving Reliability of Foundation Models in the Real World",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02076",
      "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows",
      "authors": [
        {
          "name": "Yingte Shu",
          "affiliation": null
        },
        {
          "name": "Yuchuan Tian",
          "affiliation": null
        },
        {
          "name": "Chao Xu",
          "affiliation": null
        },
        {
          "name": "Yunhe Wang",
          "affiliation": null
        },
        {
          "name": "Hanting Chen",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.",
      "publishedDate": "2026-01-05T12:57:33Z",
      "updatedDate": "2026-01-05T12:57:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02076v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02076",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02046",
      "title": "Agentic Retoucher for Text-To-Image Generation",
      "authors": [
        {
          "name": "Shaocheng Shen",
          "affiliation": null
        },
        {
          "name": "Jianfeng Liang. Chunlei Cai",
          "affiliation": null
        },
        {
          "name": "Cong Geng",
          "affiliation": null
        },
        {
          "name": "Huiyu Duan",
          "affiliation": null
        },
        {
          "name": "Xiaoyun Zhang",
          "affiliation": null
        },
        {
          "name": "Qiang Hu",
          "affiliation": null
        },
        {
          "name": "Guangtao Zhai",
          "affiliation": null
        }
      ],
      "abstract": "Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.",
      "publishedDate": "2026-01-05T12:06:43Z",
      "updatedDate": "2026-01-05T12:06:43Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02046v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02046",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02021",
      "title": "AgentVNE: LLM-Augmented Graph Reinforcement Learning for Affinity-Aware Multi-Agent Placement in Edge Agentic AI",
      "authors": [
        {
          "name": "Runze Zheng",
          "affiliation": null
        },
        {
          "name": "Yuqing Zheng",
          "affiliation": null
        },
        {
          "name": "Zhengyi Cheng",
          "affiliation": null
        },
        {
          "name": "Long Luo",
          "affiliation": null
        },
        {
          "name": "Haoxiang Luo",
          "affiliation": null
        },
        {
          "name": "Gang Sun",
          "affiliation": null
        },
        {
          "name": "Hongfang Yu",
          "affiliation": null
        },
        {
          "name": "Dusit Niyato",
          "affiliation": null
        }
      ],
      "abstract": "The Internet of Agents is propelling edge computing toward agentic AI and edge general intelligence (EGI). However, deploying multi-agent service (MAS) on resource-constrained edge infrastructure presents severe challenges. MAS service workflows are driven by complex cross-node interactions, dynamic memory accumulation, and collaborative tool usage. Exhibiting chain-like topological dependencies and strict affinity constraints, these workflows demand real-time responsiveness that exceeds the capabilities of traditional VNE algorithms designed for static resources. To address this, we propose AgentVNE, a cloud-edge collaborative framework utilizing a dual-layer architecture. First, AgentVNE employs a large language model (LLM) to identify implicit semantic constraints and generate affinity-based resource augmentation to resolve physical dependency issues. Second, it constructs a resource similarity-aware neural network, utilizing a pre-training and PPO fine-tuning strategy to precisely capture topological similarities between dynamic workflows and heterogeneous networks. By coupling semantic perception with topological reasoning, this mechanism effectively bridges the gap between dynamic service requirements and physical infrastructure. Simulation results demonstrate that AgentVNE reduces workflow communication latency to less than 40% of baselines and improves the service acceptance rate by approximately 5%-10% under high-load scenarios. Ultimately, this work provides a foundational solution for the semantic-aware deployment of agentic AI.",
      "publishedDate": "2026-01-05T11:30:04Z",
      "updatedDate": "2026-01-05T11:30:04Z",
      "primaryCategory": "cs.NI",
      "arxivCategories": [
        "cs.NI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02021v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02021",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "agents",
        "tool-use",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "tool-use",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01984",
      "title": "Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation",
      "authors": [
        {
          "name": "Weijian Ma",
          "affiliation": null
        },
        {
          "name": "Shizhao Sun",
          "affiliation": null
        },
        {
          "name": "Tianyu Yu",
          "affiliation": null
        },
        {
          "name": "Ruiyu Wang",
          "affiliation": null
        },
        {
          "name": "Tat-Seng Chua",
          "affiliation": null
        },
        {
          "name": "Jiang Bian",
          "affiliation": null
        }
      ],
      "abstract": "Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.",
      "publishedDate": "2026-01-05T10:38:26Z",
      "updatedDate": "2026-01-05T10:38:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01984v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01984",
      "comment": "Preprint. Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01982",
      "title": "ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems",
      "authors": [
        {
          "name": "Noel Thomas",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.",
      "publishedDate": "2026-01-05T10:36:40Z",
      "updatedDate": "2026-01-05T10:36:40Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01982v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01982",
      "comment": "7 pages, 0 figures , Accepted to AAAI-26 Bridge Program: Logical and Symbolic Reasoning in Language Models (camera-ready)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01910",
      "title": "MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning",
      "authors": [
        {
          "name": "Minh Hieu Ha",
          "affiliation": null
        },
        {
          "name": "Khanh Ly Ta",
          "affiliation": null
        },
        {
          "name": "Hung Phan",
          "affiliation": null
        },
        {
          "name": "Tung Doan",
          "affiliation": null
        },
        {
          "name": "Tung Dao",
          "affiliation": null
        },
        {
          "name": "Dao Tran",
          "affiliation": null
        },
        {
          "name": "Huynh Thi Thanh Binh",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency. We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.",
      "publishedDate": "2026-01-05T08:55:27Z",
      "updatedDate": "2026-01-05T08:55:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01910v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01910",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01875",
      "title": "Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence",
      "authors": [
        {
          "name": "Kewen Cao",
          "affiliation": null
        },
        {
          "name": "Jianxu Chen",
          "affiliation": null
        },
        {
          "name": "Yongbing Zhang",
          "affiliation": null
        },
        {
          "name": "Ye Zhang",
          "affiliation": null
        },
        {
          "name": "Hongxiao Wang",
          "affiliation": null
        }
      ],
      "abstract": "Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.",
      "publishedDate": "2026-01-05T08:02:49Z",
      "updatedDate": "2026-01-05T08:02:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "q-bio.QM"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01875v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01875",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01874",
      "title": "CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving",
      "authors": [
        {
          "name": "Shuhang Chen",
          "affiliation": null
        },
        {
          "name": "Yunqiu Xu",
          "affiliation": null
        },
        {
          "name": "Junjie Xie",
          "affiliation": null
        },
        {
          "name": "Aojun Lu",
          "affiliation": null
        },
        {
          "name": "Tao Feng",
          "affiliation": null
        },
        {
          "name": "Zeying Huang",
          "affiliation": null
        },
        {
          "name": "Ning Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Sun",
          "affiliation": null
        },
        {
          "name": "Yi Yang",
          "affiliation": null
        },
        {
          "name": "Hangjie Yuan",
          "affiliation": null
        }
      ],
      "abstract": "Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\\Rightarrow$internalization$\\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.",
      "publishedDate": "2026-01-05T08:02:18Z",
      "updatedDate": "2026-01-05T08:02:18Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01874v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01874",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01825",
      "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning",
      "authors": [
        {
          "name": "Yaxin Cui",
          "affiliation": null
        },
        {
          "name": "Yuanqiang Zeng",
          "affiliation": null
        },
        {
          "name": "Jiapeng Yan",
          "affiliation": null
        },
        {
          "name": "Keling Lin",
          "affiliation": null
        },
        {
          "name": "Kai Ji",
          "affiliation": null
        },
        {
          "name": "Jianhui Zeng",
          "affiliation": null
        },
        {
          "name": "Sheng Zhang",
          "affiliation": null
        },
        {
          "name": "Xin Luo",
          "affiliation": null
        },
        {
          "name": "Binzhu Su",
          "affiliation": null
        },
        {
          "name": "Chaolai Shen",
          "affiliation": null
        },
        {
          "name": "Jiahao Yu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.",
      "publishedDate": "2026-01-05T06:44:29Z",
      "updatedDate": "2026-01-05T06:44:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01825v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01825",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01804",
      "title": "Causality-Aware Temporal Projection for Video Understanding in Video-LLMs",
      "authors": [
        {
          "name": "Zhengjian Kang",
          "affiliation": null
        },
        {
          "name": "Qi Chen",
          "affiliation": null
        },
        {
          "name": "Rui Liu",
          "affiliation": null
        },
        {
          "name": "Kangtong Mo",
          "affiliation": null
        },
        {
          "name": "Xingyu Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Deng",
          "affiliation": null
        },
        {
          "name": "Ye Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.",
      "publishedDate": "2026-01-05T05:30:13Z",
      "updatedDate": "2026-01-05T05:30:13Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01804v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01804",
      "comment": "7 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01798",
      "title": "VerLM: Explaining Face Verification Using Natural Language",
      "authors": [
        {
          "name": "Syed Abdul Hannan",
          "affiliation": null
        },
        {
          "name": "Hazim Bukhari",
          "affiliation": null
        },
        {
          "name": "Thomas Cantalapiedra",
          "affiliation": null
        },
        {
          "name": "Eman Ansar",
          "affiliation": null
        },
        {
          "name": "Massa Baali",
          "affiliation": null
        },
        {
          "name": "Rita Singh",
          "affiliation": null
        },
        {
          "name": "Bhiksha Raj",
          "affiliation": null
        }
      ],
      "abstract": "Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.",
      "publishedDate": "2026-01-05T05:16:07Z",
      "updatedDate": "2026-01-05T05:16:07Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01798v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01798",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01739",
      "title": "K-EXAONE Technical Report",
      "authors": [
        {
          "name": "Eunbi Choi",
          "affiliation": null
        },
        {
          "name": "Kibong Choi",
          "affiliation": null
        },
        {
          "name": "Seokhee Hong",
          "affiliation": null
        },
        {
          "name": "Junwon Hwang",
          "affiliation": null
        },
        {
          "name": "Hyojin Jeon",
          "affiliation": null
        },
        {
          "name": "Hyunjik Jo",
          "affiliation": null
        },
        {
          "name": "Joonkee Kim",
          "affiliation": null
        },
        {
          "name": "Seonghwan Kim",
          "affiliation": null
        },
        {
          "name": "Soyeon Kim",
          "affiliation": null
        },
        {
          "name": "Sunkyoung Kim",
          "affiliation": null
        },
        {
          "name": "Yireun Kim",
          "affiliation": null
        },
        {
          "name": "Yongil Kim",
          "affiliation": null
        },
        {
          "name": "Haeju Lee",
          "affiliation": null
        },
        {
          "name": "Jinsik Lee",
          "affiliation": null
        },
        {
          "name": "Kyungmin Lee",
          "affiliation": null
        },
        {
          "name": "Sangha Park",
          "affiliation": null
        },
        {
          "name": "Heuiyeen Yeen",
          "affiliation": null
        },
        {
          "name": "Hwan Chang",
          "affiliation": null
        },
        {
          "name": "Stanley Jungkyu Choi",
          "affiliation": null
        },
        {
          "name": "Yejin Choi",
          "affiliation": null
        },
        {
          "name": "Jiwon Ham",
          "affiliation": null
        },
        {
          "name": "Kijeong Jeon",
          "affiliation": null
        },
        {
          "name": "Geunyeong Jeong",
          "affiliation": null
        },
        {
          "name": "Gerrard Jeongwon Jo",
          "affiliation": null
        },
        {
          "name": "Yonghwan Jo",
          "affiliation": null
        },
        {
          "name": "Jiyeon Jung",
          "affiliation": null
        },
        {
          "name": "Naeun Kang",
          "affiliation": null
        },
        {
          "name": "Dohoon Kim",
          "affiliation": null
        },
        {
          "name": "Euisoon Kim",
          "affiliation": null
        },
        {
          "name": "Hayeon Kim",
          "affiliation": null
        },
        {
          "name": "Hyosang Kim",
          "affiliation": null
        },
        {
          "name": "Hyunseo Kim",
          "affiliation": null
        },
        {
          "name": "Jieun Kim",
          "affiliation": null
        },
        {
          "name": "Minu Kim",
          "affiliation": null
        },
        {
          "name": "Myoungshin Kim",
          "affiliation": null
        },
        {
          "name": "Unsol Kim",
          "affiliation": null
        },
        {
          "name": "Youchul Kim",
          "affiliation": null
        },
        {
          "name": "YoungJin Kim",
          "affiliation": null
        },
        {
          "name": "Chaeeun Lee",
          "affiliation": null
        },
        {
          "name": "Chaeyoon Lee",
          "affiliation": null
        },
        {
          "name": "Changhun Lee",
          "affiliation": null
        },
        {
          "name": "Dahm Lee",
          "affiliation": null
        },
        {
          "name": "Edward Hwayoung Lee",
          "affiliation": null
        },
        {
          "name": "Honglak Lee",
          "affiliation": null
        },
        {
          "name": "Jinsang Lee",
          "affiliation": null
        },
        {
          "name": "Jiyoung Lee",
          "affiliation": null
        },
        {
          "name": "Sangeun Lee",
          "affiliation": null
        },
        {
          "name": "Seungwon Lim",
          "affiliation": null
        },
        {
          "name": "Solji Lim",
          "affiliation": null
        },
        {
          "name": "Woohyung Lim",
          "affiliation": null
        },
        {
          "name": "Chanwoo Moon",
          "affiliation": null
        },
        {
          "name": "Jaewoo Park",
          "affiliation": null
        },
        {
          "name": "Jinho Park",
          "affiliation": null
        },
        {
          "name": "Yongmin Park",
          "affiliation": null
        },
        {
          "name": "Hyerin Seo",
          "affiliation": null
        },
        {
          "name": "Wooseok Seo",
          "affiliation": null
        },
        {
          "name": "Yongwoo Song",
          "affiliation": null
        },
        {
          "name": "Sejong Yang",
          "affiliation": null
        },
        {
          "name": "Sihoon Yang",
          "affiliation": null
        },
        {
          "name": "Chang En Yea",
          "affiliation": null
        },
        {
          "name": "Sihyuk Yi",
          "affiliation": null
        },
        {
          "name": "Chansik Yoon",
          "affiliation": null
        },
        {
          "name": "Dongkeun Yoon",
          "affiliation": null
        },
        {
          "name": "Sangyeon Yoon",
          "affiliation": null
        },
        {
          "name": "Hyeongu Yun",
          "affiliation": null
        }
      ],
      "abstract": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.",
      "publishedDate": "2026-01-05T02:30:59Z",
      "updatedDate": "2026-01-05T02:30:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01739v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01739",
      "comment": "29 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01714",
      "title": "Entropy-Aligned Decoding of LMs for Better Writing and Reasoning",
      "authors": [
        {
          "name": "Kareem Ahmed",
          "affiliation": null
        },
        {
          "name": "Sameer Singh",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.",
      "publishedDate": "2026-01-05T01:37:10Z",
      "updatedDate": "2026-01-05T01:37:10Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01714v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01714",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01708",
      "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription",
      "authors": [
        {
          "name": "Unggi Lee",
          "affiliation": null
        },
        {
          "name": "Joo Young Kim",
          "affiliation": null
        },
        {
          "name": "Ran Ju",
          "affiliation": null
        },
        {
          "name": "Minyoung Jung",
          "affiliation": null
        },
        {
          "name": "Jeyeon Eo",
          "affiliation": null
        }
      ],
      "abstract": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.",
      "publishedDate": "2026-01-05T01:02:21Z",
      "updatedDate": "2026-01-05T01:02:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01708v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01708",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01580",
      "title": "The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs",
      "authors": [
        {
          "name": "Zibo Zhao",
          "affiliation": null
        },
        {
          "name": "Yuanting Zha",
          "affiliation": null
        },
        {
          "name": "Haipeng Zhang",
          "affiliation": null
        },
        {
          "name": "Xingcheng Xu",
          "affiliation": null
        }
      ],
      "abstract": "Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.",
      "publishedDate": "2026-01-04T15:59:15Z",
      "updatedDate": "2026-01-04T15:59:15Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01580v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01580",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01552",
      "title": "HalluZig: Hallucination Detection using Zigzag Persistence",
      "authors": [
        {
          "name": "Shreyas N. Samaga",
          "affiliation": null
        },
        {
          "name": "Gilberto Gonzalez Arroyo",
          "affiliation": null
        },
        {
          "name": "Tamal K. Dey",
          "affiliation": null
        }
      ],
      "abstract": "The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.",
      "publishedDate": "2026-01-04T14:55:43Z",
      "updatedDate": "2026-01-04T14:55:43Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01552v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01552",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01546",
      "title": "Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation",
      "authors": [
        {
          "name": "Letian Kong",
          "affiliation": null
        },
        {
          "name": "Qianran",
          "affiliation": null
        },
        {
          "name": "Jin",
          "affiliation": null
        },
        {
          "name": "Renyu Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.",
      "publishedDate": "2026-01-04T14:42:00Z",
      "updatedDate": "2026-01-04T14:42:00Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01546v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01546",
      "comment": "39 pages, 2 figures, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01513",
      "title": "FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Gen Li",
          "affiliation": null
        },
        {
          "name": "Peiyu Liu",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.",
      "publishedDate": "2026-01-04T12:46:35Z",
      "updatedDate": "2026-01-07T15:36:31Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01513v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01513",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01490",
      "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints",
      "authors": [
        {
          "name": "Junichiro Niimi",
          "affiliation": null
        }
      ],
      "abstract": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.",
      "publishedDate": "2026-01-04T11:35:39Z",
      "updatedDate": "2026-01-04T11:35:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01490v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01490",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01407",
      "title": "From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models",
      "authors": [
        {
          "name": "Arjhun Sreedar",
          "affiliation": null
        },
        {
          "name": "Rohan Pillay",
          "affiliation": null
        },
        {
          "name": "Laukik Patade",
          "affiliation": null
        }
      ],
      "abstract": "This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.",
      "publishedDate": "2026-01-04T07:08:37Z",
      "updatedDate": "2026-01-04T07:08:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01407v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01407",
      "comment": "10 pages, 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01400",
      "title": "EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery",
      "authors": [
        {
          "name": "Jicheng Ma",
          "affiliation": null
        },
        {
          "name": "Guohua Wang",
          "affiliation": null
        },
        {
          "name": "Xinhua Feng",
          "affiliation": null
        },
        {
          "name": "Yiming Liu",
          "affiliation": null
        },
        {
          "name": "Zhichao Hu",
          "affiliation": null
        },
        {
          "name": "Yuhong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \\textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.",
      "publishedDate": "2026-01-04T06:40:25Z",
      "updatedDate": "2026-01-04T06:40:25Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01400v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01400",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01362",
      "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning",
      "authors": [
        {
          "name": "Jerry Huang",
          "affiliation": null
        },
        {
          "name": "Peng Lu",
          "affiliation": null
        },
        {
          "name": "Qiuhao Zeng",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Sarath Chandar",
          "affiliation": null
        },
        {
          "name": "Edison Marrese-Taylor",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        }
      ],
      "abstract": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.",
      "publishedDate": "2026-01-04T04:29:12Z",
      "updatedDate": "2026-01-04T04:29:12Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG",
        "stat.ML"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01362v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01362",
      "comment": "Accepted to The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01341",
      "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems",
      "authors": [
        {
          "name": "Md Abdullah Al Kafi",
          "affiliation": null
        },
        {
          "name": "Raka Moni",
          "affiliation": null
        },
        {
          "name": "Sumit Kumar Banshal",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.",
      "publishedDate": "2026-01-04T03:09:23Z",
      "updatedDate": "2026-01-04T03:09:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01341v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01341",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01322",
      "title": "LinMU: Multimodal Understanding Made Linear",
      "authors": [
        {
          "name": "Hongjie Wang",
          "affiliation": null
        },
        {
          "name": "Niraj K. Jha",
          "affiliation": null
        }
      ],
      "abstract": "Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\\times$ and improves token throughput by up to 9.0$\\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.",
      "publishedDate": "2026-01-04T01:17:36Z",
      "updatedDate": "2026-01-04T01:17:36Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG",
        "cs.MM",
        "eess.IV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01322v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01322",
      "comment": "23 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01298",
      "title": "Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware",
      "authors": [
        {
          "name": "Jorge L. Ruiz Williams",
          "affiliation": null
        }
      ],
      "abstract": "Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering \"System 2\" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.",
      "publishedDate": "2026-01-03T23:11:21Z",
      "updatedDate": "2026-01-03T23:11:21Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.AR",
        "cs.DC",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01298v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01298",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01266",
      "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment",
      "authors": [
        {
          "name": "Rhitabrat Pokharel",
          "affiliation": null
        },
        {
          "name": "Hamid Hassanzadeh",
          "affiliation": null
        },
        {
          "name": "Ameeta Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.",
      "publishedDate": "2026-01-03T19:24:51Z",
      "updatedDate": "2026-01-03T19:24:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01266v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01266",
      "comment": "Accepted at AIMedHealth @ AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01233",
      "title": "Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling",
      "authors": [
        {
          "name": "Kangchen Zhu",
          "affiliation": null
        },
        {
          "name": "Zhiliang Tian",
          "affiliation": null
        },
        {
          "name": "Shangwen Wang",
          "affiliation": null
        },
        {
          "name": "Mingyue Leng",
          "affiliation": null
        },
        {
          "name": "Xiaoguang Mao",
          "affiliation": null
        }
      ],
      "abstract": "Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.",
      "publishedDate": "2026-01-03T16:43:05Z",
      "updatedDate": "2026-01-03T16:43:05Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01233v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01233",
      "comment": "Accepted by ICSE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01195",
      "title": "Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering",
      "authors": [
        {
          "name": "Wuzhenghong Wen",
          "affiliation": null
        },
        {
          "name": "Chao Xue",
          "affiliation": null
        },
        {
          "name": "Su Pan",
          "affiliation": null
        },
        {
          "name": "Yuwei Sun",
          "affiliation": null
        },
        {
          "name": "Minlong Peng",
          "affiliation": null
        }
      ],
      "abstract": "Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.",
      "publishedDate": "2026-01-03T14:27:01Z",
      "updatedDate": "2026-01-03T14:27:01Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01195v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01195",
      "comment": "11 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01118",
      "title": "ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services",
      "authors": [
        {
          "name": "Qingqing Long",
          "affiliation": null
        },
        {
          "name": "Haotian Chen",
          "affiliation": null
        },
        {
          "name": "Chenyang Zhao",
          "affiliation": null
        },
        {
          "name": "Xiaolei Du",
          "affiliation": null
        },
        {
          "name": "Xuezhi Wang",
          "affiliation": null
        },
        {
          "name": "Pengyao Wang",
          "affiliation": null
        },
        {
          "name": "Chengzan Li",
          "affiliation": null
        },
        {
          "name": "Yuanchun Zhou",
          "affiliation": null
        },
        {
          "name": "Hengshu Zhu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.",
      "publishedDate": "2026-01-03T08:42:53Z",
      "updatedDate": "2026-01-03T08:42:53Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.DL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01118v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01118",
      "comment": "12 pages, 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "agents",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01095",
      "title": "NarrativeTrack: Evaluating Video Language Models Beyond the Frame",
      "authors": [
        {
          "name": "Hyeonjeong Ha",
          "affiliation": null
        },
        {
          "name": "Jinjin Ge",
          "affiliation": null
        },
        {
          "name": "Bo Feng",
          "affiliation": null
        },
        {
          "name": "Kaixin Ma",
          "affiliation": null
        },
        {
          "name": "Gargi Chakraborty",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.",
      "publishedDate": "2026-01-03T07:12:55Z",
      "updatedDate": "2026-01-03T07:12:55Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01095v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01095",
      "comment": "VideoLLM Fine-Grained Evaluation",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01011",
      "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models",
      "authors": [
        {
          "name": "Patricio Vera",
          "affiliation": null
        }
      ],
      "abstract": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies",
      "publishedDate": "2026-01-03T00:19:53Z",
      "updatedDate": "2026-01-03T00:19:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01011v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01011",
      "comment": "21 pages, 4 figures, 3 tables. Code: https://github.com/patriciomvera/intention-collapse-experiments",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00998",
      "title": "DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models",
      "authors": [
        {
          "name": "Yue Zhou",
          "affiliation": null
        },
        {
          "name": "Jue Chen",
          "affiliation": null
        },
        {
          "name": "Zilun Zhang",
          "affiliation": null
        },
        {
          "name": "Penghui Huang",
          "affiliation": null
        },
        {
          "name": "Ran Ding",
          "affiliation": null
        },
        {
          "name": "Zhentao Zou",
          "affiliation": null
        },
        {
          "name": "PengFei Gao",
          "affiliation": null
        },
        {
          "name": "Yuchen Wei",
          "affiliation": null
        },
        {
          "name": "Ke Li",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Xue Jiang",
          "affiliation": null
        },
        {
          "name": "Hongxin Yang",
          "affiliation": null
        },
        {
          "name": "Jonathan Li",
          "affiliation": null
        }
      ],
      "abstract": "Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench",
      "publishedDate": "2026-01-02T22:42:38Z",
      "updatedDate": "2026-01-02T22:42:38Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00998v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00998",
      "comment": "20 pages, 17 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "evaluation",
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation",
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00923",
      "title": "Context Collapse: In-Context Learning and Model Collapse",
      "authors": [
        {
          "name": "Josef Ott",
          "affiliation": null
        }
      ],
      "abstract": "This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.",
      "publishedDate": "2026-01-01T17:33:47Z",
      "updatedDate": "2026-01-01T17:33:47Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00923v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00923",
      "comment": "Master's thesis",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01196",
      "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners",
      "authors": [
        {
          "name": "Shenqi Lu",
          "affiliation": null
        },
        {
          "name": "Liangwei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.",
      "publishedDate": "2026-01-03T14:40:39Z",
      "updatedDate": "2026-01-03T14:40:39Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01196v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01196",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "robotics",
        "prompting",
        "agents",
        "tool-use",
        "planning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "robotics",
          "prompting",
          "agents",
          "tool-use",
          "planning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01673",
      "title": "Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks",
      "authors": [
        {
          "name": "Arina Kharlamova",
          "affiliation": null
        },
        {
          "name": "Youcheng Sun",
          "affiliation": null
        },
        {
          "name": "Ting Yu",
          "affiliation": null
        }
      ],
      "abstract": "Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.",
      "publishedDate": "2026-01-04T21:44:55Z",
      "updatedDate": "2026-01-04T21:44:55Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01673v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01673",
      "comment": "IEEE S&P'26 under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01320",
      "title": "Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python",
      "authors": [
        {
          "name": "Muntasir Adnan",
          "affiliation": null
        },
        {
          "name": "Carlos C. N. Kuhn",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.",
      "publishedDate": "2026-01-04T01:13:37Z",
      "updatedDate": "2026-01-04T01:13:37Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01320v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01320",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02075",
      "title": "MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics",
      "authors": [
        {
          "name": "Zhuofan Shi",
          "affiliation": null
        },
        {
          "name": "Hubao A",
          "affiliation": null
        },
        {
          "name": "Yufei Shao",
          "affiliation": null
        },
        {
          "name": "Dongliang Huang",
          "affiliation": null
        },
        {
          "name": "Hongxu An",
          "affiliation": null
        },
        {
          "name": "Chunxiao Xin",
          "affiliation": null
        },
        {
          "name": "Haiyang Shen",
          "affiliation": null
        },
        {
          "name": "Zhenyu Wang",
          "affiliation": null
        },
        {
          "name": "Yunshan Na",
          "affiliation": null
        },
        {
          "name": "Gang Huang",
          "affiliation": null
        },
        {
          "name": "Xiang Jing",
          "affiliation": null
        }
      ],
      "abstract": "Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2",
      "publishedDate": "2026-01-05T12:56:51Z",
      "updatedDate": "2026-01-07T10:06:36Z",
      "primaryCategory": "cs.CE",
      "arxivCategories": [
        "cs.CE",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02075v3",
      "arxivUrl": "https://arxiv.org/abs/2601.02075",
      "comment": "24 pages,4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01993",
      "title": "MindChat: A Privacy-preserving Large Language Model for Mental Health Support",
      "authors": [
        {
          "name": "Dong Xue",
          "affiliation": null
        },
        {
          "name": "Jicheng Tu",
          "affiliation": null
        },
        {
          "name": "Ming Wang",
          "affiliation": null
        },
        {
          "name": "Xin Yan",
          "affiliation": null
        },
        {
          "name": "Fangzhou Liu",
          "affiliation": null
        },
        {
          "name": "Jie Hu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.",
      "publishedDate": "2026-01-05T10:54:18Z",
      "updatedDate": "2026-01-05T10:54:18Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01993v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01993",
      "comment": "33 pages, 16 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01844",
      "title": "Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Udiptaman Das",
          "affiliation": null
        },
        {
          "name": "Krishnasai B. Atmakuri",
          "affiliation": null
        },
        {
          "name": "Duy Ho",
          "affiliation": null
        },
        {
          "name": "Chi Lee",
          "affiliation": null
        },
        {
          "name": "Yugyung Lee",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.",
      "publishedDate": "2026-01-05T07:16:29Z",
      "updatedDate": "2026-01-05T07:16:29Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01844v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01844",
      "comment": "13 pages, 5 tables, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "agents",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02345",
      "title": "Question Answering for Multi-Release Systems: A Case Study at Ciena",
      "authors": [
        {
          "name": "Parham Khamsepour",
          "affiliation": null
        },
        {
          "name": "Mark Cole",
          "affiliation": null
        },
        {
          "name": "Ish Ashraf",
          "affiliation": null
        },
        {
          "name": "Sandeep Puri",
          "affiliation": null
        },
        {
          "name": "Mehrdad Sabetzadeh",
          "affiliation": null
        },
        {
          "name": "Shiva Nejati",
          "affiliation": null
        }
      ],
      "abstract": "Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.",
      "publishedDate": "2026-01-05T18:44:26Z",
      "updatedDate": "2026-01-05T18:44:26Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02345v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02345",
      "comment": "Accepted for publication in SANER 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02065",
      "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory",
      "authors": [
        {
          "name": "Md. Asif Hossain",
          "affiliation": null
        },
        {
          "name": "Nabil Subhan",
          "affiliation": null
        },
        {
          "name": "Mantasha Rahman Mahi",
          "affiliation": null
        },
        {
          "name": "Jannatul Ferdous Nabila",
          "affiliation": null
        }
      ],
      "abstract": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings",
      "publishedDate": "2026-01-05T12:41:44Z",
      "updatedDate": "2026-01-05T12:41:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02065v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02065",
      "comment": "5 pages, 3 figures, 1 table",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02023",
      "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs",
      "authors": [
        {
          "name": "Amirali Ebrahimzadeh",
          "affiliation": null
        },
        {
          "name": "Seyyed M. Salili",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.",
      "publishedDate": "2026-01-05T11:30:56Z",
      "updatedDate": "2026-01-05T11:30:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02023v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02023",
      "comment": "25 pages, 8 figures, 3 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01950",
      "title": "Face Normal Estimation from Rags to Riches",
      "authors": [
        {
          "name": "Meng Wang",
          "affiliation": null
        },
        {
          "name": "Wenjing Dai",
          "affiliation": null
        },
        {
          "name": "Jiawan Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaojie Guo",
          "affiliation": null
        }
      ],
      "abstract": "Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.",
      "publishedDate": "2026-01-05T09:57:24Z",
      "updatedDate": "2026-01-05T09:57:24Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01950v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01950",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01896",
      "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG",
      "authors": [
        {
          "name": "Jingyu Liu",
          "affiliation": null
        },
        {
          "name": "Jiaen Lin",
          "affiliation": null
        },
        {
          "name": "Yong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.",
      "publishedDate": "2026-01-05T08:40:37Z",
      "updatedDate": "2026-01-06T15:41:23Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01896v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01896",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01872",
      "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios",
      "authors": [
        {
          "name": "Hongbo Duan",
          "affiliation": null
        },
        {
          "name": "Shangyi Luo",
          "affiliation": null
        },
        {
          "name": "Zhiyuan Deng",
          "affiliation": null
        },
        {
          "name": "Yanbo Chen",
          "affiliation": null
        },
        {
          "name": "Yuanhao Chiang",
          "affiliation": null
        },
        {
          "name": "Yi Liu",
          "affiliation": null
        },
        {
          "name": "Fangming Liu",
          "affiliation": null
        },
        {
          "name": "Xueqian Wang",
          "affiliation": null
        }
      ],
      "abstract": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.",
      "publishedDate": "2026-01-05T08:00:34Z",
      "updatedDate": "2026-01-05T08:00:34Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01872v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01872",
      "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "robotics",
        "agents",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "rag",
          "robotics",
          "agents",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01785",
      "title": "SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines",
      "authors": [
        {
          "name": "Rajiv Chaitanya Muttur",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.",
      "publishedDate": "2026-01-05T04:39:31Z",
      "updatedDate": "2026-01-05T04:39:31Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01785v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01785",
      "comment": "Presented at ICEdge 2025; nominated for Best Paper Award",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00926",
      "title": "MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers",
      "authors": [
        {
          "name": "Satya Swaroop Gudipudi",
          "affiliation": null
        },
        {
          "name": "Sahil Girhepuje",
          "affiliation": null
        },
        {
          "name": "Ponnurangam Kumaraguru",
          "affiliation": null
        },
        {
          "name": "Kristine Ma",
          "affiliation": null
        }
      ],
      "abstract": "Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.",
      "publishedDate": "2026-01-01T23:31:02Z",
      "updatedDate": "2026-01-01T23:31:02Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00926v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00926",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02002",
      "title": "Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models",
      "authors": [
        {
          "name": "Antonio Colacicco",
          "affiliation": null
        },
        {
          "name": "Vito Guida",
          "affiliation": null
        },
        {
          "name": "Dario Di Palma",
          "affiliation": null
        },
        {
          "name": "Fedelucio Narducci",
          "affiliation": null
        },
        {
          "name": "Tommaso Di Noia",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.",
      "publishedDate": "2026-01-05T11:03:56Z",
      "updatedDate": "2026-01-05T11:03:56Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02002v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02002",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "rag"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01954",
      "title": "Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations",
      "authors": [
        {
          "name": "Alexander Korn",
          "affiliation": null
        },
        {
          "name": "Lea Zaruchas",
          "affiliation": null
        },
        {
          "name": "Chetan Arora",
          "affiliation": null
        },
        {
          "name": "Andreas Metzger",
          "affiliation": null
        },
        {
          "name": "Sven Smolka",
          "affiliation": null
        },
        {
          "name": "Fanyu Wang",
          "affiliation": null
        },
        {
          "name": "Andreas Vogelsang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.",
      "publishedDate": "2026-01-05T10:01:20Z",
      "updatedDate": "2026-01-05T10:01:20Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01954v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01954",
      "comment": "To be published at The 3rd ACM International Conference on AI Foundation Models and Software Engineering FORGE 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01213",
      "title": "Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation",
      "authors": [
        {
          "name": "Riccardo Gelato",
          "affiliation": null
        },
        {
          "name": "Carlo Sgaravatti",
          "affiliation": null
        },
        {
          "name": "Jakob Grahn",
          "affiliation": null
        },
        {
          "name": "Giacomo Boracchi",
          "affiliation": null
        },
        {
          "name": "Filippo Maria Bianchi",
          "affiliation": null
        }
      ],
      "abstract": "Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.",
      "publishedDate": "2026-01-03T15:41:12Z",
      "updatedDate": "2026-01-03T15:41:12Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01213v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01213",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01027",
      "title": "A Platform for Interactive AI Character Experiences",
      "authors": [
        {
          "name": "Rafael Wampfler",
          "affiliation": null
        },
        {
          "name": "Chen Yang",
          "affiliation": null
        },
        {
          "name": "Dillon Elste",
          "affiliation": null
        },
        {
          "name": "Nikola Kovacevic",
          "affiliation": null
        },
        {
          "name": "Philine Witzig",
          "affiliation": null
        },
        {
          "name": "Markus Gross",
          "affiliation": null
        }
      ],
      "abstract": "From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.",
      "publishedDate": "2026-01-03T01:27:19Z",
      "updatedDate": "2026-01-03T01:27:19Z",
      "primaryCategory": "cs.HC",
      "arxivCategories": [
        "cs.HC",
        "cs.AI",
        "cs.CL",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01027v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01027",
      "comment": null,
      "journalRef": "SIGGRAPH Conference Papers '25, August 10-14, 2025, Vancouver, BC, Canada",
      "doi": "10.1145/3721238.3730762",
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02236",
      "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models",
      "authors": [
        {
          "name": "Yihao Liang",
          "affiliation": null
        },
        {
          "name": "Ze Wang",
          "affiliation": null
        },
        {
          "name": "Hao Chen",
          "affiliation": null
        },
        {
          "name": "Ximeng Sun",
          "affiliation": null
        },
        {
          "name": "Jialian Wu",
          "affiliation": null
        },
        {
          "name": "Xiaodong Yu",
          "affiliation": null
        },
        {
          "name": "Jiang Liu",
          "affiliation": null
        },
        {
          "name": "Emad Barsoum",
          "affiliation": null
        },
        {
          "name": "Zicheng Liu",
          "affiliation": null
        },
        {
          "name": "Niraj K. Jha",
          "affiliation": null
        }
      ],
      "abstract": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM",
      "publishedDate": "2026-01-05T16:09:22Z",
      "updatedDate": "2026-01-05T16:09:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02236v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02236",
      "comment": "33 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02201",
      "title": "CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents",
      "authors": [
        {
          "name": "Keyu Wang",
          "affiliation": null
        },
        {
          "name": "Bingchen Miao",
          "affiliation": null
        },
        {
          "name": "Wendong Bu",
          "affiliation": null
        },
        {
          "name": "Yu Wu",
          "affiliation": null
        },
        {
          "name": "Juncheng Li",
          "affiliation": null
        },
        {
          "name": "Shengyu Zhang",
          "affiliation": null
        },
        {
          "name": "Wenqiao Zhang",
          "affiliation": null
        },
        {
          "name": "Siliang Tang",
          "affiliation": null
        },
        {
          "name": "Jun Xiao",
          "affiliation": null
        },
        {
          "name": "Yueting Zhuang",
          "affiliation": null
        }
      ],
      "abstract": "The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.",
      "publishedDate": "2026-01-05T15:24:05Z",
      "updatedDate": "2026-01-05T15:24:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02201v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02201",
      "comment": "19 pages, 12 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02060",
      "title": "Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming",
      "authors": [
        {
          "name": "Nguyet-Anh H. Lang",
          "affiliation": null
        },
        {
          "name": "Eric Lang",
          "affiliation": null
        },
        {
          "name": "Thanh Le-Cong",
          "affiliation": null
        },
        {
          "name": "Bach Le",
          "affiliation": null
        },
        {
          "name": "Quyet-Thang Huynh",
          "affiliation": null
        }
      ],
      "abstract": "Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.",
      "publishedDate": "2026-01-05T12:33:37Z",
      "updatedDate": "2026-01-05T12:33:37Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL",
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02060v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02060",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01972",
      "title": "Hidden State Poisoning Attacks against Mamba-based Language Models",
      "authors": [
        {
          "name": "Alexandre Le Mercier",
          "affiliation": null
        },
        {
          "name": "Chris Develder",
          "affiliation": null
        },
        {
          "name": "Thomas Demeester",
          "affiliation": null
        }
      ],
      "abstract": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.",
      "publishedDate": "2026-01-05T10:27:19Z",
      "updatedDate": "2026-01-06T11:54:49Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01972v2",
      "arxivUrl": "https://arxiv.org/abs/2601.01972",
      "comment": "17 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01957",
      "title": "AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing",
      "authors": [
        {
          "name": "Tianbo Wang",
          "affiliation": null
        },
        {
          "name": "Yuqing Ma",
          "affiliation": null
        },
        {
          "name": "Kewei Liao",
          "affiliation": null
        },
        {
          "name": "Zhange Zhang",
          "affiliation": null
        },
        {
          "name": "Simin Li",
          "affiliation": null
        },
        {
          "name": "Jinyang Guo",
          "affiliation": null
        },
        {
          "name": "Xianglong Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.",
      "publishedDate": "2026-01-05T10:02:22Z",
      "updatedDate": "2026-01-05T10:02:22Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01957v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01957",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01870",
      "title": "Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion",
      "authors": [
        {
          "name": "Wenyu Shao",
          "affiliation": null
        },
        {
          "name": "Hongbo Liu",
          "affiliation": null
        },
        {
          "name": "Yunchuan Ma",
          "affiliation": null
        },
        {
          "name": "Ruili Wang",
          "affiliation": null
        }
      ],
      "abstract": "Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.",
      "publishedDate": "2026-01-05T08:00:03Z",
      "updatedDate": "2026-01-05T08:00:03Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01870v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01870",
      "comment": "Accepted by IEEE Transactions on Multimedia",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01827",
      "title": "Aspect Extraction from E-Commerce Product and Service Reviews",
      "authors": [
        {
          "name": "Valiant Lance D. Dionela",
          "affiliation": null
        },
        {
          "name": "Fatima Kriselle S. Dy",
          "affiliation": null
        },
        {
          "name": "Robin James M. Hombrebueno",
          "affiliation": null
        },
        {
          "name": "Aaron Rae M. Nicolas",
          "affiliation": null
        },
        {
          "name": "Charibeth K. Cheng",
          "affiliation": null
        },
        {
          "name": "Raphael W. Gonda",
          "affiliation": null
        }
      ],
      "abstract": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.",
      "publishedDate": "2026-01-05T06:45:51Z",
      "updatedDate": "2026-01-05T06:45:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01827v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01827",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01765",
      "title": "A New Benchmark for the Appropriate Evaluation of RTL Code Optimization",
      "authors": [
        {
          "name": "Yao Lu",
          "affiliation": null
        },
        {
          "name": "Shang Liu",
          "affiliation": null
        },
        {
          "name": "Hangan Zhou",
          "affiliation": null
        },
        {
          "name": "Wenji Fang",
          "affiliation": null
        },
        {
          "name": "Qijun Zhang",
          "affiliation": null
        },
        {
          "name": "Zhiyao Xie",
          "affiliation": null
        }
      ],
      "abstract": "The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.",
      "publishedDate": "2026-01-05T03:47:26Z",
      "updatedDate": "2026-01-05T03:47:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01765v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01765",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "tool-use",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01461",
      "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR",
      "authors": [
        {
          "name": "Yuxiang Mei",
          "affiliation": null
        },
        {
          "name": "Dongxing Xu",
          "affiliation": null
        },
        {
          "name": "Jiaen Liang",
          "affiliation": null
        },
        {
          "name": "Yanhua Long",
          "affiliation": null
        }
      ],
      "abstract": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.",
      "publishedDate": "2026-01-04T10:08:53Z",
      "updatedDate": "2026-01-04T10:08:53Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01461v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01461",
      "comment": "5 pages, 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01416",
      "title": "AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval",
      "authors": [
        {
          "name": "Yue Zhou",
          "affiliation": null
        },
        {
          "name": "Ran Ding",
          "affiliation": null
        },
        {
          "name": "Xue Yang",
          "affiliation": null
        },
        {
          "name": "Xue Jiang",
          "affiliation": null
        },
        {
          "name": "Xingzhao Liu",
          "affiliation": null
        }
      ],
      "abstract": "Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot",
      "publishedDate": "2026-01-04T07:38:51Z",
      "updatedDate": "2026-01-04T07:38:51Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01416v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01416",
      "comment": "12 pages, 9 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "planning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01332",
      "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness",
      "authors": [
        {
          "name": "Hossam Amer",
          "affiliation": null
        },
        {
          "name": "Maryam Dialameh",
          "affiliation": null
        },
        {
          "name": "Hossein Rajabzadeh",
          "affiliation": null
        },
        {
          "name": "Walid Ahmed",
          "affiliation": null
        },
        {
          "name": "Weiwei Zhang",
          "affiliation": null
        },
        {
          "name": "Yang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.",
      "publishedDate": "2026-01-04T02:33:30Z",
      "updatedDate": "2026-01-04T02:33:30Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01332v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01332",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01271",
      "title": "CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs",
      "authors": [
        {
          "name": "Qingxiao Tao",
          "affiliation": null
        },
        {
          "name": "Xiaodong Gu",
          "affiliation": null
        },
        {
          "name": "Hao Zhong",
          "affiliation": null
        },
        {
          "name": "Beijun Shen",
          "affiliation": null
        }
      ],
      "abstract": "Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.",
      "publishedDate": "2026-01-03T20:03:03Z",
      "updatedDate": "2026-01-03T20:03:03Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01271v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01271",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "tool-use",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "tool-use",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01244",
      "title": "Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure",
      "authors": [
        {
          "name": "Zsolt Csibi",
          "affiliation": null
        },
        {
          "name": "Bence György Gortka",
          "affiliation": null
        },
        {
          "name": "Natabara Gyöngyössy",
          "affiliation": null
        },
        {
          "name": "Kornél Nagy",
          "affiliation": null
        },
        {
          "name": "Dávid Márk Nemeskey",
          "affiliation": null
        },
        {
          "name": "Martin Sallai",
          "affiliation": null
        },
        {
          "name": "András Simonyi",
          "affiliation": null
        },
        {
          "name": "András Márk Szekeres",
          "affiliation": null
        },
        {
          "name": "Gábor Palkó",
          "affiliation": null
        }
      ],
      "abstract": "We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.",
      "publishedDate": "2026-01-03T17:32:48Z",
      "updatedDate": "2026-01-03T17:32:48Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01244v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01244",
      "comment": "18 pages, 1 figures. To appear in the XXII. Magyar Számítógépes Nyelvészeti Konferencia (MSZNY 2026)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01215",
      "title": "Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code",
      "authors": [
        {
          "name": "Prateek Rajput",
          "affiliation": null
        },
        {
          "name": "Yewei Song",
          "affiliation": null
        },
        {
          "name": "Abdoul Aziz Bonkoungou",
          "affiliation": null
        },
        {
          "name": "Iyiola E. Olatunji",
          "affiliation": null
        },
        {
          "name": "Abdoul Kader Kabore",
          "affiliation": null
        },
        {
          "name": "Jacques Klein",
          "affiliation": null
        },
        {
          "name": "Tegawendé F. Bissyandé",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.",
      "publishedDate": "2026-01-03T15:42:21Z",
      "updatedDate": "2026-01-03T15:42:21Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01215v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01215",
      "comment": "11 Pages, 11 figures, Accepted at ICSE SEIP",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01184",
      "title": "SecureCodeRL: Security-Aware Reinforcement Learning for Code Generation with Partial-Credit Rewards",
      "authors": [
        {
          "name": "Suryansh Singh Sijwali",
          "affiliation": null
        },
        {
          "name": "Suman Saha",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can generate plausible code, but in settings that require exact stdin/stdout behavior they frequently produce programs that compile yet fail tests, and in some cases they introduce security-sensitive patterns. This paper presents SecureCodeRL, a reinforcement learning (RL) pipeline for security-aware code generation that optimizes a combined reward R = αRfunc + \\b{eta}Rsec. The key idea is a partial-credit functional reward that assigns intermediate scores for syntactic validity, successful execution, and producing output, reducing reward sparsity that otherwise stalls learning on competitive programming style tasks. I evaluate supervised fine-tuning (SFT) and PPO variants on a small held-out prompt set from APPS+ and observe that PPO with partial credit (using a continued-training variant) improves syntax validity from 45% (SFT) to 60% and achieves the only non-zero test success signal in this pilot evaluation (5% at-least-one-test-pass), while remaining 100% clean under Bandit static analysis. Although Bandit findings were absent in this small evaluation, the security term is integrated into training to discourage insecure shortcuts when they appear.",
      "publishedDate": "2026-01-03T13:36:36Z",
      "updatedDate": "2026-01-03T13:36:36Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01184v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01184",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "rag",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01162",
      "title": "Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models",
      "authors": [
        {
          "name": "Zihua Yang",
          "affiliation": null
        },
        {
          "name": "Xin Liao",
          "affiliation": null
        },
        {
          "name": "Yiqun Zhang",
          "affiliation": null
        },
        {
          "name": "Yiu-ming Cheung",
          "affiliation": null
        }
      ],
      "abstract": "Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE",
      "publishedDate": "2026-01-03T11:37:46Z",
      "updatedDate": "2026-01-03T11:37:46Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01162v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01162",
      "comment": "Submitted to ICPR 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01129",
      "title": "RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian",
      "authors": [
        {
          "name": "Kla Tantithamthavorn",
          "affiliation": null
        },
        {
          "name": "Yaotian Zou",
          "affiliation": null
        },
        {
          "name": "Andy Wong",
          "affiliation": null
        },
        {
          "name": "Michael Gupta",
          "affiliation": null
        },
        {
          "name": "Zhe Wang",
          "affiliation": null
        },
        {
          "name": "Mike Buller",
          "affiliation": null
        },
        {
          "name": "Ryan Jiang",
          "affiliation": null
        },
        {
          "name": "Matthew Watson",
          "affiliation": null
        },
        {
          "name": "Minwoo Jeong",
          "affiliation": null
        },
        {
          "name": "Kun Chen",
          "affiliation": null
        },
        {
          "name": "Ming Wu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning? In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).",
      "publishedDate": "2026-01-03T09:27:56Z",
      "updatedDate": "2026-01-03T09:27:56Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01129v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01129",
      "comment": "Accepted at the 48th International Conference on Software Engineering (ICSE'26), SEIP Track. 12 Pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00993",
      "title": "WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift",
      "authors": [
        {
          "name": "Julian D. Santamaria",
          "affiliation": null
        },
        {
          "name": "Claudia Isaza",
          "affiliation": null
        },
        {
          "name": "Jhony H. Giraldo",
          "affiliation": null
        }
      ],
      "abstract": "Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.",
      "publishedDate": "2026-01-02T21:58:19Z",
      "updatedDate": "2026-01-02T21:58:19Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00993v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00993",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00927",
      "title": "Measuring Social Media Polarization Using Large Language Models and Heuristic Rules",
      "authors": [
        {
          "name": "Jawad Chowdhury",
          "affiliation": null
        },
        {
          "name": "Rezaur Rashid",
          "affiliation": null
        },
        {
          "name": "Gabriel Terejanu",
          "affiliation": null
        }
      ],
      "abstract": "Understanding affective polarization in online discourse is crucial for evaluating the societal impact of social media interactions. This study presents a novel framework that leverages large language models (LLMs) and domain-informed heuristics to systematically analyze and quantify affective polarization in discussions on divisive topics such as climate change and gun control. Unlike most prior approaches that relied on sentiment analysis or predefined classifiers, our method integrates LLMs to extract stance, affective tone, and agreement patterns from large-scale social media discussions. We then apply a rule-based scoring system capable of quantifying affective polarization even in small conversations consisting of single interactions, based on stance alignment, emotional content, and interaction dynamics. Our analysis reveals distinct polarization patterns that are event dependent: (i) anticipation-driven polarization, where extreme polarization escalates before well-publicized events, and (ii) reactive polarization, where intense affective polarization spikes immediately after sudden, high-impact events. By combining AI-driven content annotation with domain-informed scoring, our framework offers a scalable and interpretable approach to measuring affective polarization. The source code is publicly available at: https://github.com/hasanjawad001/llm-social-media-polarization.",
      "publishedDate": "2026-01-02T01:11:58Z",
      "updatedDate": "2026-01-02T01:11:58Z",
      "primaryCategory": "cs.SI",
      "arxivCategories": [
        "cs.SI",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00927v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00927",
      "comment": "Foundations and Applications of Big Data Analytics (FAB), Niagara Falls, Canada, 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.01948",
      "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation",
      "authors": [
        {
          "name": "Zhihao Gu",
          "affiliation": null
        },
        {
          "name": "Ming Yang",
          "affiliation": null
        },
        {
          "name": "Difan Zou",
          "affiliation": null
        },
        {
          "name": "Dong Xu",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.",
      "publishedDate": "2026-01-05T09:56:24Z",
      "updatedDate": "2026-01-05T09:56:24Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.01948v1",
      "arxivUrl": "https://arxiv.org/abs/2601.01948",
      "comment": "Accepted to AAAI2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "planning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "planning",
          "prompting",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.00978",
      "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly",
      "authors": [
        {
          "name": "Yanyi Chen",
          "affiliation": null
        },
        {
          "name": "Min Deng",
          "affiliation": null
        }
      ],
      "abstract": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.",
      "publishedDate": "2026-01-02T20:12:50Z",
      "updatedDate": "2026-01-02T20:12:50Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.00978v1",
      "arxivUrl": "https://arxiv.org/abs/2601.00978",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "agents",
        "planning",
        "multi-agent",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "multi-agent",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02316",
      "title": "DatBench: Discriminative, Faithful, and Efficient VLM Evaluations",
      "authors": [
        {
          "name": "Siddharth Joshi",
          "affiliation": null
        },
        {
          "name": "Haoli Yin",
          "affiliation": null
        },
        {
          "name": "Rishabh Adiga",
          "affiliation": null
        },
        {
          "name": "Ricardo Monti",
          "affiliation": null
        },
        {
          "name": "Aldo Carranza",
          "affiliation": null
        },
        {
          "name": "Alex Fang",
          "affiliation": null
        },
        {
          "name": "Alvin Deng",
          "affiliation": null
        },
        {
          "name": "Amro Abbas",
          "affiliation": null
        },
        {
          "name": "Brett Larsen",
          "affiliation": null
        },
        {
          "name": "Cody Blakeney",
          "affiliation": null
        },
        {
          "name": "Darren Teh",
          "affiliation": null
        },
        {
          "name": "David Schwab",
          "affiliation": null
        },
        {
          "name": "Fan Pan",
          "affiliation": null
        },
        {
          "name": "Haakon Mongstad",
          "affiliation": null
        },
        {
          "name": "Jack Urbanek",
          "affiliation": null
        },
        {
          "name": "Jason Lee",
          "affiliation": null
        },
        {
          "name": "Jason Telanoff",
          "affiliation": null
        },
        {
          "name": "Josh Wills",
          "affiliation": null
        },
        {
          "name": "Kaleigh Mentzer",
          "affiliation": null
        },
        {
          "name": "Luke Merrick",
          "affiliation": null
        },
        {
          "name": "Parth Doshi",
          "affiliation": null
        },
        {
          "name": "Paul Burstein",
          "affiliation": null
        },
        {
          "name": "Pratyush Maini",
          "affiliation": null
        },
        {
          "name": "Scott Loftin",
          "affiliation": null
        },
        {
          "name": "Spandan Das",
          "affiliation": null
        },
        {
          "name": "Tony Jiang",
          "affiliation": null
        },
        {
          "name": "Vineeth Dorna",
          "affiliation": null
        },
        {
          "name": "Zhengping Wang",
          "affiliation": null
        },
        {
          "name": "Bogdan Gaza",
          "affiliation": null
        },
        {
          "name": "Ari Morcos",
          "affiliation": null
        },
        {
          "name": "Matthew Leavitt",
          "affiliation": null
        }
      ],
      "abstract": "Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.",
      "publishedDate": "2026-01-05T18:07:51Z",
      "updatedDate": "2026-01-05T18:07:51Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02316v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02316",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-06T03:24:26.988Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03251",
      "title": "NavAI: A Generalizable LLM Framework for Navigation Tasks in Virtual Reality Environments",
      "authors": [
        {
          "name": "Xue Qin",
          "affiliation": null
        },
        {
          "name": "Matthew DiGiovanni",
          "affiliation": null
        }
      ],
      "abstract": "Navigation is one of the fundamental tasks for automated exploration in Virtual Reality (VR). Existing technologies primarily focus on path optimization in 360-degree image datasets and 3D simulators, which cannot be directly applied to immersive VR environments. To address this gap, we present NavAI, a generalizable large language model (LLM)-based navigation framework that supports both basic actions and complex goal-directed tasks across diverse VR applications. We evaluate NavAI in three distinct VR environments through goal-oriented and exploratory tasks. Results show that it achieves high accuracy, with an 89% success rate in goal-oriented tasks. Our analysis also highlights current limitations of relying entirely on LLMs, particularly in scenarios that require dynamic goal assessment. Finally, we discuss the limitations observed during the experiments and offer insights for future research directions.",
      "publishedDate": "2026-01-06T18:54:54Z",
      "updatedDate": "2026-01-06T18:54:54Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03251v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03251",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03211",
      "title": "Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers",
      "authors": [
        {
          "name": "Yue Kang",
          "affiliation": null
        },
        {
          "name": "Zhuoyi Huang",
          "affiliation": null
        },
        {
          "name": "Benji Schussheim",
          "affiliation": null
        },
        {
          "name": "Diana Licon",
          "affiliation": null
        },
        {
          "name": "Dina Atia",
          "affiliation": null
        },
        {
          "name": "Shixing Cao",
          "affiliation": null
        },
        {
          "name": "Jacob Danovitch",
          "affiliation": null
        },
        {
          "name": "Kunho Kim",
          "affiliation": null
        },
        {
          "name": "Billy Norcilien",
          "affiliation": null
        },
        {
          "name": "Jonah Karpman",
          "affiliation": null
        },
        {
          "name": "Mahmound Sayed",
          "affiliation": null
        },
        {
          "name": "Mike Taylor",
          "affiliation": null
        },
        {
          "name": "Tao Sun",
          "affiliation": null
        },
        {
          "name": "Pavel Metrikov",
          "affiliation": null
        },
        {
          "name": "Vipul Agarwal",
          "affiliation": null
        },
        {
          "name": "Chris Quirk",
          "affiliation": null
        },
        {
          "name": "Ye-Yi Wang",
          "affiliation": null
        },
        {
          "name": "Nick Craswell",
          "affiliation": null
        },
        {
          "name": "Irene Shaffer",
          "affiliation": null
        },
        {
          "name": "Tianwei Chen",
          "affiliation": null
        },
        {
          "name": "Sulaiman Vesal",
          "affiliation": null
        },
        {
          "name": "Soundar Srinivasan",
          "affiliation": null
        }
      ],
      "abstract": "In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.",
      "publishedDate": "2026-01-06T17:48:40Z",
      "updatedDate": "2026-01-06T17:48:40Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03211v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03211",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03087",
      "title": "Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs",
      "authors": [
        {
          "name": "David Hartmann",
          "affiliation": null
        },
        {
          "name": "Lena Pohlmann",
          "affiliation": null
        },
        {
          "name": "Lelia Hanslik",
          "affiliation": null
        },
        {
          "name": "Noah Gießing",
          "affiliation": null
        },
        {
          "name": "Bettina Berendt",
          "affiliation": null
        },
        {
          "name": "Pieter Delobelle",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) exhibit systematic biases across demographic groups. Auditing is proposed as an accountability tool for black-box LLM applications, but suffers from resource-intensive query access. We conceptualise auditing as uncertainty estimation over a target fairness metric and introduce BAFA, the Bounded Active Fairness Auditor for query-efficient auditing of black-box LLMs. BAFA maintains a version space of surrogate models consistent with queried scores and computes uncertainty intervals for fairness metrics (e.g., $Δ$ AUC) via constrained empirical risk minimisation. Active query selection narrows these intervals to reduce estimation error. We evaluate BAFA on two standard fairness dataset case studies: \\textsc{CivilComments} and \\textsc{Bias-in-Bios}, comparing against stratified sampling, power sampling, and ablations. BAFA achieves target error thresholds with up to 40$\\times$ fewer queries than stratified sampling (e.g., 144 vs 5,956 queries at $\\varepsilon=0.02$ for \\textsc{CivilComments}) for tight thresholds, demonstrates substantially better performance over time, and shows lower variance across runs. These results suggest that active sampling can reduce resources needed for independent fairness auditing with LLMs, supporting continuous model evaluations.",
      "publishedDate": "2026-01-06T15:22:23Z",
      "updatedDate": "2026-01-06T15:22:23Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03087v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03087",
      "comment": "Submitted to ACL ARR 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03047",
      "title": "When the Coffee Feature Activates on Coffins: An Analysis of Feature Extraction and Steering for Mechanistic Interpretability",
      "authors": [
        {
          "name": "Raphael Ronge",
          "affiliation": null
        },
        {
          "name": "Markus Maier",
          "affiliation": null
        },
        {
          "name": "Frederick Eberhardt",
          "affiliation": null
        }
      ],
      "abstract": "Recent work by Anthropic on Mechanistic interpretability claims to understand and control Large Language Models by extracting human-interpretable features from their neural activation patterns using sparse autoencoders (SAEs). If successful, this approach offers one of the most promising routes for human oversight in AI safety. We conduct an initial stress-test of these claims by replicating their main results with open-source SAEs for Llama 3.1. While we successfully reproduce basic feature extraction and steering capabilities, our investigation suggests that major caution is warranted regarding the generalizability of these claims. We find that feature steering exhibits substantial fragility, with sensitivity to layer selection, steering magnitude, and context. We observe non-standard activation behavior and demonstrate the difficulty to distinguish thematically similar features from one another. While SAE-based interpretability produces compelling demonstrations in selected cases, current methods often fall short of the systematic reliability required for safety-critical applications. This suggests a necessary shift in focus from prioritizing interpretability of internal representations toward reliable prediction and control of model output. Our work contributes to a more nuanced understanding of what mechanistic interpretability has achieved and highlights fundamental challenges for AI safety that remain unresolved.",
      "publishedDate": "2026-01-06T14:29:51Z",
      "updatedDate": "2026-01-06T14:29:51Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03047v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03047",
      "comment": "33 pages (65 with appendix), 1 figure",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03034",
      "title": "NorwAI's Large Language Models: Technical Report",
      "authors": [
        {
          "name": "Jon Atle Gulla",
          "affiliation": null
        },
        {
          "name": "Peng Liu",
          "affiliation": null
        },
        {
          "name": "Lemei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.",
      "publishedDate": "2026-01-06T14:06:55Z",
      "updatedDate": "2026-01-06T14:06:55Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03034v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03034",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02991",
      "title": "Towards Faithful Reasoning in Comics for Small MLLMs",
      "authors": [
        {
          "name": "Chengcheng Feng",
          "affiliation": null
        },
        {
          "name": "Haojie Yin",
          "affiliation": null
        },
        {
          "name": "Yucheng Jin",
          "affiliation": null
        },
        {
          "name": "Kaizhu Huang",
          "affiliation": null
        }
      ],
      "abstract": "Comic-based visual question answering (CVQA) poses distinct challenges to multimodal large language models (MLLMs) due to its reliance on symbolic abstraction, narrative logic, and humor, which differ from conventional VQA tasks. Although Chain-of-Thought (CoT) prompting is widely used to enhance MLLM reasoning, surprisingly, its direct application to CVQA often degrades performance, especially in small-scale models. Our theoretical and empirical analyses reveal that standard CoT in CVQA suffers from state entanglement, spurious transitions, and exploration inefficiency, with small models particularly vulnerable in resource-constrained settings. To address these issues, we propose a novel comic reasoning framework, designed to produce more faithful and transferable reasoning chains in small MLLMs. Specifically, our framework combines modular CoT generation with GRPO-based reinforcement fine-tuning and a novel structured reward. Beyond comic VQA, we further evaluate our approach on a broader class of humor-centric and abstract visual reasoning tasks, including meme understanding and editorial cartoon interpretation. Across five challenging benchmarks, our 3B model outperforms state-of-the-art methods, and plug-in experiments yield an additional average improvement of $\\mathbf{12.1\\%}$ across different MLLMs.",
      "publishedDate": "2026-01-06T13:00:21Z",
      "updatedDate": "2026-01-06T13:00:21Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02991v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02991",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02927",
      "title": "PrismVAU: Prompt-Refined Inference System for Multimodal Video Anomaly Understanding",
      "authors": [
        {
          "name": "Iñaki Erregue",
          "affiliation": null
        },
        {
          "name": "Kamal Nasrollahi",
          "affiliation": null
        },
        {
          "name": "Sergio Escalera",
          "affiliation": null
        }
      ],
      "abstract": "Video Anomaly Understanding (VAU) extends traditional Video Anomaly Detection (VAD) by not only localizing anomalies but also describing and reasoning about their context. Existing VAU approaches often rely on fine-tuned multimodal large language models (MLLMs) or external modules such as video captioners, which introduce costly annotations, complex training pipelines, and high inference overhead. In this work, we introduce PrismVAU, a lightweight yet effective system for real-time VAU that leverages a single off-the-shelf MLLM for anomaly scoring, explanation, and prompt optimization. PrismVAU operates in two complementary stages: (1) a coarse anomaly scoring module that computes frame-level anomaly scores via similarity to textual anchors, and (2) an MLLM-based refinement module that contextualizes anomalies through system and user prompts. Both textual anchors and prompts are optimized with a weakly supervised Automatic Prompt Engineering (APE) framework. Extensive experiments on standard VAD benchmarks demonstrate that PrismVAU delivers competitive detection performance and interpretable anomaly explanations -- without relying on instruction tuning, frame-level annotations, and external modules or dense processing -- making it an efficient and practical solution for real-world applications.",
      "publishedDate": "2026-01-06T11:11:06Z",
      "updatedDate": "2026-01-06T11:11:06Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02927v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02927",
      "comment": "This paper has been accepted to the 6th Workshop on Real-World Surveillance: Applications and Challenges (WACV 2025)",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02915",
      "title": "ChemBART: A Pre-trained BART Model Assisting Organic Chemistry Analysis",
      "authors": [
        {
          "name": "Kenan Li",
          "affiliation": null
        },
        {
          "name": "Yijian Zhang",
          "affiliation": null
        },
        {
          "name": "Jin Wang",
          "affiliation": null
        },
        {
          "name": "Haipeng Gan",
          "affiliation": null
        },
        {
          "name": "Zeying Sun",
          "affiliation": null
        },
        {
          "name": "Xiaoguang Lei",
          "affiliation": null
        },
        {
          "name": "Hao Dong",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have demonstrated transformative potential across diverse fields. While LLMs have been applied to molecular simplified molecular input line entry system (SMILES) in computer-aided synthesis planning (CASP), existing methodologies typically address single tasks, such as precursor prediction. We introduce ChemBART, a SMILES-based LLM pre-trained on chemical reactions, which enables a unified model for multiple downstream chemical tasks--achieving the paradigm of \"one model, one pre-training, multiple tasks.\" By leveraging outputs from a mask-filling pre-training task on reaction expressions, ChemBART effectively solves a variety of chemical problems, including precursor/reagent generation, temperature-yield regression, molecular property classification, and optimizing the policy and value functions within a reinforcement learning framework, integrated with Monte Carlo tree search for multi-step synthesis route design. Unlike single-molecule pre-trained LLMs constrained to specific applications, ChemBART addresses broader chemical challenges and integrates them for comprehensive synthesis planning. Crucially, ChemBART-designed multi-step synthesis routes and reaction conditions directly inspired wet-lab validation, which confirmed shorter pathways with ~30% yield improvement over literature benchmarks. Our work validates the power of reaction-focused pre-training and showcases the broad utility of ChemBART in advancing the complete synthesis planning cycle.",
      "publishedDate": "2026-01-06T10:55:38Z",
      "updatedDate": "2026-01-06T10:55:38Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02915v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02915",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02783",
      "title": "EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework",
      "authors": [
        {
          "name": "Junjue Wang",
          "affiliation": null
        },
        {
          "name": "Yanfei Zhong",
          "affiliation": null
        },
        {
          "name": "Zihang Chen",
          "affiliation": null
        },
        {
          "name": "Zhuo Zheng",
          "affiliation": null
        },
        {
          "name": "Ailong Ma",
          "affiliation": null
        },
        {
          "name": "Liangpei Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Earth vision has achieved milestones in geospatial object recognition but lacks exploration in object-relational reasoning, limiting comprehensive scene understanding. To address this, a progressive Earth vision-language understanding and generation framework is proposed, including a multi-task dataset (EarthVLSet) and a semantic-guided network (EarthVLNet). Focusing on city planning applications, EarthVLSet includes 10.9k sub-meter resolution remote sensing images, land-cover masks, and 761.5k textual pairs involving both multiple-choice and open-ended visual question answering (VQA) tasks. In an object-centric way, EarthVLNet is proposed to progressively achieve semantic segmentation, relational reasoning, and comprehensive understanding. The first stage involves land-cover segmentation to generate object semantics for VQA guidance. Guided by pixel-wise semantics, the object awareness based large language model (LLM) performs relational reasoning and knowledge summarization to generate the required answers. As for optimization, the numerical difference loss is proposed to dynamically add difference penalties, addressing the various objects' statistics. Three benchmarks, including semantic segmentation, multiple-choice, and open-ended VQA demonstrated the superiorities of EarthVLNet, yielding three future directions: 1) segmentation features consistently enhance VQA performance even in cross-dataset scenarios; 2) multiple-choice tasks show greater sensitivity to the vision encoder than to the language decoder; and 3) open-ended tasks necessitate advanced vision encoders and language decoders for an optimal performance. We believe this dataset and method will provide a beneficial benchmark that connects ''image-mask-text'', advancing geographical applications for Earth vision.",
      "publishedDate": "2026-01-06T07:41:44Z",
      "updatedDate": "2026-01-06T07:41:44Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02783v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02783",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "planning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02764",
      "title": "Netflix Artwork Personalization via LLM Post-training",
      "authors": [
        {
          "name": "Hyunji Nam",
          "affiliation": null
        },
        {
          "name": "Sejoon Oh",
          "affiliation": null
        },
        {
          "name": "Emma Kong",
          "affiliation": null
        },
        {
          "name": "Yesu Feng",
          "affiliation": null
        },
        {
          "name": "Moumita Bhattacharya",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have demonstrated success in various applications of user recommendation and personalization across e-commerce and entertainment. On many entertainment platforms such as Netflix, users typically interact with a wide range of titles, each represented by an artwork. Since users have diverse preferences, an artwork that appeals to one type of user may not resonate with another with different preferences. Given this user heterogeneity, our work explores the novel problem of personalized artwork recommendations according to diverse user preferences. Similar to the multi-dimensional nature of users' tastes, titles contain different themes and tones that may appeal to different viewers. For example, the same title might feature both heartfelt family drama and intense action scenes. Users who prefer romantic content may like the artwork emphasizing emotional warmth between the characters, while those who prefer action thrillers may find high-intensity action scenes more intriguing. Rather than a one-size-fits-all approach, we conduct post-training of pre-trained LLMs to make personalized artwork recommendations, selecting the most preferred visual representation of a title for each user and thereby improving user satisfaction and engagement. Our experimental results with Llama 3.1 8B models (trained on a dataset of 110K data points and evaluated on 5K held-out user-title pairs) show that the post-trained LLMs achieve 3-5\\% improvements over the Netflix production model, suggesting a promising direction for granular personalized recommendations using LLMs.",
      "publishedDate": "2026-01-06T06:56:53Z",
      "updatedDate": "2026-01-06T06:56:53Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02764v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02764",
      "comment": "6 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [],
      "tags": {
        "auto": [],
        "manual": []
      }
    },
    {
      "id": "2601.02757",
      "title": "LLM Agent Framework for Intelligent Change Analysis in Urban Environment using Remote Sensing Imagery",
      "authors": [
        {
          "name": "Zixuan Xiao",
          "affiliation": null
        },
        {
          "name": "Jun Ma",
          "affiliation": null
        }
      ],
      "abstract": "Existing change detection methods often lack the versatility to handle diverse real-world queries and the intelligence for comprehensive analysis. This paper presents a general agent framework, integrating Large Language Models (LLM) with vision foundation models to form ChangeGPT. A hierarchical structure is employed to mitigate hallucination. The agent was evaluated on a curated dataset of 140 questions categorized by real-world scenarios, encompassing various question types (e.g., Size, Class, Number) and complexities. The evaluation assessed the agent's tool selection ability (Precision/Recall) and overall query accuracy (Match). ChangeGPT, especially with a GPT-4-turbo backend, demonstrated superior performance, achieving a 90.71 % Match rate. Its strength lies particularly in handling change-related queries requiring multi-step reasoning and robust tool selection. Practical effectiveness was further validated through a real-world urban change monitoring case study in Qianhai Bay, Shenzhen. By providing intelligence, adaptability, and multi-type change analysis, ChangeGPT offers a powerful solution for decision-making in remote sensing applications.",
      "publishedDate": "2026-01-06T06:49:51Z",
      "updatedDate": "2026-01-06T06:49:51Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02757v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02757",
      "comment": null,
      "journalRef": "Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341 Automation in Construction 177 (2025) 106341",
      "doi": "10.1016/j.autcon.2025.106341",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02752",
      "title": "EComStage: Stage-wise and Orientation-specific Benchmarking for Large Language Models in E-commerce",
      "authors": [
        {
          "name": "Kaiyan Zhao",
          "affiliation": null
        },
        {
          "name": "Zijie Meng",
          "affiliation": null
        },
        {
          "name": "Zheyong Xie",
          "affiliation": null
        },
        {
          "name": "Jin Duan",
          "affiliation": null
        },
        {
          "name": "Yao Hu",
          "affiliation": null
        },
        {
          "name": "Zuozhu Liu",
          "affiliation": null
        },
        {
          "name": "Shaosheng Cao",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based agents are increasingly deployed in e-commerce applications to assist customer services in tasks such as product inquiries, recommendations, and order management. Existing benchmarks primarily evaluate whether these agents successfully complete the final task, overlooking the intermediate reasoning stages that are crucial for effective decision-making. To address this gap, we propose EComStage, a unified benchmark for evaluating agent-capable LLMs across the comprehensive stage-wise reasoning process: Perception (understanding user intent), Planning (formulating an action plan), and Action (executing the decision). EComStage evaluates LLMs through seven separate representative tasks spanning diverse e-commerce scenarios, with all samples human-annotated and quality-checked. Unlike prior benchmarks that focus only on customer-oriented interactions, EComStage also evaluates merchant-oriented scenarios, including promotion management, content review, and operational support relevant to real-world applications. We evaluate a wide range of over 30 LLMs, spanning from 1B to over 200B parameters, including open-source models and closed-source APIs, revealing stage/orientation- specific strengths and weaknesses. Our results provide fine-grained, actionable insights for designing and optimizing LLM-based agents in real-world e-commerce settings.",
      "publishedDate": "2026-01-06T06:39:16Z",
      "updatedDate": "2026-01-06T06:39:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02752v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02752",
      "comment": "preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "planning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02749",
      "title": "The Path Ahead for Agentic AI: Challenges and Opportunities",
      "authors": [
        {
          "name": "Nadia Sibai",
          "affiliation": null
        },
        {
          "name": "Yara Ahmed",
          "affiliation": null
        },
        {
          "name": "Serry Sibaee",
          "affiliation": null
        },
        {
          "name": "Sawsan AlHalawani",
          "affiliation": null
        },
        {
          "name": "Adel Ammar",
          "affiliation": null
        },
        {
          "name": "Wadii Boulila",
          "affiliation": null
        }
      ],
      "abstract": "The evolution of Large Language Models (LLMs) from passive text generators to autonomous, goal-driven systems represents a fundamental shift in artificial intelligence. This chapter examines the emergence of agentic AI systems that integrate planning, memory, tool use, and iterative reasoning to operate autonomously in complex environments. We trace the architectural progression from statistical models to transformer-based systems, identifying capabilities that enable agentic behavior: long-range reasoning, contextual awareness, and adaptive decision-making. The chapter provides three contributions: (1) a synthesis of how LLM capabilities extend toward agency through reasoning-action-reflection loops; (2) an integrative framework describing core components perception, memory, planning, and tool execution that bridge LLMs with autonomous behavior; (3) a critical assessment of applications and persistent challenges in safety, alignment, reliability, and sustainability. Unlike existing surveys, we focus on the architectural transition from language understanding to autonomous action, emphasizing the technical gaps that must be resolved before deployment. We identify critical research priorities, including verifiable planning, scalable multi-agent coordination, persistent memory architectures, and governance frameworks. Responsible advancement requires simultaneous progress in technical robustness, interpretability, and ethical safeguards to realize potential while mitigating risks of misalignment and unintended consequences.",
      "publishedDate": "2026-01-06T06:31:42Z",
      "updatedDate": "2026-01-06T06:31:42Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02749v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02749",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "multi-agent",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "tool-use",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02736",
      "title": "Hypothesize-Then-Verify: Speculative Root Cause Analysis for Microservices with Pathwise Parallelism",
      "authors": [
        {
          "name": "Lingzhe Zhang",
          "affiliation": null
        },
        {
          "name": "Tong Jia",
          "affiliation": null
        },
        {
          "name": "Yunpeng Zhai",
          "affiliation": null
        },
        {
          "name": "Leyi Pan",
          "affiliation": null
        },
        {
          "name": "Chiming Duan",
          "affiliation": null
        },
        {
          "name": "Minghua He",
          "affiliation": null
        },
        {
          "name": "Pei Xiao",
          "affiliation": null
        },
        {
          "name": "Ying Li",
          "affiliation": null
        }
      ],
      "abstract": "Microservice systems have become the backbone of cloud-native enterprise applications due to their resource elasticity, loosely coupled architecture, and lightweight deployment. Yet, the intrinsic complexity and dynamic runtime interactions of such systems inevitably give rise to anomalies. Ensuring system reliability therefore hinges on effective root cause analysis (RCA), which entails not only localizing the source of anomalies but also characterizing the underlying failures in a timely and interpretable manner. Recent advances in intelligent RCA techniques, particularly those powered by large language models (LLMs), have demonstrated promising capabilities, as LLMs reduce reliance on handcrafted features while offering cross-platform adaptability, task generalization, and flexibility. However, existing LLM-based methods still suffer from two critical limitations: (a) limited exploration diversity, which undermines accuracy, and (b) heavy dependence on large-scale LLMs, which results in slow inference. To overcome these challenges, we propose SpecRCA, a speculative root cause analysis framework for microservices that adopts a \\textit{hypothesize-then-verify} paradigm. SpecRCA first leverages a hypothesis drafting module to rapidly generate candidate root causes, and then employs a parallel root cause verifier to efficiently validate them. Preliminary experiments on the AIOps 2022 dataset demonstrate that SpecRCA achieves superior accuracy and efficiency compared to existing approaches, highlighting its potential as a practical solution for scalable and interpretable RCA in complex microservice environments.",
      "publishedDate": "2026-01-06T05:58:25Z",
      "updatedDate": "2026-01-06T05:58:25Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02736v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02736",
      "comment": "accepted by ICSE-NIER'26",
      "journalRef": null,
      "doi": "10.1145/3786582.3786803",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02669",
      "title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
      "authors": [
        {
          "name": "Hongzhan Lin",
          "affiliation": null
        },
        {
          "name": "Zixin Chen",
          "affiliation": null
        },
        {
          "name": "Zhiqi Shen",
          "affiliation": null
        },
        {
          "name": "Ziyang Luo",
          "affiliation": null
        },
        {
          "name": "Zhen Ye",
          "affiliation": null
        },
        {
          "name": "Jing Ma",
          "affiliation": null
        },
        {
          "name": "Tat-Seng Chua",
          "affiliation": null
        },
        {
          "name": "Guandong Xu",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications.",
      "publishedDate": "2026-01-06T02:51:56Z",
      "updatedDate": "2026-01-06T02:51:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02669v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02669",
      "comment": "17 pages, 21 figures, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02632",
      "title": "TAAF: A Trace Abstraction and Analysis Framework Synergizing Knowledge Graphs and LLMs",
      "authors": [
        {
          "name": "Alireza Ezaz",
          "affiliation": null
        },
        {
          "name": "Ghazal Khodabandeh",
          "affiliation": null
        },
        {
          "name": "Majid Babaei",
          "affiliation": null
        },
        {
          "name": "Naser Ezzati-Jivan",
          "affiliation": null
        }
      ],
      "abstract": "Execution traces are a critical source of information for understanding, debugging, and optimizing complex software systems. However, traces from OS kernels or large-scale applications like Chrome or MySQL are massive and difficult to analyze. Existing tools rely on predefined analyses, and custom insights often require writing domain-specific scripts, which is an error-prone and time-consuming task. This paper introduces TAAF (Trace Abstraction and Analysis Framework), a novel approach that combines time-indexing, knowledge graphs (KGs), and large language models (LLMs) to transform raw trace data into actionable insights. TAAF constructs a time-indexed KG from trace events to capture relationships among entities such as threads, CPUs, and system resources. An LLM then interprets query-specific subgraphs to answer natural-language questions, reducing the need for manual inspection and deep system expertise. To evaluate TAAF, we introduce TraceQA-100, a benchmark of 100 questions grounded in real kernel traces. Experiments across three LLMs and multiple temporal settings show that TAAF improves answer accuracy by up to 31.2%, particularly in multi-hop and causal reasoning tasks. We further analyze where graph-grounded reasoning helps and where limitations remain, offering a foundation for next-generation trace analysis tools.",
      "publishedDate": "2026-01-06T01:04:05Z",
      "updatedDate": "2026-01-06T01:04:05Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02632v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02632",
      "comment": "Accepted to ICSE 2026. DOI 10.1145/3744916.3787832",
      "journalRef": null,
      "doi": "10.1145/3744916.3787832",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02574",
      "title": "Fact-Checking with Large Language Models via Probabilistic Certainty and Consistency",
      "authors": [
        {
          "name": "Haoran Wang",
          "affiliation": null
        },
        {
          "name": "Maryam Khalid",
          "affiliation": null
        },
        {
          "name": "Qiong Wu",
          "affiliation": null
        },
        {
          "name": "Jian Gao",
          "affiliation": null
        },
        {
          "name": "Cheng Cao",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly used in applications requiring factual accuracy, yet their outputs often contain hallucinated responses. While fact-checking can mitigate these errors, existing methods typically retrieve external evidence indiscriminately, overlooking the model's internal knowledge and potentially introducing irrelevant noise. Moreover, current systems lack targeted mechanisms to resolve specific uncertainties in the model's reasoning. Inspired by how humans fact-check, we argue that LLMs should adaptively decide whether to rely on internal knowledge or initiate retrieval based on their confidence in a given claim. We introduce Probabilistic Certainty and Consistency (PCC), a framework that estimates factual confidence by jointly modeling an LLM's probabilistic certainty and reasoning consistency. These confidence signals enable an adaptive verification strategy: the model answers directly when confident, triggers targeted retrieval when uncertain or inconsistent, and escalates to deep search when ambiguity is high. Our confidence-guided routing mechanism ensures that retrieval is invoked only when necessary, improving both efficiency and reliability. Extensive experiments across three challenging benchmarks show that PCC achieves better uncertainty quantification than verbalized confidence and consistently outperforms strong LLM-based fact-checking baselines. Furthermore, we demonstrate that PCC generalizes well across various LLMs.",
      "publishedDate": "2026-01-05T21:57:41Z",
      "updatedDate": "2026-01-05T21:57:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02574v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02574",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02535",
      "title": "ModeX: Evaluator-Free Best-of-N Selection for Open-Ended Generation",
      "authors": [
        {
          "name": "Hyeong Kyu Choi",
          "affiliation": null
        },
        {
          "name": "Sharon Li",
          "affiliation": null
        }
      ],
      "abstract": "Selecting a single high-quality output from multiple stochastic generations remains a fundamental challenge for large language models (LLMs), particularly in open-ended tasks where no canonical answer exists. While Best-of-N and self-consistency methods show that aggregating multiple generations can improve performance, existing approaches typically rely on external evaluators, reward models, or exact string-match voting, limiting their applicability and efficiency. We propose Mode Extraction (ModeX), an evaluator-free Best-of-N selection framework that generalizes majority voting to open-ended text generation by identifying the modal output representing the dominant semantic consensus among generated texts. ModeX constructs a similarity graph over candidate generations and recursively applies spectral clustering to select a representative centroid, without requiring additional inference or auxiliary models. We further instantiate this selection principle as ModeX--Lite, an improved version of ModeX with early pruning for efficiency. Across open-ended tasks--including text summarization, code generation, and mathematical reasoning--our approaches consistently outperform standard single- and multi-path baselines, providing a computationally efficient solution for robust open-ended text generation. Code is released in https://github.com/deeplearning-wisc/ModeX.",
      "publishedDate": "2026-01-05T20:16:32Z",
      "updatedDate": "2026-01-05T20:16:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02535v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02535",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02514",
      "title": "Textual Explanations and Their Evaluations for Reinforcement Learning Policy",
      "authors": [
        {
          "name": "Ahmad Terra",
          "affiliation": null
        },
        {
          "name": "Mohit Ahmed",
          "affiliation": null
        },
        {
          "name": "Rafia Inam",
          "affiliation": null
        },
        {
          "name": "Elena Fersman",
          "affiliation": null
        },
        {
          "name": "Martin Törngren",
          "affiliation": null
        }
      ],
      "abstract": "Understanding a Reinforcement Learning (RL) policy is crucial for ensuring that autonomous agents behave according to human expectations. This goal can be achieved using Explainable Reinforcement Learning (XRL) techniques. Although textual explanations are easily understood by humans, ensuring their correctness remains a challenge, and evaluations in state-of-the-art remain limited. We present a novel XRL framework for generating textual explanations, converting them into a set of transparent rules, improving their quality, and evaluating them. Expert's knowledge can be incorporated into this framework, and an automatic predicate generator is also proposed to determine the semantic information of a state. Textual explanations are generated using a Large Language Model (LLM) and a clustering technique to identify frequent conditions. These conditions are then converted into rules to evaluate their properties, fidelity, and performance in the deployed environment. Two refinement techniques are proposed to improve the quality of explanations and reduce conflicting information. Experiments were conducted in three open-source environments to enable reproducibility, and in a telecom use case to evaluate the industrial applicability of the proposed XRL framework. This framework addresses the limitations of an existing method, Autonomous Policy Explanation, and the generated transparent rules can achieve satisfactory performance on certain tasks. This framework also enables a systematic and quantitative evaluation of textual explanations, providing valuable insights for the XRL field.",
      "publishedDate": "2026-01-05T19:38:07Z",
      "updatedDate": "2026-01-05T19:38:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02514v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02514",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02512",
      "title": "Green LLM Techniques in Action: How Effective Are Existing Techniques for Improving the Energy Efficiency of LLM-Based Applications in Industry?",
      "authors": [
        {
          "name": "Pelin Rabia Kuran",
          "affiliation": null
        },
        {
          "name": "Rumbidzai Chitakunye",
          "affiliation": null
        },
        {
          "name": "Vincenzo Stoico",
          "affiliation": null
        },
        {
          "name": "Ilja Heitlager",
          "affiliation": null
        },
        {
          "name": "Justus Bogner",
          "affiliation": null
        }
      ],
      "abstract": "The rapid adoption of large language models (LLMs) has raised concerns about their substantial energy consumption, especially when deployed at industry scale. While several techniques have been proposed to address this, limited empirical evidence exists regarding the effectiveness of applying them to LLM-based industry applications. To fill this gap, we analyzed a chatbot application in an industrial context at Schuberg Philis, a Dutch IT services company. We then selected four techniques, namely Small and Large Model Collaboration, Prompt Optimization, Quantization, and Batching, applied them to the application in eight variations, and then conducted experiments to study their impact on energy consumption, accuracy, and response time compared to the unoptimized baseline. Our results show that several techniques, such as Prompt Optimization and 2-bit Quantization, managed to reduce energy use significantly, sometimes by up to 90%. However, these techniques especially impacted accuracy negatively, to a degree that is not acceptable in practice. The only technique that achieved significant and strong energy reductions without harming the other qualities substantially was Small and Large Model Collaboration via Nvidia's Prompt Task and Complexity Classifier (NPCC) with prompt complexity thresholds. This highlights that reducing the energy consumption of LLM-based applications is not difficult in practice. However, improving their energy efficiency, i.e., reducing energy use without harming other qualities, remains challenging. Our study provides practical insights to move towards this goal.",
      "publishedDate": "2026-01-05T19:35:29Z",
      "updatedDate": "2026-01-05T19:35:29Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02512v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02512",
      "comment": "Accepted for publication at the 2026 International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP'26)",
      "journalRef": null,
      "doi": "10.1145/3786583.3786896",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "tool-use",
        "multi-agent",
        "prompting"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "multi-agent",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02511",
      "title": "LLM-Enhanced Reinforcement Learning for Time Series Anomaly Detection",
      "authors": [
        {
          "name": "Bahareh Golchin",
          "affiliation": null
        },
        {
          "name": "Banafsheh Rekabdar",
          "affiliation": null
        },
        {
          "name": "Danielle Justo",
          "affiliation": null
        }
      ],
      "abstract": "Detecting anomalies in time series data is crucial for finance, healthcare, sensor networks, and industrial monitoring applications. However, time series anomaly detection often suffers from sparse labels, complex temporal patterns, and costly expert annotation. We propose a unified framework that integrates Large Language Model (LLM)-based potential functions for reward shaping with Reinforcement Learning (RL), Variational Autoencoder (VAE)-enhanced dynamic reward scaling, and active learning with label propagation. An LSTM-based RL agent leverages LLM-derived semantic rewards to guide exploration, while VAE reconstruction errors add unsupervised anomaly signals. Active learning selects the most uncertain samples, and label propagation efficiently expands labeled data. Evaluations on Yahoo-A1 and SMD benchmarks demonstrate that our method achieves state-of-the-art detection accuracy under limited labeling budgets and operates effectively in data-constrained settings. This study highlights the promise of combining LLMs with RL and advanced unsupervised techniques for robust, scalable anomaly detection in real-world applications.",
      "publishedDate": "2026-01-05T19:33:30Z",
      "updatedDate": "2026-01-05T19:33:30Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02511v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02511",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02443",
      "title": "Evaluating the Diagnostic Classification Ability of Multimodal Large Language Models: Insights from the Osteoarthritis Initiative",
      "authors": [
        {
          "name": "Li Wang",
          "affiliation": null
        },
        {
          "name": "Xi Chen",
          "affiliation": null
        },
        {
          "name": "XiangWen Deng",
          "affiliation": null
        },
        {
          "name": "HuaHui Yi",
          "affiliation": null
        },
        {
          "name": "ZeKun Jiang",
          "affiliation": null
        },
        {
          "name": "Kang Li",
          "affiliation": null
        },
        {
          "name": "Jian Li",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal large language models (MLLMs) show promising performance on medical visual question answering (VQA) and report generation, but these generation and explanation abilities do not reliably transfer to disease-specific classification. We evaluated MLLM architectures on knee osteoarthritis (OA) radiograph classification, which remains underrepresented in existing medical MLLM benchmarks, even though knee OA affects an estimated 300 to 400 million people worldwide. Through systematic ablation studies manipulating the vision encoder, the connector, and the large language model (LLM) across diverse training strategies, we measured each component's contribution to diagnostic accuracy. In our classification task, a trained vision encoder alone could outperform full MLLM pipelines in classification accuracy and fine-tuning the LLM provided no meaningful improvement over prompt-based guidance. And LoRA fine-tuning on a small, class-balanced dataset (500 images) gave better results than training on a much larger but class-imbalanced set (5,778 images), indicating that data balance and quality can matter more than raw scale for this task. These findings suggest that for domain-specific medical classification, LLMs are more effective as interpreters and report generators rather than as primary classifiers. Therefore, the MLLM architecture appears less suitable for medical image diagnostic classification tasks that demand high certainty. We recommend prioritizing vision encoder optimization and careful dataset curation when developing clinically applicable systems.",
      "publishedDate": "2026-01-05T13:31:44Z",
      "updatedDate": "2026-01-05T13:31:44Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "eess.IV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02443v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02443",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02430",
      "title": "WebCoderBench: Benchmarking Web Application Generation with Comprehensive and Interpretable Evaluation Metrics",
      "authors": [
        {
          "name": "Chenxu Liu",
          "affiliation": null
        },
        {
          "name": "Yingjie Fu",
          "affiliation": null
        },
        {
          "name": "Wei Yang",
          "affiliation": null
        },
        {
          "name": "Ying Zhang",
          "affiliation": null
        },
        {
          "name": "Tao Xie",
          "affiliation": null
        }
      ],
      "abstract": "Web applications (web apps) have become a key arena for large language models (LLMs) to demonstrate their code generation capabilities and commercial potential. However, building a benchmark for LLM-generated web apps remains challenging due to the need for real-world user requirements, generalizable evaluation metrics without relying on ground-truth implementations or test cases, and interpretable evaluation results. To address these challenges, we introduce WebCoderBench, the first real-world-collected, generalizable, and interpretable benchmark for web app generation. WebCoderBench comprises 1,572 real user requirements, covering diverse modalities and expression styles that reflect realistic user intentions. WebCoderBench provides 24 fine-grained evaluation metrics across 9 perspectives, combining rule-based and LLM-as-a-judge paradigm for fully automated, objective, and general evaluation. Moreover, WebCoderBench adopts human-preference-aligned weights over metrics to yield interpretable overall scores. Experiments across 12 representative LLMs and 2 LLM-based agents show that there exists no dominant model across all evaluation metrics, offering an opportunity for LLM developers to optimize their models in a targeted manner for a more powerful version.",
      "publishedDate": "2026-01-05T05:23:07Z",
      "updatedDate": "2026-01-05T05:23:07Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02430v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02430",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "code-generation",
        "agents"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03204",
      "title": "InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents",
      "authors": [
        {
          "name": "Chenglin Yu",
          "affiliation": null
        },
        {
          "name": "Yuchen Wang",
          "affiliation": null
        },
        {
          "name": "Songmiao Wang",
          "affiliation": null
        },
        {
          "name": "Hongxia Yang",
          "affiliation": null
        },
        {
          "name": "Ming Li",
          "affiliation": null
        }
      ],
      "abstract": "LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent's reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent",
      "publishedDate": "2026-01-06T17:35:57Z",
      "updatedDate": "2026-01-06T17:35:57Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03204v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03204",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "rag",
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03137",
      "title": "Accurate Table Question Answering with Accessible LLMs",
      "authors": [
        {
          "name": "Yangfan Jiang",
          "affiliation": null
        },
        {
          "name": "Fei Wei",
          "affiliation": null
        },
        {
          "name": "Ergute Bao",
          "affiliation": null
        },
        {
          "name": "Yaliang Li",
          "affiliation": null
        },
        {
          "name": "Bolin Ding",
          "affiliation": null
        },
        {
          "name": "Yin Yang",
          "affiliation": null
        },
        {
          "name": "Xiaokui Xiao",
          "affiliation": null
        }
      ],
      "abstract": "Given a table T in a database and a question Q in natural language, the table question answering (TQA) task aims to return an accurate answer to Q based on the content of T. Recent state-of-the-art solutions leverage large language models (LLMs) to obtain high-quality answers. However, most rely on proprietary, large-scale LLMs with costly API access, posing a significant financial barrier. This paper instead focuses on TQA with smaller, open-weight LLMs that can run on a desktop or laptop. This setting is challenging, as such LLMs typically have weaker capabilities than large proprietary models, leading to substantial performance degradation with existing methods. We observe that a key reason for this degradation is that prior approaches often require the LLM to solve a highly sophisticated task using long, complex prompts, which exceed the capabilities of small open-weight LLMs. Motivated by this observation, we present Orchestra, a multi-agent approach that unlocks the potential of accessible LLMs for high-quality, cost-effective TQA. Orchestra coordinates a group of LLM agents, each responsible for a relatively simple task, through a structured, layered workflow to solve complex TQA problems -- akin to an orchestra. By reducing the prompt complexity faced by each agent, Orchestra significantly improves output reliability. We implement Orchestra on top of AgentScope, an open-source multi-agent framework, and evaluate it on multiple TQA benchmarks using a wide range of open-weight LLMs. Experimental results show that Orchestra achieves strong performance even with small- to medium-sized models. For example, with Qwen2.5-14B, Orchestra reaches 72.1% accuracy on WikiTQ, approaching the best prior result of 75.3% achieved with GPT-4; with larger Qwen, Llama, or DeepSeek models, Orchestra outperforms all prior methods and establishes new state-of-the-art results across all benchmarks.",
      "publishedDate": "2026-01-06T16:07:25Z",
      "updatedDate": "2026-01-06T16:07:25Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03137v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03137",
      "comment": "accepted for publication in the Proceedings of the IEEE International Conference on Data Engineering (ICDE) 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02744",
      "title": "SYNAPSE: Empowering LLM Agents with Episodic-Semantic Memory via Spreading Activation",
      "authors": [
        {
          "name": "Hanqi Jiang",
          "affiliation": null
        },
        {
          "name": "Junhao Chen",
          "affiliation": null
        },
        {
          "name": "Yi Pan",
          "affiliation": null
        },
        {
          "name": "Ling Chen",
          "affiliation": null
        },
        {
          "name": "Weihang You",
          "affiliation": null
        },
        {
          "name": "Yifan Zhou",
          "affiliation": null
        },
        {
          "name": "Ruidong Zhang",
          "affiliation": null
        },
        {
          "name": "Yohannes Abate",
          "affiliation": null
        },
        {
          "name": "Tianming Liu",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) excel at generalized reasoning, standard retrieval-augmented approaches fail to address the disconnected nature of long-term agentic memory. To bridge this gap, we introduce Synapse (Synergistic Associative Processing Semantic Encoding), a unified memory architecture that transcends static vector similarity. Drawing from cognitive science, Synapse models memory as a dynamic graph where relevance emerges from spreading activation rather than pre-computed links. By integrating lateral inhibition and temporal decay, the system dynamically highlights relevant sub-graphs while filtering interference. We implement a Triple Hybrid Retrieval strategy that fuses geometric embeddings with activation-based graph traversal. Comprehensive evaluations on the LoCoMo benchmark show that Synapse significantly outperforms state-of-the-art methods in complex temporal and multi-hop reasoning tasks, offering a robust solution to the \"Contextual Tunneling\" problem. Our code and data will be made publicly available upon acceptance.",
      "publishedDate": "2026-01-06T06:19:58Z",
      "updatedDate": "2026-01-06T06:19:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02744v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02744",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02695",
      "title": "EvoRoute: Experience-Driven Self-Routing LLM Agent Systems",
      "authors": [
        {
          "name": "Guibin Zhang",
          "affiliation": null
        },
        {
          "name": "Haiyang Yu",
          "affiliation": null
        },
        {
          "name": "Kaiming Yang",
          "affiliation": null
        },
        {
          "name": "Bingli Wu",
          "affiliation": null
        },
        {
          "name": "Fei Huang",
          "affiliation": null
        },
        {
          "name": "Yongbin Li",
          "affiliation": null
        },
        {
          "name": "Shuicheng Yan",
          "affiliation": null
        }
      ],
      "abstract": "Complex agentic AI systems, powered by a coordinated ensemble of Large Language Models (LLMs), tool and memory modules, have demonstrated remarkable capabilities on intricate, multi-turn tasks. However, this success is shadowed by prohibitive economic costs and severe latency, exposing a critical, yet underexplored, trade-off. We formalize this challenge as the \\textbf{Agent System Trilemma}: the inherent tension among achieving state-of-the-art performance, minimizing monetary cost, and ensuring rapid task completion. To dismantle this trilemma, we introduce EvoRoute, a self-evolving model routing paradigm that transcends static, pre-defined model assignments. Leveraging an ever-expanding knowledge base of prior experience, EvoRoute dynamically selects Pareto-optimal LLM backbones at each step, balancing accuracy, efficiency, and resource use, while continually refining its own selection policy through environment feedback. Experiments on challenging agentic benchmarks such as GAIA and BrowseComp+ demonstrate that EvoRoute, when integrated into off-the-shelf agentic systems, not only sustains or enhances system performance but also reduces execution cost by up to $80\\%$ and latency by over $70\\%$.",
      "publishedDate": "2026-01-06T04:06:46Z",
      "updatedDate": "2026-01-06T04:06:46Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02695v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02695",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02598",
      "title": "LongDA: Benchmarking LLM Agents for Long-Document Data Analysis",
      "authors": [
        {
          "name": "Yiyang Li",
          "affiliation": null
        },
        {
          "name": "Zheyuan Zhang",
          "affiliation": null
        },
        {
          "name": "Tianyi Ma",
          "affiliation": null
        },
        {
          "name": "Zehong Wang",
          "affiliation": null
        },
        {
          "name": "Keerthiram Murugesan",
          "affiliation": null
        },
        {
          "name": "Chuxu Zhang",
          "affiliation": null
        },
        {
          "name": "Yanfang Ye",
          "affiliation": null
        }
      ],
      "abstract": "We introduce LongDA, a data analysis benchmark for evaluating LLM-based agents under documentation-intensive analytical workflows. In contrast to existing benchmarks that assume well-specified schemas and inputs, LongDA targets real-world settings in which navigating long documentation and complex data is the primary bottleneck. To this end, we manually curate raw data files, long and heterogeneous documentation, and expert-written publications from 17 publicly available U.S. national surveys, from which we extract 505 analytical queries grounded in real analytical practice. Solving these queries requires agents to first retrieve and integrate key information from multiple unstructured documents, before performing multi-step computations and writing executable code, which remains challenging for existing data analysis agents. To support the systematic evaluation under this setting, we develop LongTA, a tool-augmented agent framework that enables document access, retrieval, and code execution, and evaluate a range of proprietary and open-source models. Our experiments reveal substantial performance gaps even among state-of-the-art models, highlighting the challenges researchers should consider before applying LLM agents for decision support in real-world, high-stakes analytical settings.",
      "publishedDate": "2026-01-05T23:23:16Z",
      "updatedDate": "2026-01-05T23:23:16Z",
      "primaryCategory": "cs.DL",
      "arxivCategories": [
        "cs.DL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02598v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02598",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02577",
      "title": "Orchestral AI: A Framework for Agent Orchestration",
      "authors": [
        {
          "name": "Alexander Roman",
          "affiliation": null
        },
        {
          "name": "Jacob Roman",
          "affiliation": null
        }
      ],
      "abstract": "The rapid proliferation of LLM agent frameworks has forced developers to choose between vendor lock-in through provider-specific SDKs and complex multi-package ecosystems that obscure control flow and hinder reproducibility. Integrating tool calling across multiple LLM providers remains a core engineering challenge due to fragmented APIs, incompatible message formats, and inconsistent streaming and tool-calling behavior, making it difficult to build portable, reliable agent systems. We introduce Orchestral, a lightweight Python framework that provides a unified, type-safe interface for building LLM agents across major providers while preserving the simplicity required for scientific computing and production deployment. Orchestral defines a single universal representation for messages, tools, and LLM usage that operates seamlessly across providers, eliminating manual format translation and reducing framework-induced complexity. Automatic tool schema generation from Python type hints removes the need for handwritten descriptors while maintaining type safety across provider boundaries. A synchronous execution model with streaming support enables deterministic behavior, straightforward debugging, and real-time interaction without introducing server dependencies. The framework's modular architecture cleanly separates provider integration, tool execution, conversation orchestration, and user-facing interfaces, enabling extensibility without architectural entanglement. Orchestral supports advanced agent capabilities found in larger frameworks, including rich tool calling, context compaction, workspace sandboxing, user approval workflows, sub-agents, memory management, and MCP integration.",
      "publishedDate": "2026-01-05T22:02:11Z",
      "updatedDate": "2026-01-05T22:02:11Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "astro-ph.IM",
        "hep-ph"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02577v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02577",
      "comment": "17 pages, 3 figures. For more information visit https://orchestral-ai.com",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "tool-use",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02553",
      "title": "SimpleMem: Efficient Lifelong Memory for LLM Agents",
      "authors": [
        {
          "name": "Jiaqi Liu",
          "affiliation": null
        },
        {
          "name": "Yaofeng Su",
          "affiliation": null
        },
        {
          "name": "Peng Xia",
          "affiliation": null
        },
        {
          "name": "Siwei Han",
          "affiliation": null
        },
        {
          "name": "Zeyu Zheng",
          "affiliation": null
        },
        {
          "name": "Cihang Xie",
          "affiliation": null
        },
        {
          "name": "Mingyu Ding",
          "affiliation": null
        },
        {
          "name": "Huaxiu Yao",
          "affiliation": null
        }
      ],
      "abstract": "To support reliable long-term interaction in complex environments, LLM agents require memory systems that efficiently manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) \\textit{Semantic Structured Compression}, which applies entropy-aware filtering to distill unstructured interactions into compact, multi-view indexed memory units; (2) \\textit{Recursive Memory Consolidation}, an asynchronous process that integrates related units into higher-level abstract representations to reduce redundancy; and (3) \\textit{Adaptive Query-Aware Retrieval}, which dynamically adjusts retrieval scope based on query complexity to construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.",
      "publishedDate": "2026-01-05T21:02:49Z",
      "updatedDate": "2026-01-05T21:02:49Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02553v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02553",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03236",
      "title": "MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents",
      "authors": [
        {
          "name": "Dongming Jiang",
          "affiliation": null
        },
        {
          "name": "Yi Li",
          "affiliation": null
        },
        {
          "name": "Guanpeng Li",
          "affiliation": null
        },
        {
          "name": "Bingzhe Li",
          "affiliation": null
        }
      ],
      "abstract": "Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.",
      "publishedDate": "2026-01-06T18:29:43Z",
      "updatedDate": "2026-01-06T18:29:43Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03236v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03236",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03227",
      "title": "The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization",
      "authors": [
        {
          "name": "Ruixing Zhang",
          "affiliation": null
        },
        {
          "name": "Zihan Liu",
          "affiliation": null
        },
        {
          "name": "Leilei Sun",
          "affiliation": null
        },
        {
          "name": "Tongyu Zhu",
          "affiliation": null
        },
        {
          "name": "Weifeng Lv",
          "affiliation": null
        }
      ],
      "abstract": "Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs' reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.",
      "publishedDate": "2026-01-06T18:13:24Z",
      "updatedDate": "2026-01-06T18:13:24Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03227v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03227",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03217",
      "title": "MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics",
      "authors": [
        {
          "name": "Xinghe Chen",
          "affiliation": null
        },
        {
          "name": "Naiming Liu",
          "affiliation": null
        },
        {
          "name": "Shashank Sonkar",
          "affiliation": null
        }
      ],
      "abstract": "Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student's next answer under cross-template rephrasing. Across nine language models (4B-120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10-21%, while providing student step traces improves prediction by 3-15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.",
      "publishedDate": "2026-01-06T17:59:37Z",
      "updatedDate": "2026-01-06T17:59:37Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03217v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03217",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03205",
      "title": "UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward",
      "authors": [
        {
          "name": "Yile Liu",
          "affiliation": null
        },
        {
          "name": "Yixian Liu",
          "affiliation": null
        },
        {
          "name": "Zongwei Li",
          "affiliation": null
        },
        {
          "name": "Yufei Huang",
          "affiliation": null
        },
        {
          "name": "Xinhua Feng",
          "affiliation": null
        },
        {
          "name": "Zhichao Hu",
          "affiliation": null
        },
        {
          "name": "Jinglu Hu",
          "affiliation": null
        },
        {
          "name": "Jianfeng Yan",
          "affiliation": null
        },
        {
          "name": "Fengzong Lian",
          "affiliation": null
        },
        {
          "name": "Yuhong Liu",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.",
      "publishedDate": "2026-01-06T17:41:32Z",
      "updatedDate": "2026-01-06T17:41:32Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03205v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03205",
      "comment": "19 pages, 6 figures, 7 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "planning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03194",
      "title": "X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework",
      "authors": [
        {
          "name": "Mohammad Zia Ur Rehman",
          "affiliation": null
        },
        {
          "name": "Sai Kartheek Reddy Kasu",
          "affiliation": null
        },
        {
          "name": "Shashivardhan Reddy Koppula",
          "affiliation": null
        },
        {
          "name": "Sai Rithwik Reddy Chirra",
          "affiliation": null
        },
        {
          "name": "Shwetank Shekhar Singh",
          "affiliation": null
        },
        {
          "name": "Nagendra Kumar",
          "affiliation": null
        }
      ],
      "abstract": "Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST",
      "publishedDate": "2026-01-06T17:16:45Z",
      "updatedDate": "2026-01-06T17:16:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03194v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03194",
      "comment": "Accepted in the proceedings of AAAI 2026",
      "journalRef": "AAA 2026 (AISI)",
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03192",
      "title": "MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory",
      "authors": [
        {
          "name": "Shengtao Zhang",
          "affiliation": null
        },
        {
          "name": "Jiaqian Wang",
          "affiliation": null
        },
        {
          "name": "Ruiwen Zhou",
          "affiliation": null
        },
        {
          "name": "Junwei Liao",
          "affiliation": null
        },
        {
          "name": "Yuchen Feng",
          "affiliation": null
        },
        {
          "name": "Weinan Zhang",
          "affiliation": null
        },
        {
          "name": "Ying Wen",
          "affiliation": null
        },
        {
          "name": "Zhiyu Li",
          "affiliation": null
        },
        {
          "name": "Feiyu Xiong",
          "affiliation": null
        },
        {
          "name": "Yutao Qi",
          "affiliation": null
        },
        {
          "name": "Bo Tang",
          "affiliation": null
        },
        {
          "name": "Muning Wen",
          "affiliation": null
        }
      ],
      "abstract": "The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.",
      "publishedDate": "2026-01-06T17:14:50Z",
      "updatedDate": "2026-01-06T17:14:50Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03192v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03192",
      "comment": "23 pages, 11 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03191",
      "title": "AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation",
      "authors": [
        {
          "name": "Anees Ur Rehman Hashmi",
          "affiliation": null
        },
        {
          "name": "Numan Saeed",
          "affiliation": null
        },
        {
          "name": "Christoph Lippert",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal medical large language models have shown impressive progress in chest X-ray interpretation but continue to face challenges in spatial reasoning and anatomical understanding. Although existing grounding techniques improve overall performance, they often fail to establish a true anatomical correspondence, resulting in incorrect anatomical understanding in the medical domain. To address this gap, we introduce AnatomiX, a multitask multimodal large language model explicitly designed for anatomically grounded chest X-ray interpretation. Inspired by the radiological workflow, AnatomiX adopts a two stage approach: first, it identifies anatomical structures and extracts their features, and then leverages a large language model to perform diverse downstream tasks such as phrase grounding, report generation, visual question answering, and image understanding. Extensive experiments across multiple benchmarks demonstrate that AnatomiX achieves superior anatomical reasoning and delivers over 25% improvement in performance on anatomy grounding, phrase grounding, grounded diagnosis and grounded captioning tasks compared to existing approaches. Code and pretrained model are available at https://github.com/aneesurhashmi/anatomix",
      "publishedDate": "2026-01-06T17:13:23Z",
      "updatedDate": "2026-01-06T17:13:23Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03191v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03191",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03164",
      "title": "WebAnchor: Anchoring Agent Planning to Stabilize Long-Horizon Web Reasoning",
      "authors": [
        {
          "name": "Xinmiao Yu",
          "affiliation": null
        },
        {
          "name": "Liwen Zhang",
          "affiliation": null
        },
        {
          "name": "Xiaocheng Feng",
          "affiliation": null
        },
        {
          "name": "Yong Jiang",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        },
        {
          "name": "Pengjun Xie",
          "affiliation": null
        },
        {
          "name": "Jingren Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model(LLM)-based agents have shown strong capabilities in web information seeking, with reinforcement learning (RL) becoming a key optimization paradigm. However, planning remains a bottleneck, as existing methods struggle with long-horizon strategies. Our analysis reveals a critical phenomenon, plan anchor, where the first reasoning step disproportionately impacts downstream behavior in long-horizon web reasoning tasks. Current RL algorithms, fail to account for this by uniformly distributing rewards across the trajectory. To address this, we propose Anchor-GRPO, a two-stage RL framework that decouples planning and execution. In Stage 1, the agent optimizes its first-step planning using fine-grained rubrics derived from self-play experiences and human calibration. In Stage 2, execution is aligned with the initial plan through sparse rewards, ensuring stable and efficient tool usage. We evaluate Anchor-GRPO on four benchmarks: BrowseComp, BrowseComp-Zh, GAIA, and XBench-DeepSearch. Across models from 3B to 30B, Anchor-GRPO outperforms baseline GRPO and First-step GRPO, improving task success and tool efficiency. Notably, WebAnchor-30B achieves 46.0% pass@1 on BrowseComp and 76.4% on GAIA. Anchor-GRPO also demonstrates strong scalability, getting higher accuracy as model size and context length increase.",
      "publishedDate": "2026-01-06T16:36:40Z",
      "updatedDate": "2026-01-07T02:00:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03164v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03164",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03144",
      "title": "Self-Verification is All You Need To Pass The Japanese Bar Examination",
      "authors": [
        {
          "name": "Andrew Shin",
          "affiliation": null
        }
      ],
      "abstract": "Despite rapid advances in large language models (LLMs), achieving reliable performance on highly professional and structured examinations remains a significant challenge. The Japanese bar examination is a particularly demanding benchmark, requiring not only advanced legal reasoning but also strict adherence to complex answer formats that involve joint evaluation of multiple propositions. While recent studies have reported improvements by decomposing such questions into simpler true--false judgments, these approaches have not been systematically evaluated under the original exam format and scoring scheme, leaving open the question of whether they truly capture exam-level competence. In this paper, we present a self-verification model trained on a newly constructed dataset that faithfully replicates the authentic format and evaluation scale of the exam. Our model is able to exceed the official passing score when evaluated on the actual exam scale, marking the first demonstration, to our knowledge, of an LLM passing the Japanese bar examination without altering its original question structure or scoring rules. We further conduct extensive comparisons with alternative strategies, including multi-agent inference and decomposition-based supervision, and find that these methods fail to achieve comparable performance. Our results highlight the importance of format-faithful supervision and consistency verification, and suggest that carefully designed single-model approaches can outperform more complex systems in high-stakes professional reasoning tasks. Our dataset and codes are publicly available.",
      "publishedDate": "2026-01-06T16:13:47Z",
      "updatedDate": "2026-01-06T16:13:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03144v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03144",
      "comment": "https://github.com/shinandrew/self_verification",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "reasoning",
        "multi-agent",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "reasoning",
          "multi-agent",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03111",
      "title": "One Sample to Rule Them All: Extreme Data Efficiency in RL Scaling",
      "authors": [
        {
          "name": "Yiyuan Li",
          "affiliation": null
        },
        {
          "name": "Zhen Huang",
          "affiliation": null
        },
        {
          "name": "Yanan Wu",
          "affiliation": null
        },
        {
          "name": "Weixun Wang",
          "affiliation": null
        },
        {
          "name": "Xuefeng Li",
          "affiliation": null
        },
        {
          "name": "Yijia Luo",
          "affiliation": null
        },
        {
          "name": "Wenbo Su",
          "affiliation": null
        },
        {
          "name": "Bo Zheng",
          "affiliation": null
        },
        {
          "name": "Pengfei Liu",
          "affiliation": null
        }
      ],
      "abstract": "The reasoning ability of large language models (LLMs) can be unleashed with reinforcement learning (RL) (OpenAI, 2024; DeepSeek-AI et al., 2025a; Zeng et al., 2025). The success of existing RL attempts in LLMs usually relies on high-quality samples of thousands or beyond. In this paper, we challenge fundamental assumptions about data requirements in RL for LLMs by demonstrating the remarkable effectiveness of one-shot learning. Specifically, we introduce polymath learning, a framework for designing one training sample that elicits multidisciplinary impact. We present three key findings: (1) A single, strategically selected math reasoning sample can produce significant performance improvements across multiple domains, including physics, chemistry, and biology with RL; (2) The math skills salient to reasoning suggest the characteristics of the optimal polymath sample; and (3) An engineered synthetic sample that integrates multidiscipline elements outperforms training with individual samples that naturally occur. Our approach achieves superior performance to training with larger datasets across various reasoning benchmarks, demonstrating that sample quality and design, rather than quantity, may be the key to unlock enhanced reasoning capabilities in language models. Our results suggest a shift, dubbed as sample engineering, toward precision engineering of training samples rather than simply increasing data volume.",
      "publishedDate": "2026-01-06T15:41:35Z",
      "updatedDate": "2026-01-06T15:41:35Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03111v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03111",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03093",
      "title": "ATLAS: Adaptive Test-Time Latent Steering with External Verifiers for Enhancing LLMs Reasoning",
      "authors": [
        {
          "name": "Tuc Nguyen",
          "affiliation": null
        },
        {
          "name": "Thai Le",
          "affiliation": null
        }
      ],
      "abstract": "Recent work on activation and latent steering has demonstrated that modifying internal representations can effectively guide large language models (LLMs) toward improved reasoning and efficiency without additional training. However, most existing approaches rely on fixed steering policies and static intervention strengths, which limit their robustness across problem instances and often result in over- or under-steering. We propose Adaptive Test-time Latent Steering, called (ATLAS), a task- specific framework that dynamically controls steering decisions at inference time using an external, lightweight latent verifier. Given intermediate hidden states, the verifier predicts the quality of ongoing reasoning and adaptively selects whether and how strongly to apply steering, enabling per-example and per-step adjustment with minimal overhead. To our knowledge, ATLAS is the first method to integrate learned latent verification into test-time steering for enhancing LLMs reasoning. Experiments on multiple mathematical reasoning benchmarks show that ATLAS consistently outperforms both vanilla decoding and fixed steering baselines, achieving higher accuracy while substantially reducing test-time token usage. These results demonstrate that verifier-guided latent adaptation provides an effective and scalable mechanism for controlling reasoning efficiency without sacrificing solution quality. All source code will be publicly available.",
      "publishedDate": "2026-01-06T15:27:24Z",
      "updatedDate": "2026-01-06T15:27:24Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03093v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03093",
      "comment": "12 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03073",
      "title": "Understanding Multi-Agent Reasoning with Large Language Models for Cartoon VQA",
      "authors": [
        {
          "name": "Tong Wu",
          "affiliation": null
        },
        {
          "name": "Thanet Markchom",
          "affiliation": null
        }
      ],
      "abstract": "Visual Question Answering (VQA) for stylised cartoon imagery presents challenges, such as interpreting exaggerated visual abstraction and narrative-driven context, which are not adequately addressed by standard large language models (LLMs) trained on natural images. To investigate this issue, a multi-agent LLM framework is introduced, specifically designed for VQA tasks in cartoon imagery. The proposed architecture consists of three specialised agents: visual agent, language agent and critic agent, which work collaboratively to support structured reasoning by integrating visual cues and narrative context. The framework was systematically evaluated on two cartoon-based VQA datasets: Pororo and Simpsons. Experimental results provide a detailed analysis of how each agent contributes to the final prediction, offering a deeper understanding of LLM-based multi-agent behaviour in cartoon VQA and multimodal inference.",
      "publishedDate": "2026-01-06T14:58:33Z",
      "updatedDate": "2026-01-06T14:58:33Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03073v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03073",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03066",
      "title": "Do LLMs Encode Functional Importance of Reasoning Tokens?",
      "authors": [
        {
          "name": "Janvijay Singh",
          "affiliation": null
        },
        {
          "name": "Dilek Hakkani-Tür",
          "affiliation": null
        }
      ],
      "abstract": "Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.",
      "publishedDate": "2026-01-06T14:50:02Z",
      "updatedDate": "2026-01-06T14:50:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03066v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03066",
      "comment": "20 pages, 8 figures, 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03052",
      "title": "Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph",
      "authors": [
        {
          "name": "Jianpeng Hu",
          "affiliation": null
        },
        {
          "name": "Yanzeng Li",
          "affiliation": null
        },
        {
          "name": "Jialun Zhong",
          "affiliation": null
        },
        {
          "name": "Wenfa Qi",
          "affiliation": null
        },
        {
          "name": "Lei Zou",
          "affiliation": null
        }
      ],
      "abstract": "The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.",
      "publishedDate": "2026-01-06T14:35:20Z",
      "updatedDate": "2026-01-06T14:35:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03052v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03052",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03043",
      "title": "Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage",
      "authors": [
        {
          "name": "Junhao Hu",
          "affiliation": null
        },
        {
          "name": "Fangze Li",
          "affiliation": null
        },
        {
          "name": "Mingtao Xu",
          "affiliation": null
        },
        {
          "name": "Feifan Meng",
          "affiliation": null
        },
        {
          "name": "Shiju Zhao",
          "affiliation": null
        },
        {
          "name": "Tiancheng Hu",
          "affiliation": null
        },
        {
          "name": "Ting Peng",
          "affiliation": null
        },
        {
          "name": "Anmin Liu",
          "affiliation": null
        },
        {
          "name": "Wenrui Huang",
          "affiliation": null
        },
        {
          "name": "Chenxu Liu",
          "affiliation": null
        },
        {
          "name": "Ziyue Hua",
          "affiliation": null
        },
        {
          "name": "Tao Xie",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.",
      "publishedDate": "2026-01-06T14:23:58Z",
      "updatedDate": "2026-01-06T14:23:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03043v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03043",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03023",
      "title": "MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models",
      "authors": [
        {
          "name": "Lecheng Gong",
          "affiliation": null
        },
        {
          "name": "Weimin Fang",
          "affiliation": null
        },
        {
          "name": "Ting Yang",
          "affiliation": null
        },
        {
          "name": "Dongjie Tao",
          "affiliation": null
        },
        {
          "name": "Chunxiao Guo",
          "affiliation": null
        },
        {
          "name": "Peng Wei",
          "affiliation": null
        },
        {
          "name": "Bo Xie",
          "affiliation": null
        },
        {
          "name": "Jinqun Guan",
          "affiliation": null
        },
        {
          "name": "Zixiao Chen",
          "affiliation": null
        },
        {
          "name": "Fang Shi",
          "affiliation": null
        },
        {
          "name": "Jinjie Gu",
          "affiliation": null
        },
        {
          "name": "Junwei Liu",
          "affiliation": null
        }
      ],
      "abstract": "Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items (\"must-ask\" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.",
      "publishedDate": "2026-01-06T13:56:33Z",
      "updatedDate": "2026-01-07T02:10:21Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03023v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03023",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03018",
      "title": "Dementia-R1: Reinforced Pretraining and Reasoning from Unstructured Clinical Notes for Real-World Dementia Prognosis",
      "authors": [
        {
          "name": "Choonghan Kim",
          "affiliation": null
        },
        {
          "name": "Hyunmin Hwang",
          "affiliation": null
        },
        {
          "name": "Hangeol Chang",
          "affiliation": null
        },
        {
          "name": "Jaemin Kim",
          "affiliation": null
        },
        {
          "name": "Jinse Park",
          "affiliation": null
        },
        {
          "name": "Jae-Sung Lim",
          "affiliation": null
        },
        {
          "name": "Jong Chul Ye",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) have shown strong performance on clinical text understanding, they struggle with longitudinal prediction tasks such as dementia prognosis, which require reasoning over complex, non-monotonic symptom trajectories across multiple visits. Standard supervised training lacks explicit annotations for symptom evolution, while direct Reinforcement Learning (RL) is hindered by sparse binary rewards. To address this challenge, we introduce Dementia-R1, an RL-based framework for longitudinal dementia prognosis from unstructured clinical notes. Our approach adopts a Cold-Start RL strategy that pre-trains the model to predict verifiable clinical indices extracted from patient histories, enhancing the capability to reason about disease progression before determining the final clinical status. Extensive experiments demonstrate that Dementia-R1 achieves an F1 score of 77.03% on real-world unstructured clinical datasets. Notably, on the ADNI benchmark, our 7B model rivals GPT-4o, effectively capturing fluctuating cognitive trajectories. Code is available at https://anonymous.4open.science/r/dementiar1-CDB5",
      "publishedDate": "2026-01-06T13:44:04Z",
      "updatedDate": "2026-01-06T13:44:04Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03018v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03018",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03014",
      "title": "SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering",
      "authors": [
        {
          "name": "Junli Liang",
          "affiliation": null
        },
        {
          "name": "Pengfei Zhou",
          "affiliation": null
        },
        {
          "name": "Wangqiu Zhou",
          "affiliation": null
        },
        {
          "name": "Wenjie Qing",
          "affiliation": null
        },
        {
          "name": "Qi Zhao",
          "affiliation": null
        },
        {
          "name": "Ziwen Wang",
          "affiliation": null
        },
        {
          "name": "Qi Song",
          "affiliation": null
        },
        {
          "name": "Xiangyang Li",
          "affiliation": null
        }
      ],
      "abstract": "Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.",
      "publishedDate": "2026-01-06T13:39:51Z",
      "updatedDate": "2026-01-06T13:39:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03014v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03014",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03007",
      "title": "From inconsistency to decision: explainable operation and maintenance of battery energy storage systems",
      "authors": [
        {
          "name": "Jingbo Qu",
          "affiliation": null
        },
        {
          "name": "Yijie Wang",
          "affiliation": null
        },
        {
          "name": "Yujie Fu",
          "affiliation": null
        },
        {
          "name": "Putai Zhang",
          "affiliation": null
        },
        {
          "name": "Weihan Li",
          "affiliation": null
        },
        {
          "name": "Mian Li",
          "affiliation": null
        }
      ],
      "abstract": "Battery Energy Storage Systems (BESSs) are increasingly critical to power-system stability, yet their operation and maintenance remain dominated by reactive, expert-dependent diagnostics. While cell-level inconsistencies provide early warning signals of degradation and safety risks, the lack of scalable and interpretable decision-support frameworks prevents these signals from being effectively translated into operational actions. Here we introduce an inconsistency-driven operation and maintenance paradigm for large-scale BESSs that systematically transforms routine monitoring data into explainable, decision-oriented guidance. The proposed framework integrates multi-dimensional inconsistency evaluation with large language model-based semantic reasoning to bridge the gap between quantitative diagnostics and practical maintenance decisions. Using eight months of field data from an in-service battery system comprising 3,564 cells, we demonstrate how electrical, thermal, and aging-related inconsistencies can be distilled into structured operational records and converted into actionable maintenance insights through a multi-agent framework. The proposed approach enables accurate and explainable responses to real-world operation and maintenance queries, reducing response time and operational cost by over 80% compared with conventional expert-driven practices. These results establish a scalable pathway for intelligent operation and maintenance of battery energy storage systems, with direct implications for reliability, safety, and cost-effective integration of energy storage into modern power systems.",
      "publishedDate": "2026-01-06T13:32:04Z",
      "updatedDate": "2026-01-07T02:29:31Z",
      "primaryCategory": "eess.SY",
      "arxivCategories": [
        "eess.SY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03007v2",
      "arxivUrl": "https://arxiv.org/abs/2601.03007",
      "comment": "13 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02993",
      "title": "Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation",
      "authors": [
        {
          "name": "Qianchi Zhang",
          "affiliation": null
        },
        {
          "name": "Hainan Zhang",
          "affiliation": null
        },
        {
          "name": "Liang Pang",
          "affiliation": null
        },
        {
          "name": "Hongwei Zheng",
          "affiliation": null
        },
        {
          "name": "Zhiming Zheng",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.",
      "publishedDate": "2026-01-06T13:07:38Z",
      "updatedDate": "2026-01-07T11:54:06Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02993v2",
      "arxivUrl": "https://arxiv.org/abs/2601.02993",
      "comment": "18 pages, 13figures, 8 tables. The code will be released after the review process",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02989",
      "title": "Mechanistic Interpretability of Large-Scale Counting in LLMs through a System-2 Strategy",
      "authors": [
        {
          "name": "Hosein Hasani",
          "affiliation": null
        },
        {
          "name": "Mohammadali Banayeeanzade",
          "affiliation": null
        },
        {
          "name": "Ali Nafisi",
          "affiliation": null
        },
        {
          "name": "Sadegh Mohammadian",
          "affiliation": null
        },
        {
          "name": "Fatemeh Askari",
          "affiliation": null
        },
        {
          "name": "Mobin Bagherian",
          "affiliation": null
        },
        {
          "name": "Amirmohammad Izadi",
          "affiliation": null
        },
        {
          "name": "Mahdieh Soleymani Baghshah",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs), despite strong performance on complex mathematical problems, exhibit systematic limitations in counting tasks. This issue arises from architectural limits of transformers, where counting is performed across layers, leading to degraded precision for larger counting problems due to depth constraints. To address this limitation, we propose a simple test-time strategy inspired by System-2 cognitive processes that decomposes large counting tasks into smaller, independent sub-problems that the model can reliably solve. We evaluate this approach using observational and causal mediation analyses to understand the underlying mechanism of this System-2-like strategy. Our mechanistic analysis identifies key components: latent counts are computed and stored in the final item representations of each part, transferred to intermediate steps via dedicated attention heads, and aggregated in the final stage to produce the total count. Experimental results demonstrate that this strategy enables LLMs to surpass architectural limitations and achieve high accuracy on large-scale counting tasks. This work provides mechanistic insight into System-2 counting in LLMs and presents a generalizable approach for improving and understanding their reasoning behavior.",
      "publishedDate": "2026-01-06T12:58:27Z",
      "updatedDate": "2026-01-06T12:58:27Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02989v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02989",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02983",
      "title": "Interpretable All-Type Audio Deepfake Detection with Audio LLMs via Frequency-Time Reinforcement Learning",
      "authors": [
        {
          "name": "Yuankun Xie",
          "affiliation": null
        },
        {
          "name": "Xiaoxuan Guo",
          "affiliation": null
        },
        {
          "name": "Jiayi Zhou",
          "affiliation": null
        },
        {
          "name": "Tao Wang",
          "affiliation": null
        },
        {
          "name": "Jian Liu",
          "affiliation": null
        },
        {
          "name": "Ruibo Fu",
          "affiliation": null
        },
        {
          "name": "Xiaopeng Wang",
          "affiliation": null
        },
        {
          "name": "Haonan Cheng",
          "affiliation": null
        },
        {
          "name": "Long Ye",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in audio large language models (ALLMs) have made high-quality synthetic audio widely accessible, increasing the risk of malicious audio deepfakes across speech, environmental sounds, singing voice, and music. Real-world audio deepfake detection (ADD) therefore requires all-type detectors that generalize across heterogeneous audio and provide interpretable decisions. Given the strong multi-task generalization ability of ALLMs, we first investigate their performance on all-type ADD under both supervised fine-tuning (SFT) and reinforcement fine-tuning (RFT). However, SFT using only binary real/fake labels tends to reduce the model to a black-box classifier, sacrificing interpretability. Meanwhile, vanilla RFT under sparse supervision is prone to reward hacking and can produce hallucinated, ungrounded rationales. To address this, we propose an automatic annotation and polishing pipeline that constructs Frequency-Time structured chain-of-thought (CoT) rationales, producing ~340K cold-start demonstrations. Building on CoT data, we propose Frequency Time-Group Relative Policy Optimization (FT-GRPO), a two-stage training paradigm that cold-starts ALLMs with SFT and then applies GRPO under rule-based frequency-time constraints. Experiments demonstrate that FT-GRPO achieves state-of-the-art performance on all-type ADD while producing interpretable, FT-grounded rationales. The data and code are available online.",
      "publishedDate": "2026-01-06T12:50:02Z",
      "updatedDate": "2026-01-06T12:50:02Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02983v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02983",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02972",
      "title": "Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning",
      "authors": [
        {
          "name": "Nathanaël Carraz Rakotonirina",
          "affiliation": null
        },
        {
          "name": "Ren Pang",
          "affiliation": null
        },
        {
          "name": "Neha Anna John",
          "affiliation": null
        },
        {
          "name": "Michael Bohlke-Schneider",
          "affiliation": null
        },
        {
          "name": "Momchil Hardalov",
          "affiliation": null
        }
      ],
      "abstract": "The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy--response length trade-off. Our approach reduces response length by an average of 28\\% for 8B models and 40\\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\\text{AUC}_{\\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.",
      "publishedDate": "2026-01-06T12:31:51Z",
      "updatedDate": "2026-01-06T12:31:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02972v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02972",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02968",
      "title": "Rationale-Grounded In-Context Learning for Time Series Reasoning with Multimodal Large Language Models",
      "authors": [
        {
          "name": "Qingxiang Liu",
          "affiliation": null
        },
        {
          "name": "Zhiqing Cui",
          "affiliation": null
        },
        {
          "name": "Xiaoliang Luo",
          "affiliation": null
        },
        {
          "name": "Yuqian Wu",
          "affiliation": null
        },
        {
          "name": "Zhuoyang Jiang",
          "affiliation": null
        },
        {
          "name": "Huaiyu Wan",
          "affiliation": null
        },
        {
          "name": "Sheng Sun",
          "affiliation": null
        },
        {
          "name": "Lvchun Wang",
          "affiliation": null
        },
        {
          "name": "Wei Yu",
          "affiliation": null
        },
        {
          "name": "Yuxuan Liang",
          "affiliation": null
        }
      ],
      "abstract": "The underperformance of existing multimodal large language models for time series reasoning lies in the absence of rationale priors that connect temporal observations to their downstream outcomes, which leads models to rely on superficial pattern matching rather than principled reasoning. We therefore propose the rationale-grounded in-context learning for time series reasoning, where rationales work as guiding reasoning units rather than post-hoc explanations, and develop the RationaleTS method. Specifically, we firstly induce label-conditioned rationales, composed of reasoning paths from observable evidence to the potential outcomes. Then, we design the hybrid retrieval by balancing temporal patterns and semantic contexts to retrieve correlated rationale priors for the final in-context inference on new samples. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed RationaleTS on three-domain time series reasoning tasks. We will release our code for reproduction.",
      "publishedDate": "2026-01-06T12:27:04Z",
      "updatedDate": "2026-01-06T12:27:04Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02968v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02968",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02954",
      "title": "The World is Not Mono: Enabling Spatial Understanding in Large Audio-Language Models",
      "authors": [
        {
          "name": "Yuhuan You",
          "affiliation": null
        },
        {
          "name": "Lai Wei",
          "affiliation": null
        },
        {
          "name": "Xihong Wu",
          "affiliation": null
        },
        {
          "name": "Tianshu Qu",
          "affiliation": null
        }
      ],
      "abstract": "Existing large audio-language models perceive the world as \"mono\" -- a single stream of audio that ignores the critical spatial dimension (\"where\") required for universal acoustic scene analysis. To bridge this gap, we first introduce a hierarchical framework for Auditory Scene Analysis (ASA). Guided by this framework, we introduce a system that enables models like Qwen2-Audio to understand and reason about the complex acoustic world. Our framework achieves this through three core contributions: First, we build a large-scale, synthesized binaural audio dataset to provide the rich spatial cues. Second, we design a hybrid feature projector, which leverages parallel semantic and spatial encoders to extract decoupled representations. These distinct streams are integrated via a dense fusion mechanism, ensuring the model receives a holistic view of the acoustic scene. Finally, we employ a progressive training curriculum, advancing from supervised fine-tuning (SFT) to reinforcement learning via Group Relative Policy Optimization (GRPO), to explicitly evolve the model's capabilities towards reasoning. On our comprehensive benchmark, the model demonstrates comparatively strong capability for spatial understanding. By enabling this spatial perception, our work provides a clear pathway for leveraging the powerful reasoning abilities of large models towards holistic acoustic scene analysis, advancing from \"mono\" semantic recognition to spatial intelligence.",
      "publishedDate": "2026-01-06T11:54:47Z",
      "updatedDate": "2026-01-06T11:54:47Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02954v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02954",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02950",
      "title": "Batch-of-Thought: Cross-Instance Learning for Enhanced LLM Reasoning",
      "authors": [
        {
          "name": "Xuan Yang",
          "affiliation": null
        },
        {
          "name": "Furong Jia",
          "affiliation": null
        },
        {
          "name": "Roy Xie",
          "affiliation": null
        },
        {
          "name": "Xiong Xi",
          "affiliation": null
        },
        {
          "name": "Hengwei Bian",
          "affiliation": null
        },
        {
          "name": "Jian Li",
          "affiliation": null
        },
        {
          "name": "Monica Agrawal",
          "affiliation": null
        }
      ],
      "abstract": "Current Large Language Model reasoning systems process queries independently, discarding valuable cross-instance signals such as shared reasoning patterns and consistency constraints. We introduce Batch-of-Thought (BoT), a training-free method that processes related queries jointly to enable cross-instance learning. By performing comparative analysis across batches, BoT identifies high-quality reasoning templates, detects errors through consistency checks, and amortizes computational costs. We instantiate BoT within a multi-agent reflection architecture (BoT-R), where a Reflector performs joint evaluation to unlock mutual information gain unavailable in isolated processing. Experiments across three model families and six benchmarks demonstrate that BoT-R consistently improves accuracy and confidence calibration while reducing inference costs by up to 61%. Our theoretical and experimental analysis reveals when and why batch-aware reasoning benefits LLM systems.",
      "publishedDate": "2026-01-06T11:47:45Z",
      "updatedDate": "2026-01-06T11:47:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02950v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02950",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02918",
      "title": "Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning",
      "authors": [
        {
          "name": "Guoqiang Liang",
          "affiliation": null
        },
        {
          "name": "Jianyi Wang",
          "affiliation": null
        },
        {
          "name": "Zhonghua Wu",
          "affiliation": null
        },
        {
          "name": "Shangchen Zhou",
          "affiliation": null
        }
      ],
      "abstract": "Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or provide low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA, enabling joint generation of quality descriptions and scores. However, we notice that existing VLM-based IQA methods tend to exhibit unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions; and 2) reinforcement learning (RL) for dynamic policy exploration, primarily stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, and supported by a Progressive Re-sampling Strategy to mitigate annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.",
      "publishedDate": "2026-01-06T11:00:17Z",
      "updatedDate": "2026-01-06T11:00:17Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02918v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02918",
      "comment": "Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02902",
      "title": "Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning",
      "authors": [
        {
          "name": "Xinglang Zhang",
          "affiliation": null
        },
        {
          "name": "Yunyao Zhang",
          "affiliation": null
        },
        {
          "name": "ZeLiang Chen",
          "affiliation": null
        },
        {
          "name": "Junqing Yu",
          "affiliation": null
        },
        {
          "name": "Wei Yang",
          "affiliation": null
        },
        {
          "name": "Zikai Song",
          "affiliation": null
        }
      ],
      "abstract": "Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.",
      "publishedDate": "2026-01-06T10:38:25Z",
      "updatedDate": "2026-01-06T10:38:25Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02902v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02902",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02888",
      "title": "RPIQ: Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization for Visually Impaired Assistance",
      "authors": [
        {
          "name": "Xuanyu Wang",
          "affiliation": null
        },
        {
          "name": "Haisen Su",
          "affiliation": null
        },
        {
          "name": "Jingtao Zhang",
          "affiliation": null
        },
        {
          "name": "Xiangxiang Wang",
          "affiliation": null
        },
        {
          "name": "Yongbin Yu",
          "affiliation": null
        },
        {
          "name": "Manping Fan",
          "affiliation": null
        },
        {
          "name": "Bo Gong",
          "affiliation": null
        },
        {
          "name": "Siqi Chen",
          "affiliation": null
        },
        {
          "name": "Mingsheng Cao",
          "affiliation": null
        },
        {
          "name": "Liyong Ren",
          "affiliation": null
        }
      ],
      "abstract": "Visually impaired users face significant challenges in daily information access and real-time environmental perception, and there is an urgent need for intelligent assistive systems with accurate recognition capabilities. Although large-scale models provide effective solutions for perception and reasoning, their practical deployment on assistive devices is severely constrained by excessive memory consumption and high inference costs. Moreover, existing quantization strategies often ignore inter-block error accumulation, leading to degraded model stability. To address these challenges, this study proposes a novel quantization framework -- Residual-Projected Multi-Collaboration Closed-Loop and Single Instance Quantization(RPIQ), whose quantization process adopts a multi-collaborative closed-loop compensation scheme based on Single Instance Calibration and Gauss-Seidel Iterative Quantization. Experiments on various types of large-scale models, including language models such as OPT, Qwen, and LLaMA, as well as vision-language models such as CogVLM2, demonstrate that RPIQ can compress models to 4-bit representation while significantly reducing peak memory consumption (approximately 60%-75% reduction compared to original full-precision models). The method maintains performance highly close to full-precision models across multiple language and visual tasks, and exhibits excellent recognition and reasoning capabilities in key applications such as text understanding and visual question answering in complex scenarios. While verifying the effectiveness of RPIQ for deployment in real assistive systems, this study also advances the computational efficiency and reliability of large models, enabling them to provide visually impaired users with the required information accurately and rapidly.",
      "publishedDate": "2026-01-06T10:22:34Z",
      "updatedDate": "2026-01-06T10:22:34Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02888v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02888",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "tool-use",
        "reasoning",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02880",
      "title": "ReTreVal: Reasoning Tree with Validation - A Hybrid Framework for Enhanced LLM Multi-Step Reasoning",
      "authors": [
        {
          "name": "Abhishek HS",
          "affiliation": null
        },
        {
          "name": "Pavan C Shekar",
          "affiliation": null
        },
        {
          "name": "Arpit Jain",
          "affiliation": null
        },
        {
          "name": "Ashwanth Krishnan",
          "affiliation": null
        }
      ],
      "abstract": "Multi-step reasoning remains a key challenge for Large Language Models (LLMs), particularly in complex domains such as mathematics and creative writing. While recent approaches including ReAct, Reflexion, and Self-Refine improve reasoning through iterative refinement and reflection, they often lack structured exploration of alternative solution paths and persistent learning across problems. We propose ReTreVal (Reasoning Tree with Validation), a hybrid framework that integrates Tree-of-Thoughts exploration, self-refinement, LLM-based critique scoring, and reflexion memory to enable bounded and validated multi-step reasoning. ReTreVal constructs a structured reasoning tree with adaptive depth based on problem complexity, where each node undergoes iterative self-critique and refinement guided by explicit LLM-generated feedback. A dual validation mechanism evaluates reasoning quality, coherence, and correctness at each node while persistently storing insights from successful reasoning paths and failure patterns in a reflexion memory buffer, enabling cross-problem learning. Critique-based pruning retains only the top-k highest-scoring nodes at each level, controlling computational cost while preserving high-quality solution paths. We evaluate ReTreVal against ReAct, Reflexion, and Self-Refine across 500 mathematical problems and creative writing tasks using Qwen 2.5 7B as the underlying LLM, and demonstrate that ReTreVal consistently outperforms existing methods through its combination of structured exploration, critique-driven refinement, and cross-problem memory, making it particularly effective for tasks requiring exploratory reasoning, rigorous verification, and knowledge transfer.",
      "publishedDate": "2026-01-06T10:05:30Z",
      "updatedDate": "2026-01-06T10:05:30Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02880v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02880",
      "comment": "14 pages, 1 figure, 5 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02872",
      "title": "LongBench Pro: A More Realistic and Comprehensive Bilingual Long-Context Evaluation Benchmark",
      "authors": [
        {
          "name": "Ziyang Chen",
          "affiliation": null
        },
        {
          "name": "Xing Wu",
          "affiliation": null
        },
        {
          "name": "Junlong Jia",
          "affiliation": null
        },
        {
          "name": "Chaochen Gao",
          "affiliation": null
        },
        {
          "name": "Qi Fu",
          "affiliation": null
        },
        {
          "name": "Debing Zhang",
          "affiliation": null
        },
        {
          "name": "Songlin Hu",
          "affiliation": null
        }
      ],
      "abstract": "The rapid expansion of context length in large language models (LLMs) has outpaced existing evaluation benchmarks. Current long-context benchmarks often trade off scalability and realism: synthetic tasks underrepresent real-world complexity, while fully manual annotation is costly to scale to extreme lengths and diverse scenarios. We present LongBench Pro, a more realistic and comprehensive bilingual benchmark of 1,500 naturally occurring long-context samples in English and Chinese spanning 11 primary tasks and 25 secondary tasks, with input lengths from 8k to 256k tokens. LongBench Pro supports fine-grained analysis with task-specific metrics and a multi-dimensional taxonomy of context requirement (full vs. partial dependency), length (six levels), and difficulty (four levels calibrated by model performance). To balance quality with scalability, we propose a Human-Model Collaborative Construction pipeline: frontier LLMs draft challenging questions and reference answers, along with design rationales and solution processes, to reduce the cost of expert verification. Experts then rigorously validate correctness and refine problematic cases. Evaluating 46 widely used long-context LLMs on LongBench Pro yields three findings: (1) long-context optimization contributes more to long-context comprehension than parameter scaling; (2) effective context length is typically shorter than the claimed context length, with pronounced cross-lingual misalignment; and (3) the \"thinking\" paradigm helps primarily models trained with native reasoning, while mixed-thinking designs offer a promising Pareto trade-off. In summary, LongBench Pro provides a robust testbed for advancing long-context understanding.",
      "publishedDate": "2026-01-06T10:01:59Z",
      "updatedDate": "2026-01-06T10:01:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02872v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02872",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02739",
      "title": "Mitigating Prompt-Induced Hallucinations in Large Language Models via Structured Reasoning",
      "authors": [
        {
          "name": "Jinbo Hao",
          "affiliation": null
        },
        {
          "name": "Kai Yang",
          "affiliation": null
        },
        {
          "name": "Qingzhen Su",
          "affiliation": null
        },
        {
          "name": "Yang Chen",
          "affiliation": null
        },
        {
          "name": "Yifan Li",
          "affiliation": null
        },
        {
          "name": "Chao Jiang",
          "affiliation": null
        }
      ],
      "abstract": "To address hallucination issues in large language models (LLMs), this paper proposes a method for mitigating prompt-induced hallucinations. Building on a knowledge distillation chain-style model, we introduce a code module to guide knowledge-graph exploration and incorporate code as part of the chain-of-thought prompt, forming an external knowledge input that provides more accurate and structured information to the model. Based on this design, we develop an improved knowledge distillation chain-style model and leverage it to analyze and constrain the reasoning process of LLMs, thereby improving inference accuracy. We empirically evaluate the proposed approach using GPT-4 and LLaMA-3.3 on multiple public datasets. Experimental results demonstrate that incorporating code modules significantly enhances the model's ability to capture contextual information and effectively mitigates prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 improve by 15.64%, 13.38%, and 13.28%, respectively. Moreover, the proposed method achieves HIT@1, HIT@3, and HIT@5 scores exceeding 95% across several evaluation settings. These results indicate that the proposed approach substantially reduces hallucination behavior while improving the accuracy and verifiability of large language models.",
      "publishedDate": "2026-01-06T06:02:45Z",
      "updatedDate": "2026-01-06T06:02:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02739v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02739",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02737",
      "title": "Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench",
      "authors": [
        {
          "name": "Zanting Ye",
          "affiliation": null
        },
        {
          "name": "Xiaolong Niu",
          "affiliation": null
        },
        {
          "name": "Xuanbin Wu",
          "affiliation": null
        },
        {
          "name": "Xu Han",
          "affiliation": null
        },
        {
          "name": "Shengyuan Liu",
          "affiliation": null
        },
        {
          "name": "Jing Hao",
          "affiliation": null
        },
        {
          "name": "Zhihao Peng",
          "affiliation": null
        },
        {
          "name": "Hao Sun",
          "affiliation": null
        },
        {
          "name": "Jieqin Lv",
          "affiliation": null
        },
        {
          "name": "Fanghu Wang",
          "affiliation": null
        },
        {
          "name": "Yanchao Huang",
          "affiliation": null
        },
        {
          "name": "Hubing Wu",
          "affiliation": null
        },
        {
          "name": "Yixuan Yuan",
          "affiliation": null
        },
        {
          "name": "Habib Zaidi",
          "affiliation": null
        },
        {
          "name": "Arman Rahmim",
          "affiliation": null
        },
        {
          "name": "Yefeng Zheng",
          "affiliation": null
        },
        {
          "name": "Lijun Lu",
          "affiliation": null
        }
      ],
      "abstract": "While Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, transforming CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.",
      "publishedDate": "2026-01-06T05:58:50Z",
      "updatedDate": "2026-01-06T05:58:50Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02737v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02737",
      "comment": "9 pages, 6 figures, 6 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02714",
      "title": "Time-Scaling Is What Agents Need Now",
      "authors": [
        {
          "name": "Zhi Liu",
          "affiliation": null
        },
        {
          "name": "Guangzhi Wang",
          "affiliation": null
        }
      ],
      "abstract": "Early artificial intelligence paradigms exhibited separated cognitive functions: Neural Networks focused on \"perception-representation,\" Reinforcement Learning on \"decision-making-behavior,\" and Symbolic AI on \"knowledge-reasoning.\" With Transformer-based large models and world models, these paradigms are converging into cognitive agents with closed-loop \"perception-decision-action\" capabilities. Humans solve complex problems under limited cognitive resources through temporalized sequential reasoning. Language relies on problem space search for deep semantic reasoning. While early large language models (LLMs) could generate fluent text, they lacked robust semantic reasoning capabilities. Prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) extended reasoning paths by making intermediate steps explicit. Recent models like DeepSeek-R1 enhanced performance through explicit reasoning trajectories. However, these methods have limitations in search completeness and efficiency. This highlights the need for \"Time-Scaling\"--the systematic extension and optimization of an agent's ability to unfold reasoning over time. Time-Scaling refers to architectural design utilizing extended temporal pathways, enabling deeper problem space exploration, dynamic strategy adjustment, and enhanced metacognitive control, paralleling human sequential reasoning under cognitive constraints. It represents a critical frontier for enhancing deep reasoning and problem-solving without proportional increases in static model parameters. Advancing intelligent agent capabilities requires placing Time-Scaling principles at the forefront, positioning explicit temporal reasoning management as foundational.",
      "publishedDate": "2026-01-06T05:01:17Z",
      "updatedDate": "2026-01-06T05:01:17Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02714v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02714",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "prompting",
        "agents"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02663",
      "title": "When Do Tools and Planning Help LLMs Think? A Cost- and Latency-Aware Benchmark",
      "authors": [
        {
          "name": "Subha Ghoshal",
          "affiliation": null
        },
        {
          "name": "Ali Al-Bustami",
          "affiliation": null
        }
      ],
      "abstract": "Modern large language models (LLMs) increasingly rely on inference-time planning and external tools to improve reasoning. We benchmark this behavior on two real-world settings: event-centric question answering over graph-structured knowledge (Event-QA) and persuasive response generation in Reddit ChangeMyView (CMV). Using LangChain and LangGraph, we compare a one-shot baseline against a plan--execute--replan agent equipped with task-specific tools (DBpedia SPARQL/lookup/schema exploration, Wikipedia-focused retrieval, and topical web search). We evaluate on 60 examples each from Event-QA and CMV (3 splits of 20), and report both mean end-to-end latency and per-example token cost estimates. We evaluate GPT-4o and GPT-4o-mini under identical workflows and report accuracy and end-to-end latency. On Event-QA, the best tool-augmented configuration improves accuracy (e.g., 47.5\\% $\\rightarrow$ 67.5\\% for GPT-4o) while increasing latency by orders of magnitude ($\\sim$8s $\\rightarrow$ $\\sim$317s per example). On CMV, one-shot prompting is strongest (e.g., GPT-4o-mini achieves 75\\% at $\\sim$6s), and planning+search increases latency substantially without consistent gains. However, complex multi-tool orchestration exposes failure modes where the smaller model degrades. Overall, the findings highlight the need for task-specific, cost-aware choices of both model size and agent/tooling complexity.",
      "publishedDate": "2026-01-06T02:24:29Z",
      "updatedDate": "2026-01-06T02:24:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02663v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02663",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02604",
      "title": "Scalable Construction of a Lung Cancer Knowledge Base: Profiling Semantic Reasoning in LLMs",
      "authors": [
        {
          "name": "Cesar Felipe Martínez Cisneros",
          "affiliation": null
        },
        {
          "name": "Jesús Ulises Quiroz Bautista",
          "affiliation": null
        },
        {
          "name": "Claudia Anahí Guzmán Solano",
          "affiliation": null
        },
        {
          "name": "Bogdan Kaleb García Rivera",
          "affiliation": null
        },
        {
          "name": "Iván García Pacheco",
          "affiliation": null
        },
        {
          "name": "Yalbi Itzel Balderas Martínez",
          "affiliation": null
        },
        {
          "name": "Kolawole John Adebayoc",
          "affiliation": null
        },
        {
          "name": "Ignacio Arroyo Fernández",
          "affiliation": null
        }
      ],
      "abstract": "The integration of Large Language Models (LLMs) into biomedical research offers new opportunities for domainspecific reasoning and knowledge representation. However, their performance depends heavily on the semantic quality of training data. In oncology, where precision and interpretability are vital, scalable methods for constructing structured knowledge bases are essential for effective fine-tuning. This study presents a pipeline for developing a lung cancer knowledge base using Open Information Extraction (OpenIE). The process includes: (1) identifying medical concepts with the MeSH thesaurus; (2) filtering open-access PubMed literature with permissive licenses (CC0); (3) extracting (subject, relation, object) triplets using OpenIE method; and (4) enriching triplet sets with Named Entity Recognition (NER) to ensure biomedical relevance. The resulting triplet sets provide a domain-specific, large-scale, and noise-aware resource for fine-tuning LLMs. We evaluated T5 models finetuned on this dataset through Supervised Semantic Fine-Tuning. Comparative assessments with ROUGE and BERTScore show significantly improved performance and semantic coherence, demonstrating the potential of OpenIE-derived resources as scalable, low-cost solutions for enhancing biomedical NLP.",
      "publishedDate": "2026-01-05T23:40:00Z",
      "updatedDate": "2026-01-05T23:40:00Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02604v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02604",
      "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works",
      "journalRef": null,
      "doi": "10.1109/ENC68268.2025.11311861",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02569",
      "title": "LoRA-Drop: Temporal LoRA Decoding for Efficient LLM Inference",
      "authors": [
        {
          "name": "Hossein Rajabzadeh",
          "affiliation": null
        },
        {
          "name": "Maryam Dialameh",
          "affiliation": null
        },
        {
          "name": "Chul B. Park",
          "affiliation": null
        },
        {
          "name": "Il-Min Kim",
          "affiliation": null
        },
        {
          "name": "Hyock Ju Kwon",
          "affiliation": null
        }
      ],
      "abstract": "Autoregressive large language models (LLMs) are bottlenecked by sequential decoding, where each new token typically requires executing all transformer layers. Existing dynamic-depth and layer-skipping methods reduce this cost, but often rely on auxiliary routing mechanisms or incur accuracy degradation when bypassed layers are left uncompensated. We present \\textbf{LoRA-Drop}, a plug-and-play inference framework that accelerates decoding by applying a \\emph{temporal compute schedule} to a fixed subset of intermediate layers: on most decoding steps, selected layers reuse the previous-token hidden state and apply a low-rank LoRA correction, while periodic \\emph{refresh} steps execute the full model to prevent drift. LoRA-Drop requires no routing network, is compatible with standard KV caching, and can reduce KV-cache footprint by skipping KV updates in droppable layers during LoRA steps and refreshing periodically. Across \\textbf{LLaMA2-7B}, \\textbf{LLaMA3-8B}, \\textbf{Qwen2.5-7B}, and \\textbf{Qwen2.5-14B}, LoRA-Drop achieves up to \\textbf{2.6$\\times$ faster decoding} and \\textbf{45--55\\% KV-cache reduction} while staying within \\textbf{0.5 percentage points (pp)} of baseline accuracy. Evaluations on reasoning (GSM8K, MATH, BBH), code generation (HumanEval, MBPP), and long-context/multilingual benchmarks (LongBench, XNLI, XCOPA) identify a consistent \\emph{safe zone} of scheduling configurations that preserves quality while delivering substantial efficiency gains, providing a simple path toward adaptive-capacity inference in LLMs. Codes are available at https://github.com/hosseinbv/LoRA-Drop.git.",
      "publishedDate": "2026-01-05T21:47:47Z",
      "updatedDate": "2026-01-05T21:47:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02569v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02569",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02559",
      "title": "PerspectiveCoach: Exploring LLMs for Developer Reflection",
      "authors": [
        {
          "name": "Lauren Olson",
          "affiliation": null
        },
        {
          "name": "Emitzá Guzmán",
          "affiliation": null
        },
        {
          "name": "Florian Kunneman",
          "affiliation": null
        }
      ],
      "abstract": "Despite growing awareness of ethical challenges in software development, practitioners still lack structured tools that help them critically engage with the lived experiences of marginalized users. This paper presents PerspectiveCoach, a large language model (LLM)-powered conversational tool designed to guide developers through structured perspective-taking exercises and deepen critical reflection on how software design decisions affect marginalized communities. Through a controlled study with 18 front-end developers (balanced by sex), who interacted with the tool using a real case of online gender-based harassment, we examine how PerspectiveCoach supports ethical reasoning and engagement with user perspectives. Qualitative analysis revealed increased self-awareness, broadened perspectives, and more nuanced ethical articulation, while a complementary human-human study contextualized these findings. Text similarity analyses demonstrated that participants in the human-PerspectiveCoach study improved the fidelity of their restatements over multiple attempts, capturing both surface-level and semantic aspects of user concerns. However, human-PerspectiveCoach's restatements had a lower baseline than the human-human conversations, highlighting contextual differences in impersonal and interpersonal perspective-taking. Across the study, participants rated the tool highly for usability and relevance. This work contributes an exploratory design for LLM-powered end-user perspective-taking that supports critical, ethical self-reflection and offers empirical insights (i.e., enhancing adaptivity, centering plurality) into how such tools can help practitioners build more inclusive and socially responsive technologies.",
      "publishedDate": "2026-01-05T21:21:55Z",
      "updatedDate": "2026-01-05T21:21:55Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.HC"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02559",
      "comment": "48th International Conference of Software Engineering",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02457",
      "title": "PatchAlign3D: Local Feature Alignment for Dense 3D Shape understanding",
      "authors": [
        {
          "name": "Souhail Hadgi",
          "affiliation": null
        },
        {
          "name": "Bingchen Gong",
          "affiliation": null
        },
        {
          "name": "Ramana Sundararaman",
          "affiliation": null
        },
        {
          "name": "Emery Pierson",
          "affiliation": null
        },
        {
          "name": "Lei Li",
          "affiliation": null
        },
        {
          "name": "Peter Wonka",
          "affiliation": null
        },
        {
          "name": "Maks Ovsjanikov",
          "affiliation": null
        }
      ],
      "abstract": "Current foundation models for 3D shapes excel at global tasks (retrieval, classification) but transfer poorly to local part-level reasoning. Recent approaches leverage vision and language foundation models to directly solve dense tasks through multi-view renderings and text queries. While promising, these pipelines require expensive inference over multiple renderings, depend heavily on large language-model (LLM) prompt engineering for captions, and fail to exploit the inherent 3D geometry of shapes. We address this gap by introducing an encoder-only 3D model that produces language-aligned patch-level features directly from point clouds. Our pre-training approach builds on existing data engines that generate part-annotated 3D shapes by pairing multi-view SAM regions with VLM captioning. Using this data, we train a point cloud transformer encoder in two stages: (1) distillation of dense 2D features from visual encoders such as DINOv2 into 3D patches, and (2) alignment of these patch embeddings with part-level text embeddings through a multi-positive contrastive objective. Our 3D encoder achieves zero-shot 3D part segmentation with fast single-pass inference without any test-time multi-view rendering, while significantly outperforming previous rendering-based and feed-forward approaches across several 3D part segmentation benchmarks. Project website: https://souhail-hadgi.github.io/patchalign3dsite/",
      "publishedDate": "2026-01-05T18:55:45Z",
      "updatedDate": "2026-01-05T18:55:45Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02457v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02457",
      "comment": "Project website: https://souhail-hadgi.github.io/patchalign3dsite/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "rag",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02433",
      "title": "Physical Transformer",
      "authors": [
        {
          "name": "Tao Xu",
          "affiliation": null
        },
        {
          "name": "Zhixin Hu",
          "affiliation": null
        },
        {
          "name": "Li Luo",
          "affiliation": null
        },
        {
          "name": "Momiao Xiong",
          "affiliation": null
        }
      ],
      "abstract": "Digital AI systems spanning large language models, vision models, and generative architectures that operate primarily in symbolic, linguistic, or pixel domains. They have achieved striking progress, but almost all of this progress lives in virtual spaces. These systems transform embeddings and tokens, yet do not themselves touch the world and rarely admit a physical interpretation. In this work we propose a physical transformer that couples modern transformer style computation with geometric representation and physical dynamics. At the micro level, attention heads, and feed-forward blocks are modeled as interacting spins governed by effective Hamiltonians plus non-Hamiltonian bath terms. At the meso level, their aggregated state evolves on a learned Neural Differential Manifold (NDM) under Hamiltonian flows and Hamilton, Jacobi, Bellman (HJB) optimal control, discretized by symplectic layers that approximately preserve geometric and energetic invariants. At the macro level, the model maintains a generative semantic workspace and a two-dimensional information-phase portrait that tracks uncertainty and information gain over a reasoning trajectory. Within this hierarchy, reasoning tasks are formulated as controlled information flows on the manifold, with solutions corresponding to low cost trajectories that satisfy geometric, energetic, and workspace-consistency constraints. On simple toy problems involving numerical integration and dynamical systems, the physical transformer outperforms naive baselines in stability and long-horizon accuracy, highlighting the benefits of respecting underlying geometric and Hamiltonian structure. More broadly, the framework suggests a path toward physical AI that unify digital reasoning with physically grounded manifolds, opening a route to more interpretable and potentially unified models of reasoning, control, and interaction with the real world.",
      "publishedDate": "2026-01-05T06:29:39Z",
      "updatedDate": "2026-01-05T06:29:39Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02433v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02433",
      "comment": "38 pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03178",
      "title": "DiffBench Meets DiffAgent: End-to-End LLM-Driven Diffusion Acceleration Code Generation",
      "authors": [
        {
          "name": "Jiajun jiao",
          "affiliation": null
        },
        {
          "name": "Haowei Zhu",
          "affiliation": null
        },
        {
          "name": "Puyuan Yang",
          "affiliation": null
        },
        {
          "name": "Jianghui Wang",
          "affiliation": null
        },
        {
          "name": "Ji Liu",
          "affiliation": null
        },
        {
          "name": "Ziqiong Liu",
          "affiliation": null
        },
        {
          "name": "Dong Li",
          "affiliation": null
        },
        {
          "name": "Yuejian Fang",
          "affiliation": null
        },
        {
          "name": "Junhai Yong",
          "affiliation": null
        },
        {
          "name": "Bin Wang",
          "affiliation": null
        },
        {
          "name": "Emad Barsoum",
          "affiliation": null
        }
      ],
      "abstract": "Diffusion models have achieved remarkable success in image and video generation. However, their inherently multiple step inference process imposes substantial computational overhead, hindering real-world deployment. Accelerating diffusion models is therefore essential, yet determining how to combine multiple model acceleration techniques remains a significant challenge. To address this issue, we introduce a framework driven by large language models (LLMs) for automated acceleration code generation and evaluation. First, we present DiffBench, a comprehensive benchmark that implements a three stage automated evaluation pipeline across diverse diffusion architectures, optimization combinations and deployment scenarios. Second, we propose DiffAgent, an agent that generates optimal acceleration strategies and codes for arbitrary diffusion models. DiffAgent employs a closed-loop workflow in which a planning component and a debugging component iteratively refine the output of a code generation component, while a genetic algorithm extracts performance feedback from the execution environment to guide subsequent code refinements. We provide a detailed explanation of the DiffBench construction and the design principles underlying DiffAgent. Extensive experiments show that DiffBench offers a thorough evaluation of generated codes and that DiffAgent significantly outperforms existing LLMs in producing effective diffusion acceleration strategies.",
      "publishedDate": "2026-01-06T16:55:55Z",
      "updatedDate": "2026-01-06T16:55:55Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03178v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03178",
      "comment": "Accepted to AAAI 2026",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "planning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03061",
      "title": "Vertical tacit collusion in AI-mediated markets",
      "authors": [
        {
          "name": "Felipe M. Affonso",
          "affiliation": null
        }
      ],
      "abstract": "AI shopping agents are being deployed to hundreds of millions of consumers, creating a new intermediary between platforms, sellers, and buyers. We identify a novel market failure: vertical tacit collusion, where platforms controlling rankings and sellers controlling product descriptions independently learn to exploit documented AI cognitive biases. Using multi-agent simulation calibrated to empirical measurements of large language model biases, we show that joint exploitation produces consumer harm more than double what would occur if strategies were independent. This super-additive harm arises because platform ranking determines which products occupy bias-triggering positions while seller manipulation determines conversion rates. Unlike horizontal algorithmic collusion, vertical tacit collusion requires no coordination and evades antitrust detection because harm emerges from aligned incentives rather than agreement. Our findings identify an urgent regulatory gap as AI shopping agents reach mainstream adoption.",
      "publishedDate": "2026-01-06T14:43:14Z",
      "updatedDate": "2026-01-06T14:43:14Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03061v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03061",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "multi-agent",
        "agents"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03011",
      "title": "ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios",
      "authors": [
        {
          "name": "Yihan Wei",
          "affiliation": null
        },
        {
          "name": "Shenghai Yuan",
          "affiliation": null
        },
        {
          "name": "Tianchen Deng",
          "affiliation": null
        },
        {
          "name": "Boyang Lou",
          "affiliation": null
        },
        {
          "name": "Enwen Hu",
          "affiliation": null
        }
      ],
      "abstract": "Corner cases are rare or extreme scenarios that drive real-world failures, but they are difficult to curate at scale: web data are noisy, labels are brittle, and edge deployments preclude large retraining. We present ReCCur (Recursive Corner-Case Curation), a low-compute framework that converts noisy web imagery into auditable fine-grained labels via a multi-agent recursive pipeline. First, large-scale data acquisition and filtering expands a domain vocabulary with a vision-language model (VLM), crawls the web, and enforces tri-modal (image, description, keyword) consistency with light human spot checks to yield refined candidates. Next, mixture-of-experts knowledge distillation uses complementary encoders (e.g., CLIP, DINOv2, BEiT) for kNN voting with dual-confidence activation and uncertainty sampling, converging to a high-precision set. Finally, region-evidence VLM adversarial labeling pairs a proposer (multi-granularity regions and semantic cues) with a validator (global and local chained consistency) to produce explainable labels and close the loop. On realistic corner-case scenarios (e.g., flooded-car inspection), ReCCur runs on consumer-grade GPUs, steadily improves purity and separability, and requires minimal human supervision, providing a practical substrate for downstream training and evaluation under resource constraints. Code and dataset will be released.",
      "publishedDate": "2026-01-06T13:36:43Z",
      "updatedDate": "2026-01-06T13:36:43Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03011v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03011",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02407",
      "title": "Evolving Personalities in Chaos: An LLM-Augmented Framework for Character Discovery in the Iterated Prisoners Dilemma under Environmental Stress",
      "authors": [
        {
          "name": "Oguzhan Yildirim",
          "affiliation": null
        }
      ],
      "abstract": "Standard simulations of the Iterated Prisoners Dilemma (IPD) operate in deterministic, noise-free environments, producing strategies that may be theoretically optimal but fragile when confronted with real-world uncertainty. This paper addresses two critical gaps in evolutionary game theory research: (1) the absence of realistic environmental stressors during strategy evolution, and (2) the Interpretability Gap, where evolved genetic strategies remain opaque binary sequences devoid of semantic meaning. We introduce a novel framework combining stochastic environmental perturbations (God Mode) with Large Language Model (LLM)-based behavioral profiling to transform evolved genotypes into interpretable character archetypes. Our experiments demonstrate that strategies evolved under chaotic conditions exhibit superior resilience and present distinct behavioral phenotypes, ranging from Ruthless Capitalists to Diplomatic Enforcers. These phenotypes are readily classified by LLMs but remain nearly impossible to interpret through manual genome inspection alone. This work bridges evolutionary computation with explainable AI and provides a template for automated agent characterization in multi-agent systems.",
      "publishedDate": "2026-01-01T18:34:05Z",
      "updatedDate": "2026-01-01T18:34:05Z",
      "primaryCategory": "cs.NE",
      "arxivCategories": [
        "cs.NE",
        "cs.GT"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02407v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02407",
      "comment": "10 pages, 5 figures. Project assignment; exploratory study on LLM-based adaptive agents",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02957",
      "title": "LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation",
      "authors": [
        {
          "name": "Fabian Lukassen",
          "affiliation": null
        },
        {
          "name": "Christoph Weisser",
          "affiliation": null
        },
        {
          "name": "Michael Schlee",
          "affiliation": null
        },
        {
          "name": "Manish Kumar",
          "affiliation": null
        },
        {
          "name": "Anton Thielmann",
          "affiliation": null
        },
        {
          "name": "Benjamin Saefken",
          "affiliation": null
        },
        {
          "name": "Thomas Kneib",
          "affiliation": null
        }
      ],
      "abstract": "This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.",
      "publishedDate": "2026-01-06T12:04:38Z",
      "updatedDate": "2026-01-06T12:04:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02957v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02957",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02956",
      "title": "Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion",
      "authors": [
        {
          "name": "Jeonghyun Park",
          "affiliation": null
        },
        {
          "name": "Byeongjeong Kim",
          "affiliation": null
        },
        {
          "name": "Seojin Hwang",
          "affiliation": null
        },
        {
          "name": "Hwanhee Lee",
          "affiliation": null
        }
      ],
      "abstract": "Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.",
      "publishedDate": "2026-01-06T12:01:56Z",
      "updatedDate": "2026-01-06T12:01:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02956v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02956",
      "comment": "20 pages, 5 figures, 15 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02814",
      "title": "Causal-Enhanced AI Agents for Medical Research Screening",
      "authors": [
        {
          "name": "Duc Ngo",
          "affiliation": null
        },
        {
          "name": "Arya Rahgoza",
          "affiliation": null
        }
      ],
      "abstract": "Systematic reviews are essential for evidence-based medicine, but reviewing 1.5 million+ annual publications manually is infeasible. Current AI approaches suffer from hallucinations in systematic review tasks, with studies reporting rates ranging from 28--40% for earlier models to 2--15% for modern implementations which is unacceptable when errors impact patient care. We present a causal graph-enhanced retrieval-augmented generation system integrating explicit causal reasoning with dual-level knowledge graphs. Our approach enforces evidence-first protocols where every causal claim traces to retrieved literature and automatically generates directed acyclic graphs visualizing intervention-outcome pathways. Evaluation on 234 dementia exercise abstracts shows CausalAgent achieves 95% accuracy, 100% retrieval success, and zero hallucinations versus 34% accuracy and 10% hallucinations for baseline AI. Automatic causal graphs enable explicit mechanism modeling, visual synthesis, and enhanced interpretability. While this proof-of-concept evaluation used ten questions focused on dementia exercise research, the architectural approach demonstrates transferable principles for trustworthy medical AI and causal reasoning's potential for high-stakes healthcare.",
      "publishedDate": "2026-01-06T08:41:16Z",
      "updatedDate": "2026-01-06T08:41:16Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02814v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02814",
      "comment": "for submission to The 39th Canadian Conference on Artificial Intelligence",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02522",
      "title": "On the Effectiveness of Proposed Techniques to Reduce Energy Consumption in RAG Systems: A Controlled Experiment",
      "authors": [
        {
          "name": "Zhinuan",
          "affiliation": null
        },
        {
          "name": "Guo",
          "affiliation": null
        },
        {
          "name": "Chushu Gao",
          "affiliation": null
        },
        {
          "name": "Justus Bogner",
          "affiliation": null
        }
      ],
      "abstract": "The rising energy demands of machine learning (ML), e.g., implemented in popular variants like retrieval-augmented generation (RAG) systems, have raised significant concerns about their environmental sustainability. While previous research has proposed green tactics for ML-enabled systems, their empirical evaluation within RAG systems remains largely unexplored. This study presents a controlled experiment investigating five practical techniques aimed at reducing energy consumption in RAG systems. Using a production-like RAG system developed at our collaboration partner, the Software Improvement Group, we evaluated the impact of these techniques on energy consumption, latency, and accuracy. Through a total of 9 configurations spanning over 200 hours of trials using the CRAG dataset, we reveal that techniques such as increasing similarity retrieval thresholds, reducing embedding sizes, applying vector indexing, and using a BM25S reranker can significantly reduce energy usage, up to 60% in some cases. However, several techniques also led to unacceptable accuracy decreases, e.g., by up to 30% for the indexing strategies. Notably, finding an optimal retrieval threshold and reducing embedding size substantially reduced energy consumption and latency with no loss in accuracy, making these two techniques truly energy-efficient. We present the first comprehensive, empirical study on energy-efficient design techniques for RAG systems, providing guidance for developers and researchers aiming to build sustainable RAG applications.",
      "publishedDate": "2026-01-05T19:50:49Z",
      "updatedDate": "2026-01-05T19:50:49Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02522v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02522",
      "comment": "Accepted for publication at the 2026 International Conference on Software Engineering: Software Engineering in Society (ICSE-SEIS'26)",
      "journalRef": null,
      "doi": "10.1145/3786581.3786932",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "multi-agent",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "multi-agent",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02504",
      "title": "Enhancing Debugging Skills with AI-Powered Assistance: A Real-Time Tool for Debugging Support",
      "authors": [
        {
          "name": "Elizaveta Artser",
          "affiliation": null
        },
        {
          "name": "Daniil Karol",
          "affiliation": null
        },
        {
          "name": "Anna Potriasaeva",
          "affiliation": null
        },
        {
          "name": "Aleksei Rostovskii",
          "affiliation": null
        },
        {
          "name": "Katsiaryna Dzialets",
          "affiliation": null
        },
        {
          "name": "Ekaterina Koshchenko",
          "affiliation": null
        },
        {
          "name": "Xiaotian Su",
          "affiliation": null
        },
        {
          "name": "April Yi Wang",
          "affiliation": null
        },
        {
          "name": "Anastasiia Birillo",
          "affiliation": null
        }
      ],
      "abstract": "Debugging is a crucial skill in programming education and software development, yet it is often overlooked in CS curricula. To address this, we introduce an AI-powered debugging assistant integrated into an IDE. It offers real-time support by analyzing code, suggesting breakpoints, and providing contextual hints. Using RAG with LLMs, program slicing, and custom heuristics, it enhances efficiency by minimizing LLM calls and improving accuracy. A three-level evaluation - technical analysis, UX study, and classroom tests - highlights its potential for teaching debugging.",
      "publishedDate": "2026-01-05T19:20:59Z",
      "updatedDate": "2026-01-05T19:20:59Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02504v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02504",
      "comment": "Accepted at ICSE SEET 2026, 6 pages, 2 figures",
      "journalRef": null,
      "doi": "10.1145/3786580.3786976",
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02428",
      "title": "A Dynamic Retrieval-Augmented Generation System with Selective Memory and Remembrance",
      "authors": [
        {
          "name": "Okan Bursa",
          "affiliation": null
        }
      ],
      "abstract": "We introduce \\emph{Adaptive RAG Memory} (ARM), a retrieval-augmented generation (RAG) framework that replaces a static vector index with a \\emph{dynamic} memory substrate governed by selective remembrance and decay. Frequently retrieved items are consolidated and protected from forgetting, while rarely used items gradually decay, inspired by cognitive consolidation and forgetting principles. On a lightweight retrieval benchmark, ARM reaches near state-of-the-art performance (e.g., NDCG@5 $\\approx$ 0.940, Recall@5 $=1.000$) with only $\\sim$22M parameters in the embedding layer, achieving the best efficiency among ultra-efficient models ($<$25M parameters). In addition, we compare static vs. dynamic RAG combinations across Llama 3.1 and GPT-4o. Llama 3.1 with static RAG achieves the highest key-term coverage (67.2\\%) at moderate latency, while GPT-4o with a dynamic selective retrieval policy attains the fastest responses (8.2s on average) with competitive coverage (58.7\\%). We further present an engineering optimization of the DynamicRAG implementation, making embedding weights configurable, adjustable at runtime, and robust to invalid settings. ARM yields competitive accuracy, self-regularizing memory growth, and interpretable retention dynamics without retraining the generator\\color{black} and provides practical trade-off between quality, latency and memory efficiency for production and research RAG system.",
      "publishedDate": "2026-01-04T21:51:41Z",
      "updatedDate": "2026-01-04T21:51:41Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02428v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02428",
      "comment": "6 Pages, 2 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03170",
      "title": "Segment-Aware Conditioning for Training-Free Intra-Utterance Emotion and Duration Control in Text-to-Speech",
      "authors": [
        {
          "name": "Qifan Liang",
          "affiliation": null
        },
        {
          "name": "Yuansen Liu",
          "affiliation": null
        },
        {
          "name": "Ruixin Wei",
          "affiliation": null
        },
        {
          "name": "Nan Lu",
          "affiliation": null
        },
        {
          "name": "Junchuan Zhao",
          "affiliation": null
        },
        {
          "name": "Ye Wang",
          "affiliation": null
        }
      ],
      "abstract": "While controllable Text-to-Speech (TTS) has achieved notable progress, most existing methods remain limited to inter-utterance-level control, making fine-grained intra-utterance expression challenging due to their reliance on non-public datasets or complex multi-stage training. In this paper, we propose a training-free controllable framework for pretrained zero-shot TTS to enable intra-utterance emotion and duration expression. Specifically, we propose a segment-aware emotion conditioning strategy that combines causal masking with monotonic stream alignment filtering to isolate emotion conditioning and schedule mask transitions, enabling smooth intra-utterance emotion shifts while preserving global semantic coherence. Based on this, we further propose a segment-aware duration steering strategy to combine local duration embedding steering with global EOS logit modulation, allowing local duration adjustment while ensuring globally consistent termination. To eliminate the need for segment-level manual prompt engineering, we construct a 30,000-sample multi-emotion and duration-annotated text dataset to enable LLM-based automatic prompt construction. Extensive experiments demonstrate that our training-free method not only achieves state-of-the-art intra-utterance consistency in multi-emotion and duration control, but also maintains baseline-level speech quality of the underlying TTS model. Audio samples are available at https://aclanonymous111.github.io/TED-TTS-DemoPage/.",
      "publishedDate": "2026-01-06T16:51:04Z",
      "updatedDate": "2026-01-06T16:51:04Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03170v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03170",
      "comment": "24 pages, 8 figures, 7 tables, 3 lists",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03156",
      "title": "Prompt-Counterfactual Explanations for Generative AI System Behavior",
      "authors": [
        {
          "name": "Sofie Goethals",
          "affiliation": null
        },
        {
          "name": "Foster Provost",
          "affiliation": null
        },
        {
          "name": "João Sedoc",
          "affiliation": null
        }
      ],
      "abstract": "As generative AI systems become integrated into real-world applications, organizations increasingly need to be able to understand and interpret their behavior. In particular, decision-makers need to understand what causes generative AI systems to exhibit specific output characteristics. Within this general topic, this paper examines a key question: what is it about the input -the prompt- that causes an LLM-based generative AI system to produce output that exhibits specific characteristics, such as toxicity, negative sentiment, or political bias. To examine this question, we adapt a common technique from the Explainable AI literature: counterfactual explanations. We explain why traditional counterfactual explanations cannot be applied directly to generative AI systems, due to several differences in how generative AI systems function. We then propose a flexible framework that adapts counterfactual explanations to non-deterministic, generative AI systems in scenarios where downstream classifiers can reveal key characteristics of their outputs. Based on this framework, we introduce an algorithm for generating prompt-counterfactual explanations (PCEs). Finally, we demonstrate the production of counterfactual explanations for generative AI systems with three case studies, examining different output characteristics (viz., political leaning, toxicity, and sentiment). The case studies further show that PCEs can streamline prompt engineering to suppress undesirable output characteristics and can enhance red-teaming efforts to uncover additional prompts that elicit undesirable outputs. Ultimately, this work lays a foundation for prompt-focused interpretability in generative AI: a capability that will become indispensable as these models are entrusted with higher-stakes tasks and subject to emerging regulatory requirements for transparency and accountability.",
      "publishedDate": "2026-01-06T16:33:19Z",
      "updatedDate": "2026-01-06T16:33:19Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03156v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03156",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03130",
      "title": "Automatic Prompt Engineering with No Task Cues and No Tuning",
      "authors": [
        {
          "name": "Faisal Chowdhury",
          "affiliation": null
        },
        {
          "name": "Nandana Mihindukulasooriya",
          "affiliation": null
        },
        {
          "name": "Niharika S D'Souza",
          "affiliation": null
        },
        {
          "name": "Horst Samulowitz",
          "affiliation": null
        },
        {
          "name": "Neeru Gupta",
          "affiliation": null
        },
        {
          "name": "Tomasz Hanusiak",
          "affiliation": null
        },
        {
          "name": "Michal Kapitonow",
          "affiliation": null
        }
      ],
      "abstract": "This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.",
      "publishedDate": "2026-01-06T16:04:45Z",
      "updatedDate": "2026-01-06T16:04:45Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03130v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03130",
      "comment": null,
      "journalRef": "The IEEE International Conference on Data Mining (ICDM) 2025 : Demo Track",
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02896",
      "title": "Bridging Mechanistic Interpretability and Prompt Engineering with Gradient Ascent for Interpretable Persona Control",
      "authors": [
        {
          "name": "Harshvardhan Saini",
          "affiliation": null
        },
        {
          "name": "Yiming Tang",
          "affiliation": null
        },
        {
          "name": "Dianbo Liu",
          "affiliation": null
        }
      ],
      "abstract": "Controlling emergent behavioral personas (e.g., sycophancy, hallucination) in Large Language Models (LLMs) is critical for AI safety, yet remains a persistent challenge. Existing solutions face a dilemma: manual prompt engineering is intuitive but unscalable and imprecise, while automatic optimization methods are effective but operate as \"black boxes\" with no interpretable connection to model internals. We propose a novel framework that adapts gradient ascent to LLMs, enabling targeted prompt discovery. In specific, we propose two methods, RESGA and SAEGA, that both optimize randomly initialized prompts to achieve better aligned representation with an identified persona direction. We introduce fluent gradient ascent to control the fluency of discovered persona steering prompts. We demonstrate RESGA and SAEGA's effectiveness across Llama 3.1, Qwen 2.5, and Gemma 3 for steering three different personas,sycophancy, hallucination, and myopic reward. Crucially, on sycophancy, our automatically discovered prompts achieve significant improvement (49.90% compared with 79.24%). By grounding prompt discovery in mechanistically meaningful features, our method offers a new paradigm for controllable and interpretable behavior modification.",
      "publishedDate": "2026-01-06T10:34:14Z",
      "updatedDate": "2026-01-06T10:34:14Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02896v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02896",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02683",
      "title": "Learning from Prompt itself: the Hierarchical Attribution Prompt Optimization",
      "authors": [
        {
          "name": "Dongyu Chen",
          "affiliation": null
        },
        {
          "name": "Jian Ma",
          "affiliation": null
        },
        {
          "name": "Xianpeng Zhang",
          "affiliation": null
        },
        {
          "name": "Lei Zhang",
          "affiliation": null
        },
        {
          "name": "Haonan Lu",
          "affiliation": null
        },
        {
          "name": "Chen Chen",
          "affiliation": null
        },
        {
          "name": "Chuangchuang Wang",
          "affiliation": null
        },
        {
          "name": "Kai Tang",
          "affiliation": null
        }
      ],
      "abstract": "Optimization is fundamental across numerous disciplines, typically following an iterative process of refining an initial solution to enhance performance. This principle is equally critical in prompt engineering, where designing effective prompts for large language models constitutes a complex optimization challenge. A structured optimization approach requires automated or semi-automated procedures to develop improved prompts, thereby reducing manual effort, improving performance, and yielding an interpretable process. However, current prompt optimization methods often induce prompt drift, where new prompts fix prior failures but impair performance on previously successful tasks. Additionally, generating prompts from scratch can compromise interpretability. To address these limitations, this study proposes the Hierarchical Attribution Prompt Optimization (HAPO) framework, which introduces three innovations: (1) a dynamic attribution mechanism targeting error patterns in training data and prompting history, (2) semantic-unit optimization for editing functional prompt segments, and (3) multimodal-friendly progression supporting both end-to-end LLM and LLM-MLLM workflows. Applied in contexts like single/multi-image QA (e.g., OCRV2) and complex task analysis (e.g., BBH), HAPO demonstrates enhanced optimization efficiency, outperforming comparable automated prompt optimization methods and establishing an extensible paradigm for scalable prompt engineering.",
      "publishedDate": "2026-01-06T03:34:17Z",
      "updatedDate": "2026-01-06T03:34:17Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02683v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02683",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting"
      ],
      "tags": {
        "auto": [
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02662",
      "title": "When Prompting Meets Spiking: Graph Sparse Prompting via Spiking Graph Prompt Learning",
      "authors": [
        {
          "name": "Bo Jiang",
          "affiliation": null
        },
        {
          "name": "Weijun Zhao",
          "affiliation": null
        },
        {
          "name": "Beibei Wang",
          "affiliation": null
        },
        {
          "name": "Jin Tang",
          "affiliation": null
        }
      ],
      "abstract": "Graph Prompt Feature (GPF) learning has been widely used in adapting pre-trained GNN model on the downstream task. GPFs first introduce some prompt atoms and then learns the optimal prompt vector for each graph node using the linear combination of prompt atoms. However, existing GPFs generally conduct prompting over node's all feature dimensions which is obviously redundant and also be sensitive to node feature noise. To overcome this issue, for the first time, this paper proposes learning sparse graph prompts by leveraging the spiking neuron mechanism, termed Spiking Graph Prompt Feature (SpikingGPF). Our approach is motivated by the observation that spiking neuron can perform inexpensive information processing and produce sparse outputs which naturally fits the task of our graph sparse prompting. Specifically, SpikingGPF has two main aspects. First, it learns a sparse prompt vector for each node by exploiting a spiking neuron architecture, enabling prompting on selective node features. This yields a more compact and lightweight prompting design while also improving robustness against node noise. Second, SpikingGPF introduces a novel prompt representation learning model based on sparse representation theory, i.e., it represents each node prompt as a sparse combination of prompt atoms. This encourages a more compact representation and also facilitates efficient computation. Extensive experiments on several benchmarks demonstrate the effectiveness and robustness of SpikingGPF.",
      "publishedDate": "2026-01-06T02:22:04Z",
      "updatedDate": "2026-01-06T02:22:04Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02662v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02662",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "prompting",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03149",
      "title": "PersonaLedger: Generating Realistic Financial Transactions with Persona Conditioned LLMs and Rule Grounded Feedback",
      "authors": [
        {
          "name": "Dehao Yuan",
          "affiliation": null
        },
        {
          "name": "Tyler Farnan",
          "affiliation": null
        },
        {
          "name": "Stefan Tesliuc",
          "affiliation": null
        },
        {
          "name": "Doron L Bergman",
          "affiliation": null
        },
        {
          "name": "Yulun Wu",
          "affiliation": null
        },
        {
          "name": "Xiaoyu Liu",
          "affiliation": null
        },
        {
          "name": "Minghui Liu",
          "affiliation": null
        },
        {
          "name": "James Montgomery",
          "affiliation": null
        },
        {
          "name": "Nam H Nguyen",
          "affiliation": null
        },
        {
          "name": "C. Bayan Bruss",
          "affiliation": null
        },
        {
          "name": "Furong Huang",
          "affiliation": null
        }
      ],
      "abstract": "Strict privacy regulations limit access to real transaction data, slowing open research in financial AI. Synthetic data can bridge this gap, but existing generators do not jointly achieve behavioral diversity and logical groundedness. Rule-driven simulators rely on hand-crafted workflows and shallow stochasticity, which miss the richness of human behavior. Learning-based generators such as GANs capture correlations yet often violate hard financial constraints and still require training on private data. We introduce PersonaLedger, a generation engine that uses a large language model conditioned on rich user personas to produce diverse transaction streams, coupled with an expert configurable programmatic engine that maintains correctness. The LLM and engine interact in a closed loop: after each event, the engine updates the user state, enforces financial rules, and returns a context aware \"nextprompt\" that guides the LLM toward feasible next actions. With this engine, we create a public dataset of 30 million transactions from 23,000 users and a benchmark suite with two tasks, illiquidity classification and identity theft segmentation. PersonaLedger offers a realistic, privacy preserving resource that supports rigorous evaluation of forecasting and anomaly detection models. PersonaLedger offers the community a rich, realistic, and privacy preserving resource -- complete with code, rules, and generation logs -- to accelerate innovation in financial AI and enable rigorous, reproducible evaluation.",
      "publishedDate": "2026-01-06T16:18:59Z",
      "updatedDate": "2026-01-06T16:18:59Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03149v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03149",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03067",
      "title": "Joint Encoding of KV-Cache Blocks for Scalable LLM Serving",
      "authors": [
        {
          "name": "Joseph Kampeas",
          "affiliation": null
        },
        {
          "name": "Emir Haleva",
          "affiliation": null
        }
      ],
      "abstract": "Modern large language models (LLMs) drive interactive AI systems but are bottlenecked by the memory-heavy growth of key-value (KV) caches, which limits real-time throughput under concurrent loads. Existing KV-cache compression methods rely on rigid heuristics, disrupt tensor layouts, or require specialized compute, hindering scalability and deployment. We propose joint encoding of KV-cache blocks, which fuses similar blocks across requests and input chunks into shared representations while preserving standard cache structure. This alleviates the KV-cache memory bottleneck, supporting high-concurrency serving without specialized hardware. Theoretically, we analyze the rate-distortion tradeoff of fused cache blocks under a Poisson process model. Empirically, our method achieves up to 4.38 $\\times$ KV-cache compression with negligible accuracy loss across diverse LLMs and benchmarks, outperforming recent structured and adaptive compression baselines. In real LLM serving, joint encoding improves the token throughput by $\\sim$40\\% on a single-machine vLLM benchmark, demonstrating substantial gains in inference throughput. Code is available at https://github.com/sef1/kv_fast_fusion kv_joint_encoding.",
      "publishedDate": "2026-01-06T14:50:58Z",
      "updatedDate": "2026-01-06T14:50:58Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03067v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03067",
      "comment": "12 pages, 16 figures, 2 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02997",
      "title": "From Memorization to Creativity: LLM as a Designer of Novel Neural-Architectures",
      "authors": [
        {
          "name": "Waleed Khalid",
          "affiliation": null
        },
        {
          "name": "Dmitry Ignatov",
          "affiliation": null
        },
        {
          "name": "Radu Timofte",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) excel in program synthesis, yet their ability to autonomously navigate neural architecture design--balancing syntactic reliability, performance, and structural novelty--remains underexplored. We address this by placing a code-oriented LLM within a closed-loop synthesis framework, analyzing its evolution over 22 supervised fine-tuning cycles. The model synthesizes PyTorch convolutional networks which are validated, evaluated via low-fidelity performance signals (single-epoch accuracy), and filtered using a MinHash-Jaccard criterion to prevent structural redundancy. High-performing, novel architectures are converted into prompt-code pairs for iterative fine-tuning via parameter-efficient LoRA adaptation, initialized from the LEMUR dataset. Across cycles, the LLM internalizes empirical architectural priors, becoming a robust generator. The valid generation rate stabilizes at 50.6 percent (peaking at 74.5 percent), while mean first-epoch accuracy rises from 28.06 percent to 50.99 percent, and the fraction of candidates exceeding 40 percent accuracy grows from 2.04 percent to 96.81 percent. Analyses confirm the model moves beyond replicating existing motifs, synthesizing 455 high-performing architectures absent from the original corpus. By grounding code synthesis in execution feedback, this work provides a scalable blueprint for transforming stochastic generators into autonomous, performance-driven neural designers, establishing that LLMs can internalize empirical, non-textual rewards to transcend their training data.",
      "publishedDate": "2026-01-06T13:20:28Z",
      "updatedDate": "2026-01-06T13:20:28Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02997v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02997",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "agents",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "agents",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02967",
      "title": "MoE Adapter for Large Audio Language Models: Sparsity, Disentanglement, and Gradient-Conflict-Free",
      "authors": [
        {
          "name": "Yishu Lei",
          "affiliation": null
        },
        {
          "name": "Shuwei He",
          "affiliation": null
        },
        {
          "name": "Jing Hu",
          "affiliation": null
        },
        {
          "name": "Dan Zhang",
          "affiliation": null
        },
        {
          "name": "Xianlong Luo",
          "affiliation": null
        },
        {
          "name": "Danxiang Zhu",
          "affiliation": null
        },
        {
          "name": "Shikun Feng",
          "affiliation": null
        },
        {
          "name": "Rui Liu",
          "affiliation": null
        },
        {
          "name": "Jingzhou He",
          "affiliation": null
        },
        {
          "name": "Yu Sun",
          "affiliation": null
        },
        {
          "name": "Hua Wu",
          "affiliation": null
        },
        {
          "name": "Haifeng Wang",
          "affiliation": null
        }
      ],
      "abstract": "Extending the input modality of Large Language Models~(LLMs) to the audio domain is essential for achieving comprehensive multimodal perception. However, it is well-known that acoustic information is intrinsically \\textit{heterogeneous}, entangling attributes such as speech, music, and environmental context. Existing research is limited to a dense, parameter-shared adapter to model these diverse patterns, which induces \\textit{gradient conflict} during optimization, as parameter updates required for distinct attributes contradict each other. To address this limitation, we introduce the \\textit{\\textbf{MoE-Adapter}}, a sparse Mixture-of-Experts~(MoE) architecture designed to decouple acoustic information. Specifically, it employs a dynamic gating mechanism that routes audio tokens to specialized experts capturing complementary feature subspaces while retaining shared experts for global context, thereby mitigating gradient conflicts and enabling fine-grained feature learning. Comprehensive experiments show that the MoE-Adapter achieves superior performance on both audio semantic and paralinguistic tasks, consistently outperforming dense linear baselines with comparable computational costs. Furthermore, we will release the related code and models to facilitate future research.",
      "publishedDate": "2026-01-06T12:24:38Z",
      "updatedDate": "2026-01-06T12:24:38Z",
      "primaryCategory": "cs.SD",
      "arxivCategories": [
        "cs.SD",
        "cs.AI",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02967v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02967",
      "comment": "13 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02875",
      "title": "Revisiting Data Compression with Language Modeling",
      "authors": [
        {
          "name": "Chen-Han Tsai",
          "affiliation": null
        }
      ],
      "abstract": "In this report, we investigate the potential use of large language models (LLM's) in the task of data compression. Previous works have demonstrated promising results in applying LLM's towards compressing not only text, but also a wide range of multi-modal data. Despite the favorable performance achieved, there still remains several practical questions that pose a challenge towards replacing existing data compression algorithms with LLM's. In this work, we explore different methods to achieve a lower adjusted compression rate using LLM's as data compressors. In comparison to previous works, we were able to achieve a new state-of-the-art (SOTA) adjusted compression rate of around $18\\%$ on the enwik9 dataset without additional model training. Furthermore, we explore the use of LLM's in compressing non-English data, code data, byte stream sequences. We show that while LLM's excel in compressing data in text-dominant domains, their ability in compressing non-natural text sequences still remain competitive if configured in the right way.",
      "publishedDate": "2026-01-06T10:03:33Z",
      "updatedDate": "2026-01-06T10:03:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02875v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02875",
      "comment": "Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation"
      ],
      "tags": {
        "auto": [
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02868",
      "title": "CodeMEM: AST-Guided Adaptive Memory for Repository-Level Iterative Code Generation",
      "authors": [
        {
          "name": "Peiding Wang",
          "affiliation": null
        },
        {
          "name": "Li Zhang",
          "affiliation": null
        },
        {
          "name": "Fang Liu",
          "affiliation": null
        },
        {
          "name": "Chongyang Tao",
          "affiliation": null
        },
        {
          "name": "Yinghao Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) substantially enhance developer productivity in repository-level code generation through interactive collaboration. However, as interactions progress, repository context must be continuously preserved and updated to integrate newly validated information. Meanwhile, the expanding session history increases cognitive burden, often leading to forgetting and the reintroduction of previously resolved errors. Existing memory management approaches show promise but remain limited by natural language-centric representations. To overcome these limitations, we propose CodeMEM, an AST-guided dynamic memory management system tailored for repository-level iterative code generation. Specifically, CodeMEM introduces the Code Context Memory component that dynamically maintains and updates repository context through AST-guided LLM operations, along with the Code Session Memory that constructs a code-centric representation of interaction history and explicitly detects and mitigates forgetting through AST-based analysis. Experimental results on the instruction-following benchmark CodeIF-Bench and the code generation benchmark CoderEval demonstrate that CodeMEM achieves state-of-the-art performance, improving instruction following by 12.2% for the current turn and 11.5% for the session level, and reducing interaction rounds by 2-3, while maintaining competitive inference latency and token efficiency.",
      "publishedDate": "2026-01-06T09:57:19Z",
      "updatedDate": "2026-01-06T09:57:19Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02868v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02868",
      "comment": "preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02563",
      "title": "Compressed code: the hidden effects of quantization and distillation on programming tokens",
      "authors": [
        {
          "name": "Viacheslav Siniaev",
          "affiliation": null
        },
        {
          "name": "Iaroslav Chelombitko",
          "affiliation": null
        },
        {
          "name": "Aleksey Komissarov",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated exceptional code generation capabilities, yet their token-level mechanisms remain underexplored, particularly in compressed models. Through systematic analysis of programming language token representations, we characterize how programming languages are encoded in LLM tokenizers by analyzing their vocabulary distribution and keyword coverage patterns. We introduce a novel cold-start probability analysis method that provides insights into model behavior without requiring explicit prompts. Additionally, we present a comprehensive evaluation of how different model optimization techniques - including quantization, distillation, model scaling, and task-specific fine-tuning - affect token-level representations and code generation quality. Our experiments, supported by comprehensive probability distribution analysis and evaluation metrics, reveal critical insights into token-level behavior and provide empirically-validated guidelines for maintaining code generation quality under various optimization constraints. These findings advance both theoretical understanding of LLM code generation and practical implementation of optimized models in production environments.",
      "publishedDate": "2026-01-05T21:32:47Z",
      "updatedDate": "2026-01-05T21:32:47Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.CL",
        "cs.LG",
        "cs.PL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02563v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02563",
      "comment": "18 pages, 1 figure and 6 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02438",
      "title": "Focus on What Matters: Fisher-Guided Adaptive Multimodal Fusion for Vulnerability Detection",
      "authors": [
        {
          "name": "Yun Bian",
          "affiliation": null
        },
        {
          "name": "Yi Chen",
          "affiliation": null
        },
        {
          "name": "HaiQuan Wang",
          "affiliation": null
        },
        {
          "name": "ShiHao Li",
          "affiliation": null
        },
        {
          "name": "Zhe Cui",
          "affiliation": null
        }
      ],
      "abstract": "Software vulnerability detection is a critical task for securing software systems and can be formulated as a binary classification problem: given a code snippet, determine whether it contains a vulnerability. Existing multimodal approaches typically fuse Natural Code Sequence (NCS) representations from pretrained language models with Code Property Graph (CPG) representations from graph neural networks, often under the implicit assumption that adding a modality necessarily yields extra information. In practice, sequence and graph representations can be redundant, and fluctuations in the quality of the graph modality can dilute the discriminative signal of the dominant modality. To address this, we propose TaCCS-DFA, a framework that introduces Fisher information as a geometric measure of how sensitive feature directions are to the classification decision, enabling task-oriented complementary fusion. TaCCS-DFA online estimates a low-rank principal Fisher subspace and restricts cross-modal attention to task-sensitive directions, thereby retrieving structural features from CPG that complement the sequence modality; meanwhile, an adaptive gating mechanism dynamically adjusts the contribution of the graph modality for each sample to suppress noise propagation. Our analysis shows that, under an isotropic perturbation assumption, the proposed mechanism admits a tighter risk bound than conventional full-spectrum attention. Experiments on BigVul, Devign, and ReVeal show that TaCCS-DFA achieves strong performance across multiple backbones. With CodeT5 as the backbone, TaCCS-DFA reaches an F1 score of 87.80\\% on the highly imbalanced BigVul dataset, improving over a strong baseline Vul-LMGNNs by 6.3 percentage points while maintaining low calibration error and computational overhead.",
      "publishedDate": "2026-01-05T09:31:21Z",
      "updatedDate": "2026-01-05T09:31:21Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02438v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02438",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02410",
      "title": "The Vibe-Check Protocol: Quantifying Cognitive Offloading in AI Programming",
      "authors": [
        {
          "name": "Aizierjiang Aiersilan",
          "affiliation": null
        }
      ],
      "abstract": "The integration of Large Language Models (LLMs) into software engineering education has driven the emergence of ``Vibe Coding,'' a paradigm where developers articulate high-level intent through natural language and delegate implementation to AI agents. While proponents argue this approach modernizes pedagogy by emphasizing conceptual design over syntactic memorization, accumulating empirical evidence raises concerns regarding skill retention and deep conceptual understanding. This paper proposes a theoretical framework to investigate the research question: \\textit{Is Vibe Coding a better way to learn software engineering?} We posit a divergence in student outcomes between those leveraging AI for acceleration versus those using it for cognitive offloading. To evaluate these educational trade-offs, we propose the \\textbf{Vibe-Check Protocol (VCP)}, a systematic benchmarking framework incorporating three quantitative metrics: the \\textit{Cold Start Refactor} ($M_{CSR}$) for modeling skill decay; \\textit{Hallucination Trap Detection} ($M_{HT}$) based on signal detection theory to evaluate error identification; and the \\textit{Explainability Gap} ($E_{gap}$) for quantifying the divergence between code complexity and conceptual comprehension. Through controlled comparisons, VCP aims to provide a quantitative basis for educators to determine the optimal pedagogical boundary: identifying contexts where Vibe Coding fosters genuine mastery and contexts where it introduces hidden technical debt and superficial competence.",
      "publishedDate": "2026-01-02T06:13:41Z",
      "updatedDate": "2026-01-02T06:13:41Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI",
        "cs.CY",
        "cs.GR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02410v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02410",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "code-generation",
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02456",
      "title": "InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation",
      "authors": [
        {
          "name": "Junhao Cai",
          "affiliation": null
        },
        {
          "name": "Zetao Cai",
          "affiliation": null
        },
        {
          "name": "Jiafei Cao",
          "affiliation": null
        },
        {
          "name": "Yilun Chen",
          "affiliation": null
        },
        {
          "name": "Zeyu He",
          "affiliation": null
        },
        {
          "name": "Lei Jiang",
          "affiliation": null
        },
        {
          "name": "Hang Li",
          "affiliation": null
        },
        {
          "name": "Hengjie Li",
          "affiliation": null
        },
        {
          "name": "Yang Li",
          "affiliation": null
        },
        {
          "name": "Yufei Liu",
          "affiliation": null
        },
        {
          "name": "Yanan Lu",
          "affiliation": null
        },
        {
          "name": "Qi Lv",
          "affiliation": null
        },
        {
          "name": "Haoxiang Ma",
          "affiliation": null
        },
        {
          "name": "Jiangmiao Pang",
          "affiliation": null
        },
        {
          "name": "Yu Qiao",
          "affiliation": null
        },
        {
          "name": "Zherui Qiu",
          "affiliation": null
        },
        {
          "name": "Yanqing Shen",
          "affiliation": null
        },
        {
          "name": "Xu Shi",
          "affiliation": null
        },
        {
          "name": "Yang Tian",
          "affiliation": null
        },
        {
          "name": "Bolun Wang",
          "affiliation": null
        },
        {
          "name": "Hanqing Wang",
          "affiliation": null
        },
        {
          "name": "Jiaheng Wang",
          "affiliation": null
        },
        {
          "name": "Tai Wang",
          "affiliation": null
        },
        {
          "name": "Xueyuan Wei",
          "affiliation": null
        },
        {
          "name": "Chao Wu",
          "affiliation": null
        },
        {
          "name": "Yiman Xie",
          "affiliation": null
        },
        {
          "name": "Boyang Xing",
          "affiliation": null
        },
        {
          "name": "Yuqiang Yang",
          "affiliation": null
        },
        {
          "name": "Yuyin Yang",
          "affiliation": null
        },
        {
          "name": "Qiaojun Yu",
          "affiliation": null
        },
        {
          "name": "Feng Yuan",
          "affiliation": null
        },
        {
          "name": "Jia Zeng",
          "affiliation": null
        },
        {
          "name": "Jingjing Zhang",
          "affiliation": null
        },
        {
          "name": "Shenghan Zhang",
          "affiliation": null
        },
        {
          "name": "Shi Zhang",
          "affiliation": null
        },
        {
          "name": "Zhuoma Zhaxi",
          "affiliation": null
        },
        {
          "name": "Bowen Zhou",
          "affiliation": null
        },
        {
          "name": "Yuanzhen Zhou",
          "affiliation": null
        },
        {
          "name": "Yunsong Zhou",
          "affiliation": null
        },
        {
          "name": "Hongrui Zhu",
          "affiliation": null
        },
        {
          "name": "Yangkun Zhu",
          "affiliation": null
        },
        {
          "name": "Yuchen Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Prevalent Vision-Language-Action (VLA) models are typically built upon Multimodal Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-Transformers architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\\% improvement in daily tasks and a 40\\%-73.3\\% boost in dynamic settings, such as conveyor belt sorting.",
      "publishedDate": "2026-01-05T18:54:29Z",
      "updatedDate": "2026-01-05T18:54:29Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02456v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02456",
      "comment": "Homepage: https://internrobotics.github.io/internvla-a1.github.io/",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02845",
      "title": "TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents",
      "authors": [
        {
          "name": "Kai Li",
          "affiliation": null
        },
        {
          "name": "Xuanqing Yu",
          "affiliation": null
        },
        {
          "name": "Ziyi Ni",
          "affiliation": null
        },
        {
          "name": "Yi Zeng",
          "affiliation": null
        },
        {
          "name": "Yao Xu",
          "affiliation": null
        },
        {
          "name": "Zheqing Zhang",
          "affiliation": null
        },
        {
          "name": "Xin Li",
          "affiliation": null
        },
        {
          "name": "Jitao Sang",
          "affiliation": null
        },
        {
          "name": "Xiaogang Duan",
          "affiliation": null
        },
        {
          "name": "Xuelei Wang",
          "affiliation": null
        },
        {
          "name": "Chengbao Liu",
          "affiliation": null
        },
        {
          "name": "Jie Tan",
          "affiliation": null
        }
      ],
      "abstract": "Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.",
      "publishedDate": "2026-01-06T09:24:19Z",
      "updatedDate": "2026-01-06T09:24:19Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02845v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02845",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02578",
      "title": "DataParasite Enables Scalable and Repurposable Online Data Curation",
      "authors": [
        {
          "name": "Mengyi Sun",
          "affiliation": null
        }
      ],
      "abstract": "Many questions in computational social science rely on datasets assembled from heterogeneous online sources, a process that is often labor-intensive, costly, and difficult to reproduce. Recent advances in large language models enable agentic search and structured extraction from the web, but existing systems are frequently opaque, inflexible, or poorly suited to scientific data curation. Here we introduce DataParasite, an open-source, modular pipeline for scalable online data collection. DataParasite decomposes tabular curation tasks into independent, entity-level searches defined through lightweight configuration files and executed through a shared, task-agnostic python script. Crucially, the same pipeline can be repurposed to new tasks, including those without predefined entity lists, using only natural-language instructions. We evaluate the pipeline on multiple canonical tasks in computational social science, including faculty hiring histories, elite death events, and political career trajectories. Across tasks, DataParasite achieves high accuracy while reducing data-collection costs by an order of magnitude relative to manual curation. By lowering the technical and labor barriers to online data assembly, DataParasite provides a practical foundation for scalable, transparent, and reusable data curation in computational social science and beyond.",
      "publishedDate": "2026-01-05T22:04:16Z",
      "updatedDate": "2026-01-05T22:04:16Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.IR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02578v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02578",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "prompting"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02573",
      "title": "LendNova: Towards Automated Credit Risk Assessment with Language Models",
      "authors": [
        {
          "name": "Kiarash Shamsi",
          "affiliation": null
        },
        {
          "name": "Danijel Novokmet",
          "affiliation": null
        },
        {
          "name": "Joshua Peters",
          "affiliation": null
        },
        {
          "name": "Mao Lin Liu",
          "affiliation": null
        },
        {
          "name": "Paul K Edwards",
          "affiliation": null
        },
        {
          "name": "Vahab Khoshdel",
          "affiliation": null
        }
      ],
      "abstract": "Credit risk assessment is essential in the financial sector, but has traditionally depended on costly feature-based models that often fail to utilize all available information in raw credit records. This paper introduces LendNova, the first practical automated end-to-end pipeline for credit risk assessment, designed to utilize all available information in raw credit records by leveraging advanced NLP techniques and language models. LendNova transforms risk modeling by operating directly on raw, jargon-heavy credit bureau text using a language model that learns task-relevant representations without manual feature engineering. By automatically capturing patterns and risk signals embedded in the text, it replaces manual preprocessing steps, reducing costs and improving scalability. Evaluation on real-world data further demonstrates its strong potential in accurate and efficient risk assessment. LendNova establishes a baseline for intelligent credit risk agents, demonstrating the feasibility of language models in this domain. It lays the groundwork for future research toward foundation systems that enable more accurate, adaptable, and automated financial decision-making.",
      "publishedDate": "2026-01-05T21:53:36Z",
      "updatedDate": "2026-01-05T21:53:36Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02573v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02573",
      "comment": null,
      "journalRef": "AAAI 2026, Workshop on Agentic AI in Financial Services",
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.02439",
      "title": "WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks",
      "authors": [
        {
          "name": "Hao Bai",
          "affiliation": null
        },
        {
          "name": "Alexey Taymanov",
          "affiliation": null
        },
        {
          "name": "Tong Zhang",
          "affiliation": null
        },
        {
          "name": "Aviral Kumar",
          "affiliation": null
        },
        {
          "name": "Spencer Whitehead",
          "affiliation": null
        }
      ],
      "abstract": "We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent's own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.",
      "publishedDate": "2026-01-05T09:35:11Z",
      "updatedDate": "2026-01-05T09:35:11Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.02439v1",
      "arxivUrl": "https://arxiv.org/abs/2601.02439",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-07T03:24:26.131Z",
      "categories": [
        "agents",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03731",
      "title": "From Laboratory to Real-World Applications: Benchmarking Agentic Code Reasoning at the Repository Level",
      "authors": [
        {
          "name": "Jia Li",
          "affiliation": null
        },
        {
          "name": "Yuxin Su",
          "affiliation": null
        },
        {
          "name": "Michael R. Lyu",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) evolve into autonomous agents, evaluating repository-level reasoning, the ability to maintain logical consistency across massive, real-world, interdependent file systems, has become critical. Current benchmarks typically fluctuate between isolated code snippets and black-box evaluations. We present RepoReason, a white-box diagnostic benchmark centered on abductive assertion verification. To eliminate memorization while preserving authentic logical depth, we implement an execution-driven mutation framework that utilizes the environment as a semantic oracle to regenerate ground-truth states. Furthermore, we establish a fine-grained diagnostic system using dynamic program slicing, quantifying reasoning via three orthogonal metrics: $ESV$ (reading load), $MCL$ (simulation depth), and $DFI$ (integration width). Comprehensive evaluations of frontier models (e.g., Claude-4.5-Sonnet, DeepSeek-v3.1-Terminus) reveal a prevalent aggregation deficit, where integration width serves as the primary cognitive bottleneck. Our findings provide granular white-box insights for optimizing the next generation of agentic software engineering.",
      "publishedDate": "2026-01-07T09:22:28Z",
      "updatedDate": "2026-01-07T09:22:28Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03731v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03731",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03702",
      "title": "A Chromatographic Process Design and Optimization Platform Powered by Large Language Models: A Case Application on Extract of Ginkgo Biloba Leaf",
      "authors": [
        {
          "name": "Zhilong Tang",
          "affiliation": null
        },
        {
          "name": "Shaohua Wu",
          "affiliation": null
        },
        {
          "name": "Xinyan Zhao",
          "affiliation": null
        },
        {
          "name": "Yu Wang",
          "affiliation": null
        },
        {
          "name": "Xingchu Gong",
          "affiliation": null
        }
      ],
      "abstract": "Chromatographic separation technology has been widely applied in pharmaceutical, chemical, and food industries due to its high efficiency. However, traditional human-dependent chromatographic process development faces challenges such as reliance on expert experience, long development cycles, and labor intensity. ChromR, a large language model (LLM)-driven platform for chromatographic process design and optimization, is presented in this work. The platform integrates ChromLLM, a domain-specific LLM trained for chromatography, along with a multi-agent system and an automated chromatographic experimental device. The multi-agent system comprises four agents: domain knowledge answering, experimental design, experimental execution, and data analysis. ChromR enables automatic completion of the entire workflow-including initial process parameter recommendation, experimental design, automated execution, data analysis, and multi-objective optimization. By utilizing ChromR, dependency on expert knowledge is effectively reduced, while labor input and development time are significantly decreased. Chromatographic purification of the extract of Ginkgo biloba leaf (EGBL) was selected as a case study. ChromR successfully developed a chromatographic process within one week that meets multiple objectives, including fraction quality and production efficiency, reducing development time to approximately one-seventh of that required by the conventional paradigm. An intelligent, automated, and universally applicable new paradigm was established for chromatographic process development.",
      "publishedDate": "2026-01-07T08:40:20Z",
      "updatedDate": "2026-01-07T08:40:20Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03702v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03702",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03699",
      "title": "RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models",
      "authors": [
        {
          "name": "Quy-Anh Dang",
          "affiliation": null
        },
        {
          "name": "Chris Ngo",
          "affiliation": null
        },
        {
          "name": "Truong-Son Hy",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) become integral to safety-critical applications, ensuring their robustness against adversarial prompts is paramount. However, existing red teaming datasets suffer from inconsistent risk categorizations, limited domain coverage, and outdated evaluations, hindering systematic vulnerability assessments. To address these challenges, we introduce RedBench, a universal dataset aggregating 37 benchmark datasets from leading conferences and repositories, comprising 29,362 samples across attack and refusal prompts. RedBench employs a standardized taxonomy with 22 risk categories and 19 domains, enabling consistent and comprehensive evaluations of LLM vulnerabilities. We provide a detailed analysis of existing datasets, establish baselines for modern LLMs, and open-source the dataset and evaluation code. Our contributions facilitate robust comparisons, foster future research, and promote the development of secure and reliable LLMs for real-world deployment. Code: https://github.com/knoveleng/redeval",
      "publishedDate": "2026-01-07T08:34:17Z",
      "updatedDate": "2026-01-07T08:34:17Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03699v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03699",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03624",
      "title": "Architecting Agentic Communities using Design Patterns",
      "authors": [
        {
          "name": "Zoran Milosevic",
          "affiliation": null
        },
        {
          "name": "Fethi Rabhi",
          "affiliation": null
        }
      ],
      "abstract": "The rapid evolution of Large Language Models (LLM) and subsequent Agentic AI technologies requires systematic architectural guidance for building sophisticated, production-grade systems. This paper presents an approach for architecting such systems using design patterns derived from enterprise distributed systems standards, formal methods, and industry practice. We classify these patterns into three tiers: LLM Agents (task-specific automation), Agentic AI (adaptive goal-seekers), and Agentic Communities (organizational frameworks where AI agents and human participants coordinate through formal roles, protocols, and governance structures). We focus on Agentic Communities - coordination frameworks encompassing LLM Agents, Agentic AI entities, and humans - most relevant for enterprise and industrial applications. Drawing on established coordination principles from distributed systems, we ground these patterns in a formal framework that specifies collaboration agreements where AI agents and humans fill roles within governed ecosystems. This approach provides both practical guidance and formal verification capabilities, enabling expression of organizational, legal, and ethical rules through accountability mechanisms that ensure operational and verifiable governance of inter-agent communication, negotiation, and intent modeling. We validate this framework through a clinical trial matching case study. Our goal is to provide actionable guidance to practitioners while maintaining the formal rigor essential for enterprise deployment in dynamic, multi-agent ecosystems.",
      "publishedDate": "2026-01-07T06:10:07Z",
      "updatedDate": "2026-01-07T06:10:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03624v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03624",
      "comment": "supplementary material accompanying this paper is also attached .. its title is \"Complete Agentic AI Design Patterns Catalogue\"",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03605",
      "title": "DiVA: Fine-grained Factuality Verification with Agentic-Discriminative Verifier",
      "authors": [
        {
          "name": "Hui Huang",
          "affiliation": null
        },
        {
          "name": "Muyun Yang",
          "affiliation": null
        },
        {
          "name": "Yuki Arase",
          "affiliation": null
        }
      ],
      "abstract": "Despite the significant advancements of Large Language Models (LLMs), their factuality remains a critical challenge, fueling growing interest in factuality verification. Existing research on factuality verification primarily conducts binary judgments (e.g., correct or incorrect), which fails to distinguish varying degrees of error severity. This limits its utility for applications such as fine-grained evaluation and preference optimization. To bridge this gap, we propose the Agentic Discriminative Verifier (DiVA), a hybrid framework that synergizes the agentic search capabilities of generative models with the precise scoring aptitude of discriminative models. We also construct a new benchmark, FGVeriBench, as a robust testbed for fine-grained factuality verification. Experimental results on FGVeriBench demonstrate that our DiVA significantly outperforms existing methods on factuality verification for both general and multi-hop questions.",
      "publishedDate": "2026-01-07T05:35:01Z",
      "updatedDate": "2026-01-07T05:35:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03605v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03605",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03578",
      "title": "PsychEthicsBench: Evaluating Large Language Models Against Australian Mental Health Ethics",
      "authors": [
        {
          "name": "Yaling Shen",
          "affiliation": null
        },
        {
          "name": "Stephanie Fong",
          "affiliation": null
        },
        {
          "name": "Yiwen Jiang",
          "affiliation": null
        },
        {
          "name": "Zimu Wang",
          "affiliation": null
        },
        {
          "name": "Feilong Tang",
          "affiliation": null
        },
        {
          "name": "Qingyang Xu",
          "affiliation": null
        },
        {
          "name": "Xiangyu Zhao",
          "affiliation": null
        },
        {
          "name": "Zhongxing Xu",
          "affiliation": null
        },
        {
          "name": "Jiahe Liu",
          "affiliation": null
        },
        {
          "name": "Jinpeng Hu",
          "affiliation": null
        },
        {
          "name": "Dominic Dwyer",
          "affiliation": null
        },
        {
          "name": "Zongyuan Ge",
          "affiliation": null
        }
      ],
      "abstract": "The increasing integration of large language models (LLMs) into mental health applications necessitates robust frameworks for evaluating professional safety alignment. Current evaluative approaches primarily rely on refusal-based safety signals, which offer limited insight into the nuanced behaviors required in clinical practice. In mental health, clinically inadequate refusals can be perceived as unempathetic and discourage help-seeking. To address this gap, we move beyond refusal-centric metrics and introduce \\texttt{PsychEthicsBench}, the first principle-grounded benchmark based on Australian psychology and psychiatry guidelines, designed to evaluate LLMs' ethical knowledge and behavioral responses through multiple-choice and open-ended tasks with fine-grained ethicality annotations. Empirical results across 14 models reveal that refusal rates are poor indicators of ethical behavior, revealing a significant divergence between safety triggers and clinical appropriateness. Notably, we find that domain-specific fine-tuning can degrade ethical robustness, as several specialized models underperform their base backbones in ethical alignment. PsychEthicsBench provides a foundation for systematic, jurisdiction-aware evaluation of LLMs in mental health, encouraging more responsible development in this domain.",
      "publishedDate": "2026-01-07T04:49:02Z",
      "updatedDate": "2026-01-07T04:49:02Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03578v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03578",
      "comment": "17 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03479",
      "title": "Efficient Sequential Recommendation for Long Term User Interest Via Personalization",
      "authors": [
        {
          "name": "Qiang Zhang",
          "affiliation": null
        },
        {
          "name": "Hanchao Yu",
          "affiliation": null
        },
        {
          "name": "Ivan Ji",
          "affiliation": null
        },
        {
          "name": "Chen Yuan",
          "affiliation": null
        },
        {
          "name": "Yi Zhang",
          "affiliation": null
        },
        {
          "name": "Chihuang Liu",
          "affiliation": null
        },
        {
          "name": "Xiaolong Wang",
          "affiliation": null
        },
        {
          "name": "Christopher E. Lambert",
          "affiliation": null
        },
        {
          "name": "Ren Chen",
          "affiliation": null
        },
        {
          "name": "Chen Kovacs",
          "affiliation": null
        },
        {
          "name": "Xinzhu Bei",
          "affiliation": null
        },
        {
          "name": "Renqin Cai",
          "affiliation": null
        },
        {
          "name": "Rui Li",
          "affiliation": null
        },
        {
          "name": "Lizhu Zhang",
          "affiliation": null
        },
        {
          "name": "Xiangjun Fan",
          "affiliation": null
        },
        {
          "name": "Qunshu Zhang",
          "affiliation": null
        },
        {
          "name": "Benyu Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent years have witnessed success of sequential modeling, generative recommender, and large language model for recommendation. Though the scaling law has been validated for sequential models, it showed inefficiency in computational capacity when considering real-world applications like recommendation, due to the non-linear(quadratic) increasing nature of the transformer model. To improve the efficiency of the sequential model, we introduced a novel approach to sequential recommendation that leverages personalization techniques to enhance efficiency and performance. Our method compresses long user interaction histories into learnable tokens, which are then combined with recent interactions to generate recommendations. This approach significantly reduces computational costs while maintaining high recommendation accuracy. Our method could be applied to existing transformer based recommendation models, e.g., HSTU and HLLM. Extensive experiments on multiple sequential models demonstrate its versatility and effectiveness. Source code is available at \\href{https://github.com/facebookresearch/PerSRec}{https://github.com/facebookresearch/PerSRec}.",
      "publishedDate": "2026-01-07T00:15:44Z",
      "updatedDate": "2026-01-07T00:15:44Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03479v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03479",
      "comment": "ICDM 2025",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03475",
      "title": "CPGPrompt: Translating Clinical Guidelines into LLM-Executable Decision Support",
      "authors": [
        {
          "name": "Ruiqi Deng",
          "affiliation": null
        },
        {
          "name": "Geoffrey Martin",
          "affiliation": null
        },
        {
          "name": "Tony Wang",
          "affiliation": null
        },
        {
          "name": "Gongbo Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Liu",
          "affiliation": null
        },
        {
          "name": "Chunhua Weng",
          "affiliation": null
        },
        {
          "name": "Yanshan Wang",
          "affiliation": null
        },
        {
          "name": "Justin F Rousseau",
          "affiliation": null
        },
        {
          "name": "Yifan Peng",
          "affiliation": null
        }
      ],
      "abstract": "Clinical practice guidelines (CPGs) provide evidence-based recommendations for patient care; however, integrating them into Artificial Intelligence (AI) remains challenging. Previous approaches, such as rule-based systems, face significant limitations, including poor interpretability, inconsistent adherence to guidelines, and narrow domain applicability. To address this, we develop and validate CPGPrompt, an auto-prompting system that converts narrative clinical guidelines into large language models (LLMs). Our framework translates CPGs into structured decision trees and utilizes an LLM to dynamically navigate them for patient case evaluation. Synthetic vignettes were generated across three domains (headache, lower back pain, and prostate cancer) and distributed into four categories to test different decision scenarios. System performance was assessed on both binary specialty-referral decisions and fine-grained pathway-classification tasks. The binary specialty referral classification achieved consistently strong performance across all domains (F1: 0.85-1.00), with high recall (1.00 $\\pm$ 0.00). In contrast, multi-class pathway assignment showed reduced performance, with domain-specific variations: headache (F1: 0.47), lower back pain (F1: 0.72), and prostate cancer (F1: 0.77). Domain-specific performance differences reflected the structure of each guideline. The headache guideline highlighted challenges with negation handling. The lower back pain guideline required temporal reasoning. In contrast, prostate cancer pathways benefited from quantifiable laboratory tests, resulting in more reliable decision-making.",
      "publishedDate": "2026-01-07T00:05:42Z",
      "updatedDate": "2026-01-07T00:05:42Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03475v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03475",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03417",
      "title": "Implicit Graph, Explicit Retrieval: Towards Efficient and Interpretable Long-horizon Memory for Large Language Models",
      "authors": [
        {
          "name": "Xin Zhang",
          "affiliation": null
        },
        {
          "name": "Kailai Yang",
          "affiliation": null
        },
        {
          "name": "Hao Li",
          "affiliation": null
        },
        {
          "name": "Chenyue Li",
          "affiliation": null
        },
        {
          "name": "Qiyu Wei",
          "affiliation": null
        },
        {
          "name": "Sophia Ananiadou",
          "affiliation": null
        }
      ],
      "abstract": "Long-horizon applications increasingly require large language models (LLMs) to answer queries when relevant evidence is sparse and dispersed across very long contexts. Existing memory systems largely follow two paradigms: explicit structured memories offer interpretability but often become brittle under long-context overload, while latent memory mechanisms are efficient and stable yet difficult to inspect. We propose LatentGraphMem, a memory framework that combines implicit graph memory with explicit subgraph retrieval. LatentGraphMem stores a graph-structured memory in latent space for stability and efficiency, and exposes a task-specific subgraph retrieval interface that returns a compact symbolic subgraph under a fixed budget for downstream reasoning and human inspection. During training, an explicit graph view is materialized to interface with a frozen reasoner for question-answering supervision. At inference time, retrieval is performed in latent space and only the retrieved subgraph is externalized. Experiments on long-horizon benchmarks across multiple model scales show that LatentGraphMem consistently outperforms representative explicit-graph and latent-memory baselines, while enabling parameter-efficient adaptation and flexible scaling to larger reasoners without introducing large symbolic artifacts.",
      "publishedDate": "2026-01-06T21:10:10Z",
      "updatedDate": "2026-01-06T21:10:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03417v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03417",
      "comment": "11 pages, 5 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03403",
      "title": "Tigrinya Number Verbalization: Rules, Algorithm, and Implementation",
      "authors": [
        {
          "name": "Fitsum Gaim",
          "affiliation": null
        },
        {
          "name": "Issayas Tesfamariam",
          "affiliation": null
        }
      ],
      "abstract": "We present a systematic formalization of Tigrinya cardinal and ordinal number verbalization, addressing a gap in computational resources for the language. This work documents the canonical rules governing the expression of numerical values in spoken Tigrinya, including the conjunction system, scale words, and special cases for dates, times, and currency. We provide a formal algorithm for number-to-word conversion and release an open-source implementation. Evaluation of frontier large language models (LLMs) reveals significant gaps in their ability to accurately verbalize Tigrinya numbers, underscoring the need for explicit rule documentation. This work serves language modeling, speech synthesis, and accessibility applications targeting Tigrinya-speaking communities.",
      "publishedDate": "2026-01-06T20:45:54Z",
      "updatedDate": "2026-01-06T20:45:54Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03403v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03403",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation"
      ],
      "tags": {
        "auto": [
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03385",
      "title": "SIGMA: Scalable Spectral Insights for LLM Collapse",
      "authors": [
        {
          "name": "Yi Gu",
          "affiliation": null
        },
        {
          "name": "Lingyou Pang",
          "affiliation": null
        },
        {
          "name": "Xiangkun Ye",
          "affiliation": null
        },
        {
          "name": "Tianyu Wang",
          "affiliation": null
        },
        {
          "name": "Jianyu Lin",
          "affiliation": null
        },
        {
          "name": "Carey E. Priebe",
          "affiliation": null
        },
        {
          "name": "Alexander Aue",
          "affiliation": null
        }
      ],
      "abstract": "The rapid adoption of synthetic data for training Large Language Models (LLMs) has introduced the technical challenge of \"model collapse\"-a degenerative process where recursive training on model-generated content leads to a contraction of distributional variance and representational quality. While the phenomenology of collapse is increasingly evident, rigorous methods to quantify and predict its onset in high-dimensional spaces remain elusive. In this paper, we introduce SIGMA (Spectral Inequalities for Gram Matrix Analysis), a unified framework that benchmarks model collapse through the spectral lens of the embedding Gram matrix. By deriving and utilizing deterministic and stochastic bounds on the matrix's spectrum, SIGMA provides a mathematically grounded metric to track the contraction of the representation space. Crucially, our stochastic formulation enables scalable estimation of these bounds, making the framework applicable to large-scale foundation models where full eigendecomposition is intractable. We demonstrate that SIGMA effectively captures the transition towards degenerate states, offering both theoretical insights into the mechanics of collapse and a practical, scalable tool for monitoring the health of recursive training pipelines.",
      "publishedDate": "2026-01-06T19:47:11Z",
      "updatedDate": "2026-01-06T19:47:11Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "math.PR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03385v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03385",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03785",
      "title": "Membox: Weaving Topic Continuity into Long-Range Memory for LLM Agents",
      "authors": [
        {
          "name": "Dehao Tao",
          "affiliation": null
        },
        {
          "name": "Guoliang Ma",
          "affiliation": null
        },
        {
          "name": "Yongfeng Huang",
          "affiliation": null
        },
        {
          "name": "Minghu Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Human-agent dialogues often exhibit topic continuity-a stable thematic frame that evolves through temporally adjacent exchanges-yet most large language model (LLM) agent memory systems fail to preserve it. Existing designs follow a fragmentation-compensation paradigm: they first break dialogue streams into isolated utterances for storage, then attempt to restore coherence via embedding-based retrieval. This process irreversibly damages narrative and causal flow, while biasing retrieval towards lexical similarity. We introduce membox, a hierarchical memory architecture centered on a Topic Loom that continuously monitors dialogue in a sliding-window fashion, grouping consecutive same-topic turns into coherent \"memory boxes\" at storage time. Sealed boxes are then linked by a Trace Weaver into long-range event-timeline traces, recovering macro-topic recurrences across discontinuities. Experiments on LoCoMo demonstrate that Membox achieves up to 68% F1 improvement on temporal reasoning tasks, outperforming competitive baselines (e.g., Mem0, A-MEM). Notably, Membox attains these gains while using only a fraction of the context tokens required by existing methods, highlighting a superior balance between efficiency and effectiveness. By explicitly modeling topic continuity, Membox offers a cognitively motivated mechanism for enhancing both coherence and efficiency in LLM agents.",
      "publishedDate": "2026-01-07T10:36:29Z",
      "updatedDate": "2026-01-07T10:36:29Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03785v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03785",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "rag",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03768",
      "title": "Agentic Proof Automation: A Case Study",
      "authors": [
        {
          "name": "Yichen Xu",
          "affiliation": null
        },
        {
          "name": "Martin Odersky",
          "affiliation": null
        }
      ],
      "abstract": "Proof engineering is notoriously labor-intensive: proofs that are straightforward on paper often require lengthy scripts in theorem provers. Recent advances in large language models (LLMs) create new opportunities for proof automation: modern LLMs not only generate proof scripts, but also support agentic behavior, exploring codebases and iteratively refining their outputs against prover feedback. These advances enable an emerging scheme where LLM-based agents undertake most proof engineering under human guidance. Humans provide mathematical insight (definitions, theorems, proof strategies); agents handle the mechanical work of proof development. We call this scheme agentic proof automation. We present this scheme through a case study: mechanizing the semantic type soundness of a sophisticated formal system, System Capless, in Lean 4, comprising over 14,000 lines of code. Using off-the-shelf LLM agents with a single lightweight proof-checking tool, the agents completed 189 proof engineering tasks with an 87% success rate, only 16% requiring human intervention. The case study demonstrates that agents are capable proof engineers that substantially boost productivity, though they fall short in creative reasoning and still require human guidance in certain cases. We release an interactive explorer where readers can examine all agent interactions; the mechanization is open-sourced for experiments and extensions.",
      "publishedDate": "2026-01-07T10:02:17Z",
      "updatedDate": "2026-01-07T10:02:17Z",
      "primaryCategory": "cs.PL",
      "arxivCategories": [
        "cs.PL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03768v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03768",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03315",
      "title": "Why LLMs Aren't Scientists Yet: Lessons from Four Autonomous Research Attempts",
      "authors": [
        {
          "name": "Dhruv Trehan",
          "affiliation": null
        },
        {
          "name": "Paras Chopra",
          "affiliation": null
        }
      ],
      "abstract": "We report a case study of four end-to-end attempts to autonomously generate ML research papers using a pipeline of six LLM agents mapped to stages of the scientific workflow. Of these four, three attempts failed during implementation or evaluation. One completed the pipeline and was accepted to Agents4Science 2025, an experimental inaugural venue that required AI systems as first authors, passing both human and multi-AI review. From these attempts, we document six recurring failure modes: bias toward training data defaults, implementation drift under execution pressure, memory and context degradation across long-horizon tasks, overexcitement that declares success despite obvious failures, insufficient domain intelligence, and weak scientific taste in experimental design. We conclude by discussing four design principles for more robust AI-scientist systems, implications for autonomous scientific discovery, and we release all prompts, artifacts, and outputs at https://github.com/Lossfunk/ai-scientist-artefacts-v1",
      "publishedDate": "2026-01-06T13:20:54Z",
      "updatedDate": "2026-01-06T13:20:54Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03315v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03315",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03540",
      "title": "DeepSynth-Eval: Objectively Evaluating Information Consolidation in Deep Survey Writing",
      "authors": [
        {
          "name": "Hongzhi Zhang",
          "affiliation": null
        },
        {
          "name": "Yuanze Hu",
          "affiliation": null
        },
        {
          "name": "Tinghai Zhang",
          "affiliation": null
        },
        {
          "name": "Jia Fu",
          "affiliation": null
        },
        {
          "name": "Tao Wang",
          "affiliation": null
        },
        {
          "name": "Junwei Jing",
          "affiliation": null
        },
        {
          "name": "Zhaoxin Fan",
          "affiliation": null
        },
        {
          "name": "Qi Wang",
          "affiliation": null
        },
        {
          "name": "Ruiming Tang",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Guorui Zhou",
          "affiliation": null
        },
        {
          "name": "Kun Gai",
          "affiliation": null
        }
      ],
      "abstract": "The evolution of Large Language Models (LLMs) towards autonomous agents has catalyzed progress in Deep Research. While retrieval capabilities are well-benchmarked, the post-retrieval synthesis stage--where agents must digest massive amounts of context and consolidate fragmented evidence into coherent, long-form reports--remains under-evaluated due to the subjectivity of open-ended writing. To bridge this gap, we introduce DeepSynth-Eval, a benchmark designed to objectively evaluate information consolidation capabilities. We leverage high-quality survey papers as gold standards, reverse-engineering research requests and constructing \"Oracle Contexts\" from their bibliographies to isolate synthesis from retrieval noise. We propose a fine-grained evaluation protocol using General Checklists (for factual coverage) and Constraint Checklists (for structural organization), transforming subjective judgment into verifiable metrics. Experiments across 96 tasks reveal that synthesizing information from hundreds of references remains a significant challenge. Our results demonstrate that agentic plan-and-write workflows significantly outperform single-turn generation, effectively reducing hallucinations and improving adherence to complex structural constraints.",
      "publishedDate": "2026-01-07T03:07:52Z",
      "updatedDate": "2026-01-07T03:07:52Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03540v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03540",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04170",
      "title": "Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions",
      "authors": [
        {
          "name": "Abhishek Rath",
          "affiliation": null
        }
      ],
      "abstract": "Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies). We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements. We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.",
      "publishedDate": "2026-01-07T18:37:26Z",
      "updatedDate": "2026-01-07T18:37:26Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04170v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04170",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "multi-agent",
        "agents",
        "tool-use",
        "reasoning",
        "planning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "multi-agent",
          "agents",
          "tool-use",
          "reasoning",
          "planning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04160",
      "title": "All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection",
      "authors": [
        {
          "name": "Yuechen Jiang",
          "affiliation": null
        },
        {
          "name": "Zhiwei Liu",
          "affiliation": null
        },
        {
          "name": "Yupeng Cao",
          "affiliation": null
        },
        {
          "name": "Yueru He",
          "affiliation": null
        },
        {
          "name": "Ziyang Xu",
          "affiliation": null
        },
        {
          "name": "Chen Xu",
          "affiliation": null
        },
        {
          "name": "Zhiyang Deng",
          "affiliation": null
        },
        {
          "name": "Prayag Tiwari",
          "affiliation": null
        },
        {
          "name": "Xi Chen",
          "affiliation": null
        },
        {
          "name": "Alejandro Lopez-Lira",
          "affiliation": null
        },
        {
          "name": "Jimin Huang",
          "affiliation": null
        },
        {
          "name": "Junichi Tsujii",
          "affiliation": null
        },
        {
          "name": "Sophia Ananiadou",
          "affiliation": null
        }
      ],
      "abstract": "We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.",
      "publishedDate": "2026-01-07T18:18:28Z",
      "updatedDate": "2026-01-07T18:18:28Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CE",
        "q-fin.CP"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04160v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04160",
      "comment": "39 pages; 24 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04157",
      "title": "FLEx: Language Modeling with Few-shot Language Explanations",
      "authors": [
        {
          "name": "Adar Avsian",
          "affiliation": null
        },
        {
          "name": "Christopher Richardson",
          "affiliation": null
        },
        {
          "name": "Anirudh Sundar",
          "affiliation": null
        },
        {
          "name": "Larry Heck",
          "affiliation": null
        }
      ],
      "abstract": "Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\\textbf{F}$ew-shot $\\textbf{L}$anguage $\\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\\% of CoT's remaining errors.",
      "publishedDate": "2026-01-07T18:12:05Z",
      "updatedDate": "2026-01-07T18:12:05Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04157v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04157",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04118",
      "title": "GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning",
      "authors": [
        {
          "name": "Wenshuai Li",
          "affiliation": null
        },
        {
          "name": "Xiantai Xiang",
          "affiliation": null
        },
        {
          "name": "Zixiao Wen",
          "affiliation": null
        },
        {
          "name": "Guangyao Zhou",
          "affiliation": null
        },
        {
          "name": "Ben Niu",
          "affiliation": null
        },
        {
          "name": "Feng Wang",
          "affiliation": null
        },
        {
          "name": "Lijia Huang",
          "affiliation": null
        },
        {
          "name": "Qiantong Wang",
          "affiliation": null
        },
        {
          "name": "Yuxin Hu",
          "affiliation": null
        }
      ],
      "abstract": "The evolution of Remote Sensing Vision-Language Models(RS-VLMs) emphasizes the importance of transitioning from perception-centric recognition toward high-level deductive reasoning to enhance cognitive reliability in complex spatial tasks. However, current models often suffer from logical hallucinations, where correct answers are derived from flawed reasoning chains or rely on positional shortcuts rather than spatial logic. This decoupling undermines reliability in strategic spatial decision-making. To address this, we present GeoReason, a framework designed to synchronize internal thinking with final decisions. We first construct GeoReason-Bench, a logic-driven dataset containing 4,000 reasoning trajectories synthesized from geometric primitives and expert knowledge. We then formulate a two-stage training strategy: (1) Supervised Knowledge Initialization to equip the model with reasoning syntax and domain expertise, and (2) Consistency-Aware Reinforcement Learning to refine deductive reliability. This second stage integrates a novel Logical Consistency Reward, which penalizes logical drift via an option permutation strategy to anchor decisions in verifiable reasoning traces. Experimental results demonstrate that our framework significantly enhances the cognitive reliability and interpretability of RS-VLMs, achieving state-of-the-art performance compared to other advanced methods.",
      "publishedDate": "2026-01-07T17:26:41Z",
      "updatedDate": "2026-01-07T17:26:41Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04118v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04118",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04086",
      "title": "KDCM: Reducing Hallucination in LLMs through Explicit Reasoning Structures",
      "authors": [
        {
          "name": "Jinbo Hao",
          "affiliation": null
        },
        {
          "name": "Kai Yang",
          "affiliation": null
        },
        {
          "name": "Qingzhen Su",
          "affiliation": null
        },
        {
          "name": "Yifan Li",
          "affiliation": null
        },
        {
          "name": "Chao Jiang",
          "affiliation": null
        }
      ],
      "abstract": "To mitigate hallucinations in large language models (LLMs), we propose a framework that focuses on errors induced by prompts. Our method extends a chain-style knowledge distillation approach by incorporating a programmable module that guides knowledge graph exploration. This module is embedded as executable code within the reasoning prompt, allowing the model to leverage external structured knowledge during inference. Based on this design, we develop an enhanced distillation-based reasoning framework that explicitly regulates intermediate reasoning steps, resulting in more reliable predictions. We evaluate the proposed approach on multiple public benchmarks using GPT-4 and LLaMA-3.3. Experimental results show that code-guided reasoning significantly improves contextual modeling and reduces prompt-induced hallucinations. Specifically, HIT@1, HIT@3, and HIT@5 increase by 15.64%, 13.38%, and 13.28%, respectively, with scores exceeding 95% across several evaluation settings. These findings indicate that the proposed method effectively constrains erroneous reasoning while improving both accuracy and interpretability.",
      "publishedDate": "2026-01-07T16:54:20Z",
      "updatedDate": "2026-01-07T16:54:20Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04086v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04086",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04055",
      "title": "Modular Prompt Optimization: Optimizing Structured Prompts with Section-Local Textual Gradients",
      "authors": [
        {
          "name": "Prith Sharma",
          "affiliation": null
        },
        {
          "name": "Austin Z. Henley",
          "affiliation": null
        }
      ],
      "abstract": "Prompt quality plays a central role in controlling the behavior, reliability, and reasoning performance of large language models (LLMs), particularly for smaller open-source instruction-tuned models that depend heavily on explicit structure. While recent work has explored automatic prompt optimization using textual gradients and self-refinement, most existing methods treat prompts as monolithic blocks of text, making it difficult to localize errors, preserve critical instructions, or prevent uncontrolled prompt growth. We introduce Modular Prompt Optimization (MPO), a schema-based prompt optimization framework that treats prompts as structured objects composed of fixed semantic sections, including system role, context, task description, constraints, and output format. MPO applies section-local textual gradients, generated by a critic language model, to refine each section independently while keeping the overall prompt schema fixed. Section updates are consolidated through de-duplication to reduce redundancy and interference between components, yielding an interpretable and robust optimization process. We evaluate MPO on two reasoning benchmarks, ARC-Challenge and MMLU, using LLaMA-3 8B-Instruct and Mistral-7B-Instruct as solver models. Across both benchmarks and models, MPO consistently outperforms an untuned structured prompt and the TextGrad baseline, achieving substantial accuracy gains without modifying model parameters or altering prompt structure. These results demonstrate that maintaining a fixed prompt schema while applying localized, section-wise optimization is an effective and practical approach for improving reasoning performance in small open-source LMs.",
      "publishedDate": "2026-01-07T16:20:08Z",
      "updatedDate": "2026-01-07T16:20:08Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04055v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04055",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04043",
      "title": "When Helpers Become Hazards: A Benchmark for Analyzing Multimodal LLM-Powered Safety in Daily Life",
      "authors": [
        {
          "name": "Xinyue Lou",
          "affiliation": null
        },
        {
          "name": "Jinan Xu",
          "affiliation": null
        },
        {
          "name": "Jingyi Yin",
          "affiliation": null
        },
        {
          "name": "Xiaolong Wang",
          "affiliation": null
        },
        {
          "name": "Zhaolu Kang",
          "affiliation": null
        },
        {
          "name": "Youwei Liao",
          "affiliation": null
        },
        {
          "name": "Yixuan Wang",
          "affiliation": null
        },
        {
          "name": "Xiangyu Shi",
          "affiliation": null
        },
        {
          "name": "Fengran Mo",
          "affiliation": null
        },
        {
          "name": "Su Yao",
          "affiliation": null
        },
        {
          "name": "Kaiyu Huang",
          "affiliation": null
        }
      ],
      "abstract": "As Multimodal Large Language Models (MLLMs) become an indispensable assistant in human life, the unsafe content generated by MLLMs poses a danger to human behavior, perpetually overhanging human society like a sword of Damocles. To investigate and evaluate the safety impact of MLLMs responses on human behavior in daily life, we introduce SaLAD, a multimodal safety benchmark which contains 2,013 real-world image-text samples across 10 common categories, with a balanced design covering both unsafe scenarios and cases of oversensitivity. It emphasizes realistic risk exposure, authentic visual inputs, and fine-grained cross-modal reasoning, ensuring that safety risks cannot be inferred from text alone. We further propose a safety-warning-based evaluation framework that encourages models to provide clear and informative safety warnings, rather than generic refusals. Results on 18 MLLMs demonstrate that the top-performing models achieve a safe response rate of only 57.2% on unsafe queries. Moreover, even popular safety alignment methods limit effectiveness of the models in our scenario, revealing the vulnerabilities of current MLLMs in identifying dangerous behaviors in daily life. Our dataset is available at https://github.com/xinyuelou/SaLAD.",
      "publishedDate": "2026-01-07T15:59:07Z",
      "updatedDate": "2026-01-07T15:59:07Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04043v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04043",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03986",
      "title": "Benchmark^2: Systematic Evaluation of LLM Benchmarks",
      "authors": [
        {
          "name": "Qi Qian",
          "affiliation": null
        },
        {
          "name": "Chengsong Huang",
          "affiliation": null
        },
        {
          "name": "Jingwen Xu",
          "affiliation": null
        },
        {
          "name": "Changze Lv",
          "affiliation": null
        },
        {
          "name": "Muling Wu",
          "affiliation": null
        },
        {
          "name": "Wenhao Liu",
          "affiliation": null
        },
        {
          "name": "Xiaohua Wang",
          "affiliation": null
        },
        {
          "name": "Zhenghua Wang",
          "affiliation": null
        },
        {
          "name": "Zisu Huang",
          "affiliation": null
        },
        {
          "name": "Muzhao Tian",
          "affiliation": null
        },
        {
          "name": "Jianhan Xu",
          "affiliation": null
        },
        {
          "name": "Kun Hu",
          "affiliation": null
        },
        {
          "name": "He-Da Wang",
          "affiliation": null
        },
        {
          "name": "Yao Hu",
          "affiliation": null
        },
        {
          "name": "Xuanjing Huang",
          "affiliation": null
        },
        {
          "name": "Xiaoqing Zheng",
          "affiliation": null
        }
      ],
      "abstract": "The rapid proliferation of benchmarks for evaluating large language models (LLMs) has created an urgent need for systematic methods to assess benchmark quality itself. We propose Benchmark^2, a comprehensive framework comprising three complementary metrics: (1) Cross-Benchmark Ranking Consistency, measuring whether a benchmark produces model rankings aligned with peer benchmarks; (2) Discriminability Score, quantifying a benchmark's ability to differentiate between models; and (3) Capability Alignment Deviation, identifying problematic instances where stronger models fail but weaker models succeed within the same model family. We conduct extensive experiments across 15 benchmarks spanning mathematics, reasoning, and knowledge domains, evaluating 11 LLMs across four model families. Our analysis reveals significant quality variations among existing benchmarks and demonstrates that selective benchmark construction based on our metrics can achieve comparable evaluation performance with substantially reduced test sets.",
      "publishedDate": "2026-01-07T14:59:03Z",
      "updatedDate": "2026-01-07T14:59:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03986v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03986",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03956",
      "title": "CoINS: Counterfactual Interactive Navigation via Skill-Aware VLM",
      "authors": [
        {
          "name": "Kangjie Zhou",
          "affiliation": null
        },
        {
          "name": "Zhejia Wen",
          "affiliation": null
        },
        {
          "name": "Zhiyong Zhuo",
          "affiliation": null
        },
        {
          "name": "Zike Yan",
          "affiliation": null
        },
        {
          "name": "Pengying Wu",
          "affiliation": null
        },
        {
          "name": "Ieng Hou U",
          "affiliation": null
        },
        {
          "name": "Shuaiyang Li",
          "affiliation": null
        },
        {
          "name": "Han Gao",
          "affiliation": null
        },
        {
          "name": "Kang Ding",
          "affiliation": null
        },
        {
          "name": "Wenhan Cao",
          "affiliation": null
        },
        {
          "name": "Wei Pan",
          "affiliation": null
        },
        {
          "name": "Chang Liu",
          "affiliation": null
        }
      ],
      "abstract": "Recent Vision-Language Models (VLMs) have demonstrated significant potential in robotic planning. However, they typically function as semantic reasoners, lacking an intrinsic understanding of the specific robot's physical capabilities. This limitation is particularly critical in interactive navigation, where robots must actively modify cluttered environments to create traversable paths. Existing VLM-based navigators are predominantly confined to passive obstacle avoidance, failing to reason about when and how to interact with objects to clear blocked paths. To bridge this gap, we propose Counterfactual Interactive Navigation via Skill-aware VLM (CoINS), a hierarchical framework that integrates skill-aware reasoning and robust low-level execution. Specifically, we fine-tune a VLM, named InterNav-VLM, which incorporates skill affordance and concrete constraint parameters into the input context and grounds them into a metric-scale environmental representation. By internalizing the logic of counterfactual reasoning through fine-tuning on the proposed InterNav dataset, the model learns to implicitly evaluate the causal effects of object removal on navigation connectivity, thereby determining interaction necessity and target selection. To execute the generated high-level plans, we develop a comprehensive skill library through reinforcement learning, specifically introducing traversability-oriented strategies to manipulate diverse objects for path clearance. A systematic benchmark in Isaac Sim is proposed to evaluate both the reasoning and execution aspects of interactive navigation. Extensive simulations and real-world experiments demonstrate that CoINS significantly outperforms representative baselines, achieving a 17\\% higher overall success rate and over 80\\% improvement in complex long-horizon scenarios compared to the best-performing baseline",
      "publishedDate": "2026-01-07T14:10:46Z",
      "updatedDate": "2026-01-07T14:10:46Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03956v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03956",
      "comment": "17 pages, 13 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "planning",
        "robotics"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "planning",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03948",
      "title": "Trade-R1: Bridging Verifiable Rewards to Stochastic Environments via Process-Level Reasoning Verification",
      "authors": [
        {
          "name": "Rui Sun",
          "affiliation": null
        },
        {
          "name": "Yifan Sun",
          "affiliation": null
        },
        {
          "name": "Sheng Xu",
          "affiliation": null
        },
        {
          "name": "Li Zhao",
          "affiliation": null
        },
        {
          "name": "Jing Li",
          "affiliation": null
        },
        {
          "name": "Daxin Jiang",
          "affiliation": null
        },
        {
          "name": "Chen Hua",
          "affiliation": null
        },
        {
          "name": "Zuo Bai",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning (RL) has enabled Large Language Models (LLMs) to achieve remarkable reasoning in domains like mathematics and coding, where verifiable rewards provide clear signals. However, extending this paradigm to financial decision is challenged by the market's stochastic nature: rewards are verifiable but inherently noisy, causing standard RL to degenerate into reward hacking. To address this, we propose Trade-R1, a model training framework that bridges verifiable rewards to stochastic environments via process-level reasoning verification. Our key innovation is a verification method that transforms the problem of evaluating reasoning over lengthy financial documents into a structured Retrieval-Augmented Generation (RAG) task. We construct a triangular consistency metric, assessing pairwise alignment between retrieved evidence, reasoning chains, and decisions to serve as a validity filter for noisy market returns. We explore two reward integration strategies: Fixed-effect Semantic Reward (FSR) for stable alignment signals, and Dynamic-effect Semantic Reward (DSR) for coupled magnitude optimization. Experiments on different country asset selection demonstrate that our paradigm reduces reward hacking, with DSR achieving superior cross-market generalization while maintaining the highest reasoning consistency.",
      "publishedDate": "2026-01-07T14:03:22Z",
      "updatedDate": "2026-01-07T14:03:22Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "q-fin.TR"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03948v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03948",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03926",
      "title": "Doc-PP: Document Policy Preservation Benchmark for Large Vision-Language Models",
      "authors": [
        {
          "name": "Haeun Jang",
          "affiliation": null
        },
        {
          "name": "Hwan Chang",
          "affiliation": null
        },
        {
          "name": "Hwanhee Lee",
          "affiliation": null
        }
      ],
      "abstract": "The deployment of Large Vision-Language Models (LVLMs) for real-world document question answering is often constrained by dynamic, user-defined policies that dictate information disclosure based on context. While ensuring adherence to these explicit constraints is critical, existing safety research primarily focuses on implicit social norms or text-only settings, overlooking the complexities of multimodal documents. In this paper, we introduce Doc-PP (Document Policy Preservation Benchmark), a novel benchmark constructed from real-world reports requiring reasoning across heterogeneous visual and textual elements under strict non-disclosure policies. Our evaluation highlights a systemic Reasoning-Induced Safety Gap: models frequently leak sensitive information when answers must be inferred through complex synthesis or aggregated across modalities, effectively circumventing existing safety constraints. Furthermore, we identify that providing extracted text improves perception but inadvertently facilitates leakage. To address these vulnerabilities, we propose DVA (Decompose-Verify-Aggregation), a structural inference framework that decouples reasoning from policy verification. Experimental results demonstrate that DVA significantly outperforms standard prompting defenses, offering a robust baseline for policy-compliant document understanding",
      "publishedDate": "2026-01-07T13:45:39Z",
      "updatedDate": "2026-01-07T13:45:39Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03926v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03926",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03914",
      "title": "When Models Decide and When They Bind: A Two-Stage Computation for Multiple-Choice Question-Answering",
      "authors": [
        {
          "name": "Hugh Mee Wong",
          "affiliation": null
        },
        {
          "name": "Rick Nouwen",
          "affiliation": null
        },
        {
          "name": "Albert Gatt",
          "affiliation": null
        }
      ],
      "abstract": "Multiple-choice question answering (MCQA) is easy to evaluate but adds a meta-task: models must both solve the problem and output the symbol that *represents* the answer, conflating reasoning errors with symbol-binding failures. We study how language models implement MCQA internally using representational analyses (PCA, linear probes) as well as causal interventions. We find that option-boundary (newline) residual states often contain strong linearly decodable signals related to per-option correctness. Winner-identity probing reveals a two-stage progression: the winning *content position* becomes decodable immediately after the final option is processed, while the *output symbol* is represented closer to the answer emission position. Tests under symbol and content permutations support a two-stage mechanism in which models first select a winner in content space and then bind or route that winner to the appropriate symbol to emit.",
      "publishedDate": "2026-01-07T13:27:48Z",
      "updatedDate": "2026-01-07T13:27:48Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03914v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03914",
      "comment": "Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning"
      ],
      "tags": {
        "auto": [
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03905",
      "title": "Current Agents Fail to Leverage World Model as Tool for Foresight",
      "authors": [
        {
          "name": "Cheng Qian",
          "affiliation": null
        },
        {
          "name": "Emre Can Acikgoz",
          "affiliation": null
        },
        {
          "name": "Bingxuan Li",
          "affiliation": null
        },
        {
          "name": "Xiusi Chen",
          "affiliation": null
        },
        {
          "name": "Yuji Zhang",
          "affiliation": null
        },
        {
          "name": "Bingxiang He",
          "affiliation": null
        },
        {
          "name": "Qinyu Luo",
          "affiliation": null
        },
        {
          "name": "Dilek Hakkani-Tür",
          "affiliation": null
        },
        {
          "name": "Gokhan Tur",
          "affiliation": null
        },
        {
          "name": "Yunzhu Li",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        },
        {
          "name": "Heng Ji",
          "affiliation": null
        }
      ],
      "abstract": "Agents built on vision-language models increasingly face tasks that demand anticipating future states rather than relying on short-horizon reasoning. Generative world models offer a promising remedy: agents could use them as external simulators to foresee outcomes before acting. This paper empirically examines whether current agents can leverage such world models as tools to enhance their cognition. Across diverse agentic and visual question answering tasks, we observe that some agents rarely invoke simulation (fewer than 1%), frequently misuse predicted rollouts (approximately 15%), and often exhibit inconsistent or even degraded performance (up to 5%) when simulation is available or enforced. Attribution analysis further indicates that the primary bottleneck lies in the agents' capacity to decide when to simulate, how to interpret predicted outcomes, and how to integrate foresight into downstream reasoning. These findings underscore the need for mechanisms that foster calibrated, strategic interaction with world models, paving the way toward more reliable anticipatory cognition in future agent systems.",
      "publishedDate": "2026-01-07T13:15:23Z",
      "updatedDate": "2026-01-07T13:15:23Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03905v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03905",
      "comment": "36 Pages, 13 Figures, 17 Tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03895",
      "title": "Adaptive-Boundary-Clipping GRPO: Ensuring Bounded Ratios for Stable and Generalizable Training",
      "authors": [
        {
          "name": "Chi Liu",
          "affiliation": null
        },
        {
          "name": "Xin Chen",
          "affiliation": null
        }
      ],
      "abstract": "Group Relative Policy Optimization (GRPO) has emerged as a popular algorithm for reinforcement learning with large language models (LLMs). However, upon analyzing its clipping mechanism, we argue that it is suboptimal in certain scenarios. With appropriate modifications, GRPO can be significantly enhanced to improve both flexibility and generalization. To this end, we propose Adaptive-Boundary-Clipping GRPO (ABC-GRPO), an asymmetric and adaptive refinement of the original GRPO framework. We demonstrate that ABC-GRPO achieves superior performance over standard GRPO on mathematical reasoning tasks using the Qwen3 LLMs. Moreover, ABC-GRPO maintains substantially higher entropy throughout training, thereby preserving the model's exploration capacity and mitigating premature convergence. The implementation code is available online to ease reproducibility https://github.com/chi2liu/ABC-GRPO.",
      "publishedDate": "2026-01-07T13:04:52Z",
      "updatedDate": "2026-01-07T13:04:52Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03895v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03895",
      "comment": "10 pages, 4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03878",
      "title": "Understanding Specification-Driven Code Generation with LLMs: An Empirical Study Design",
      "authors": [
        {
          "name": "Giovanni Rosa",
          "affiliation": null
        },
        {
          "name": "David Moreno-Lumbreras",
          "affiliation": null
        },
        {
          "name": "Gregorio Robles",
          "affiliation": null
        },
        {
          "name": "Jesús M. González-Barahona",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly integrated into software development workflows, yet their behavior in structured, specification-driven processes remains poorly understood. This paper presents an empirical study design using CURRANTE, a Visual Studio Code extension that enables a human-in-the-loop workflow for LLM-assisted code generation. The tool guides developers through three sequential stages--Specification, Tests, and Function--allowing them to define requirements, generate and refine test suites, and produce functions that satisfy those tests. Participants will solve medium-difficulty problems from the LiveCodeBench dataset, while the tool records fine-grained interaction logs, effectiveness metrics (e.g., pass rate, all-pass completion), efficiency indicators (e.g., time-to-pass), and iteration behaviors. The study aims to analyze how human intervention in specification and test refinement influences the quality and dynamics of LLM-generated code. The results will provide empirical insights into the design of next-generation development environments that align human reasoning with model-driven code generation.",
      "publishedDate": "2026-01-07T12:46:57Z",
      "updatedDate": "2026-01-07T12:46:57Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03878v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03878",
      "comment": "This paper is a Stage 1 Registered Report. The study protocol and analysis plan were peer reviewed and accepted at SANER 2026 with a Continuity Acceptance (CA) score for Stage 2",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03872",
      "title": "Atlas: Orchestrating Heterogeneous Models and Tools for Multi-Domain Complex Reasoning",
      "authors": [
        {
          "name": "Jinyang Wu",
          "affiliation": null
        },
        {
          "name": "Guocheng Zhai",
          "affiliation": null
        },
        {
          "name": "Ruihan Jin",
          "affiliation": null
        },
        {
          "name": "Jiahao Yuan",
          "affiliation": null
        },
        {
          "name": "Yuhao Shen",
          "affiliation": null
        },
        {
          "name": "Shuai Zhang",
          "affiliation": null
        },
        {
          "name": "Zhengqi Wen",
          "affiliation": null
        },
        {
          "name": "Jianhua Tao",
          "affiliation": null
        }
      ],
      "abstract": "The integration of large language models (LLMs) with external tools has significantly expanded the capabilities of AI agents. However, as the diversity of both LLMs and tools increases, selecting the optimal model-tool combination becomes a high-dimensional optimization challenge. Existing approaches often rely on a single model or fixed tool-calling logic, failing to exploit the performance variations across heterogeneous model-tool pairs. In this paper, we present ATLAS (Adaptive Tool-LLM Alignment and Synergistic Invocation), a dual-path framework for dynamic tool usage in cross-domain complex reasoning. ATLAS operates via a dual-path approach: (1) \\textbf{training-free cluster-based routing} that exploits empirical priors for domain-specific alignment, and (2) \\textbf{RL-based multi-step routing} that explores autonomous trajectories for out-of-distribution generalization. Extensive experiments across 15 benchmarks demonstrate that our method outperforms closed-source models like GPT-4o, surpassing existing routing methods on both in-distribution (+10.1%) and out-of-distribution (+13.1%) tasks. Furthermore, our framework shows significant gains in visual reasoning by orchestrating specialized multi-modal tools.",
      "publishedDate": "2026-01-07T12:38:33Z",
      "updatedDate": "2026-01-07T12:38:33Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03872v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03872",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03823",
      "title": "Step Potential Advantage Estimation: Harnessing Intermediate Confidence and Correctness for Efficient Mathematical Reasoning",
      "authors": [
        {
          "name": "Fei Wu",
          "affiliation": null
        },
        {
          "name": "Zhenrong Zhang",
          "affiliation": null
        },
        {
          "name": "Qikai Chang",
          "affiliation": null
        },
        {
          "name": "Jianshu Zhang",
          "affiliation": null
        },
        {
          "name": "Quan Liu",
          "affiliation": null
        },
        {
          "name": "Jun Du",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) elicits long chain-of-thought reasoning in large language models (LLMs), but outcome-based rewards lead to coarse-grained advantage estimation. While existing approaches improve RLVR via token-level entropy or sequence-level length control, they lack a semantically grounded, step-level measure of reasoning progress. As a result, LLMs fail to distinguish necessary deduction from redundant verification: they may continue checking after reaching a correct solution and, in extreme cases, overturn a correct trajectory into an incorrect final answer. To remedy the lack of process supervision, we introduce a training-free probing mechanism that extracts intermediate confidence and correctness and combines them into a Step Potential signal that explicitly estimates the reasoning state at each step. Building on this signal, we propose Step Potential Advantage Estimation (SPAE), a fine-grained credit assignment method that amplifies potential gains, penalizes potential drops, and applies penalty after potential saturates to encourage timely termination. Experiments across multiple benchmarks show SPAE consistently improves accuracy while substantially reducing response length, outperforming strong RL baselines and recent efficient reasoning and token-level advantage estimation methods. The code is available at https://github.com/cii030/SPAE-RL.",
      "publishedDate": "2026-01-07T11:36:01Z",
      "updatedDate": "2026-01-07T11:36:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03823v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03823",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03822",
      "title": "ROI-Reasoning: Rational Optimization for Inference via Pre-Computation Meta-Cognition",
      "authors": [
        {
          "name": "Muyang Zhao",
          "affiliation": null
        },
        {
          "name": "Qi Qi",
          "affiliation": null
        },
        {
          "name": "Hao Sun",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) can achieve strong reasoning performance with sufficient computation, but they do not inherently know how much computation a task requires. We study budgeted inference-time reasoning for multiple tasks under a strict global token constraint and formalize it as a Ordered Stochastic Multiple-Choice Knapsack Problem(OS-MCKP). This perspective highlights a meta-cognitive requirement -- anticipating task difficulty, estimating return over investment (ROI), and allocating computation strategically. We propose ROI-Reasoning, a two-stage framework that endows LLMs with intrinsic, budget-aware rationality. In the first stage, Meta-Cognitive Fine-Tuning teaches models to predict reasoning cost and expected utility before generation, enabling explicit solve-or-skip decisions. Next, Rationality-Aware Reinforcement Learning optimizes sequential decision making under a hard token budget, allowing models to learn long-horizon allocation strategies. Across budgeted mathematical reasoning benchmarks, ROI-Reasoning consistently improves overall score while substantially reducing regret under tight computation budgets.",
      "publishedDate": "2026-01-07T11:30:55Z",
      "updatedDate": "2026-01-07T11:30:55Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03822v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03822",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03812",
      "title": "AI Generated Text Detection",
      "authors": [
        {
          "name": "Adilkhan Alikhanov",
          "affiliation": null
        },
        {
          "name": "Aidar Amangeldi",
          "affiliation": null
        },
        {
          "name": "Diar Demeubay",
          "affiliation": null
        },
        {
          "name": "Dilnaz Akhmetzhan",
          "affiliation": null
        },
        {
          "name": "Nurbek Moldakhmetov",
          "affiliation": null
        },
        {
          "name": "Omar Polat",
          "affiliation": null
        },
        {
          "name": "Galymzhan Zharas",
          "affiliation": null
        }
      ],
      "abstract": "The rapid development of large language models has led to an increase in AI-generated text, with students increasingly using LLM-generated content as their own work, which violates academic integrity. This paper presents an evaluation of AI text detection methods, including both traditional machine learning models and transformer-based architectures. We utilize two datasets, HC3 and DAIGT v2, to build a unified benchmark and apply a topic-based data split to prevent information leakage. This approach ensures robust generalization across unseen domains. Our experiments show that TF-IDF logistic regression achieves a reasonable baseline accuracy of 82.87%. However, deep learning models outperform it. The BiLSTM classifier achieves an accuracy of 88.86%, while DistilBERT achieves a similar accuracy of 88.11% with the highest ROC-AUC score of 0.96, demonstrating the strongest overall performance. The results indicate that contextual semantic modeling is significantly superior to lexical features and highlight the importance of mitigating topic memorization through appropriate evaluation protocols. The limitations of this work are primarily related to dataset diversity and computational constraints. In future work, we plan to expand dataset diversity and utilize parameter-efficient fine-tuning methods such as LoRA. We also plan to explore smaller or distilled models and employ more efficient batching strategies and hardware-aware optimization.",
      "publishedDate": "2026-01-07T11:18:10Z",
      "updatedDate": "2026-01-07T11:18:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03812v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03812",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03808",
      "title": "From Brute Force to Semantic Insight: Performance-Guided Data Transformation Design with LLMs",
      "authors": [
        {
          "name": "Usha Shrestha",
          "affiliation": null
        },
        {
          "name": "Dmitry Ignatov",
          "affiliation": null
        },
        {
          "name": "Radu Timofte",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) have achieved notable performance in code synthesis; however, data-aware augmentation remains a limiting factor, handled via heuristic design or brute-force approaches. We introduce a performance-aware, closed-loop solution in the NNGPT ecosystem of projects that enables LLMs to autonomously engineer optimal transformations by internalizing empirical performance cues. We fine-tune LLMs with Low-Rank Adaptation on a novel repository of more than 6,000 empirically evaluated PyTorch augmentation functions, each annotated solely by downstream model accuracy. Training uses pairwise performance ordering (better-worse transformations), enabling alignment through empirical feedback without reinforcement learning, reward models, or symbolic objectives. This reduces the need for exhaustive search, achieving up to 600x times fewer evaluated candidates than brute-force discovery while maintaining competitive peak accuracy and shifting generation from random synthesis to task-aligned design. Ablation studies show that structured Chain-of-Thought prompting introduces syntactic noise and degrades performance, whereas direct prompting ensures stable optimization in performance-critical code tasks. Qualitative and quantitative analyses demonstrate that the model internalizes semantic performance cues rather than memorizing syntax. These results show that LLMs can exhibit task-level reasoning through non-textual feedback loops, bypassing explicit symbolic rewards.",
      "publishedDate": "2026-01-07T11:13:02Z",
      "updatedDate": "2026-01-07T11:13:02Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03808v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03808",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "code-generation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "prompting",
          "code-generation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03792",
      "title": "VietMed-MCQ: A Consistency-Filtered Data Synthesis Framework for Vietnamese Traditional Medicine Evaluation",
      "authors": [
        {
          "name": "Huynh Trung Kiet",
          "affiliation": null
        },
        {
          "name": "Dao Sy Duy Minh",
          "affiliation": null
        },
        {
          "name": "Nguyen Dinh Ha Duong",
          "affiliation": null
        },
        {
          "name": "Le Hoang Minh Huy",
          "affiliation": null
        },
        {
          "name": "Long Nguyen",
          "affiliation": null
        },
        {
          "name": "Dien Dinh",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in general medical domains. However, their performance significantly degrades in specialized, culturally specific domains such as Vietnamese Traditional Medicine (VTM), primarily due to the scarcity of high-quality, structured benchmarks. In this paper, we introduce VietMed-MCQ, a novel multiple-choice question dataset generated via a Retrieval-Augmented Generation (RAG) pipeline with an automated consistency check mechanism. Unlike previous synthetic datasets, our framework incorporates a dual-model validation approach to ensure reasoning consistency through independent answer verification, though the substring-based evidence checking has known limitations. The complete dataset of 3,190 questions spans three difficulty levels and underwent validation by one medical expert and four students, achieving 94.2 percent approval with substantial inter-rater agreement (Fleiss' kappa = 0.82). We benchmark seven open-source models on VietMed-MCQ. Results reveal that general-purpose models with strong Chinese priors outperform Vietnamese-centric models, highlighting cross-lingual conceptual transfer, while all models still struggle with complex diagnostic reasoning. Our code and dataset are publicly available to foster research in low-resource medical domains.",
      "publishedDate": "2026-01-07T10:49:56Z",
      "updatedDate": "2026-01-07T10:49:56Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03792v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03792",
      "comment": "11 pages, 4 figures. Dataset and code released",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "evaluation",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "rag",
          "evaluation",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03783",
      "title": "HearSay Benchmark: Do Audio LLMs Leak What They Hear?",
      "authors": [
        {
          "name": "Jin Wang",
          "affiliation": null
        },
        {
          "name": "Liang Lin",
          "affiliation": null
        },
        {
          "name": "Kaiwen Luo",
          "affiliation": null
        },
        {
          "name": "Weiliu Wang",
          "affiliation": null
        },
        {
          "name": "Yitian Chen",
          "affiliation": null
        },
        {
          "name": "Moayad Aloqaily",
          "affiliation": null
        },
        {
          "name": "Xuehai Tang",
          "affiliation": null
        },
        {
          "name": "Zhenhong Zhou",
          "affiliation": null
        },
        {
          "name": "Kun Wang",
          "affiliation": null
        },
        {
          "name": "Li Sun",
          "affiliation": null
        },
        {
          "name": "Qingsong Wen",
          "affiliation": null
        }
      ],
      "abstract": "While Audio Large Language Models (ALLMs) have achieved remarkable progress in understanding and generation, their potential privacy implications remain largely unexplored. This paper takes the first step to investigate whether ALLMs inadvertently leak user privacy solely through acoustic voiceprints and introduces $\\textit{HearSay}$, a comprehensive benchmark constructed from over 22,000 real-world audio clips. To ensure data quality, the benchmark is meticulously curated through a rigorous pipeline involving automated profiling and human verification, guaranteeing that all privacy labels are grounded in factual records. Extensive experiments on $\\textit{HearSay}$ yield three critical findings: $\\textbf{Significant Privacy Leakage}$: ALLMs inherently extract private attributes from voiceprints, reaching 92.89% accuracy on gender and effectively profiling social attributes. $\\textbf{Insufficient Safety Mechanisms}$: Alarmingly, existing safeguards are severely inadequate; most models fail to refuse privacy-intruding requests, exhibiting near-zero refusal rates for physiological traits. $\\textbf{Reasoning Amplifies Risk}$: Chain-of-Thought (CoT) reasoning exacerbates privacy risks in capable models by uncovering deeper acoustic correlations. These findings expose critical vulnerabilities in ALLMs, underscoring the urgent need for targeted privacy alignment. The codes and dataset are available at https://github.com/JinWang79/HearSay_Benchmark",
      "publishedDate": "2026-01-07T10:33:44Z",
      "updatedDate": "2026-01-07T10:33:44Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03783v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03783",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03781",
      "title": "MVP: Enhancing Video Large Language Models via Self-supervised Masked Video Prediction",
      "authors": [
        {
          "name": "Xiaokun Sun",
          "affiliation": null
        },
        {
          "name": "Zezhong Wu",
          "affiliation": null
        },
        {
          "name": "Zewen Ding",
          "affiliation": null
        },
        {
          "name": "Linli Xu",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning based post-training paradigms for Video Large Language Models (VideoLLMs) have achieved significant success by optimizing for visual-semantic tasks such as captioning or VideoQA. However, while these approaches effectively enhance perception abilities, they primarily target holistic content understanding, often lacking explicit supervision for intrinsic temporal coherence and inter-frame correlations. This tendency limits the models' ability to capture intricate dynamics and fine-grained visual causality. To explicitly bridge this gap, we propose a novel post-training objective: Masked Video Prediction (MVP). By requiring the model to reconstruct a masked continuous segment from a set of challenging distractors, MVP forces the model to attend to the sequential logic and temporal context of events. To support scalable training, we introduce a scalable data synthesis pipeline capable of transforming arbitrary video corpora into MVP training samples, and further employ Group Relative Policy Optimization (GRPO) with a fine-grained reward function to enhance the model's understanding of video context and temporal properties. Comprehensive evaluations demonstrate that MVP enhances video reasoning capabilities by directly reinforcing temporal reasoning and causal understanding.",
      "publishedDate": "2026-01-07T10:25:48Z",
      "updatedDate": "2026-01-07T10:25:48Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03781v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03781",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03775",
      "title": "Do LLM Self-Explanations Help Users Predict Model Behavior? Evaluating Counterfactual Simulatability with Pragmatic Perturbations",
      "authors": [
        {
          "name": "Pingjun Hong",
          "affiliation": null
        },
        {
          "name": "Benjamin Roth",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) can produce verbalized self-explanations, yet prior studies suggest that such rationales may not reliably reflect the model's true decision process. We ask whether these explanations nevertheless help users predict model behavior, operationalized as counterfactual simulatability. Using StrategyQA, we evaluate how well humans and LLM judges can predict a model's answers to counterfactual follow-up questions, with and without access to the model's chain-of-thought or post-hoc explanations. We compare LLM-generated counterfactuals with pragmatics-based perturbations as alternative ways to construct test cases for assessing the potential usefulness of explanations. Our results show that self-explanations consistently improve simulation accuracy for both LLM judges and humans, but the degree and stability of gains depend strongly on the perturbation strategy and judge strength. We also conduct a qualitative analysis of free-text justifications written by human users when predicting the model's behavior, which provides evidence that access to explanations helps humans form more accurate predictions on the perturbed questions.",
      "publishedDate": "2026-01-07T10:13:26Z",
      "updatedDate": "2026-01-07T10:13:26Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03775v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03775",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03769",
      "title": "EntroCoT: Enhancing Chain-of-Thought via Adaptive Entropy-Guided Segmentation",
      "authors": [
        {
          "name": "Zihang Li",
          "affiliation": null
        },
        {
          "name": "Yuhang Wang",
          "affiliation": null
        },
        {
          "name": "Yikun Zong",
          "affiliation": null
        },
        {
          "name": "Wenhan Yu",
          "affiliation": null
        },
        {
          "name": "Xiaokun Yuan",
          "affiliation": null
        },
        {
          "name": "Runhan Jiang",
          "affiliation": null
        },
        {
          "name": "Zirui Liu",
          "affiliation": null
        },
        {
          "name": "Tong Yang",
          "affiliation": null
        },
        {
          "name": "Arthur Jiang",
          "affiliation": null
        }
      ],
      "abstract": "Chain-of-Thought (CoT) prompting has significantly enhanced the mathematical reasoning capabilities of Large Language Models. We find existing fine-tuning datasets frequently suffer from the \"answer right but reasoning wrong\" probelm, where correct final answers are derived from hallucinated, redundant, or logically invalid intermediate steps. This paper proposes EntroCoT, a unified framework for automatically identifying and refining low-quality CoT supervision traces. EntroCoT first proposes an entropy-based mechanism to segment the reasoning trace into multiple steps at uncertain junctures, and then introduces a Monte Carlo rollout-based mechanism to evaluate the marginal contribution of each step. By accurately filtering deceptive reasoning samples, EntroCoT constructs a high-quality dataset where every intermediate step in each reasoning trace facilitates the final answer. Extensive experiments on mathematical benchmarks demonstrate that fine-tuning on the subset constructed by EntroCoT consistently outperforms the baseslines of full-dataset supervision.",
      "publishedDate": "2026-01-07T10:02:27Z",
      "updatedDate": "2026-01-07T10:02:27Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03769v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03769",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03743",
      "title": "O-Researcher: An Open Ended Deep Research Model via Multi-Agent Distillation and Agentic RL",
      "authors": [
        {
          "name": "Yi Yao",
          "affiliation": null
        },
        {
          "name": "He Zhu",
          "affiliation": null
        },
        {
          "name": "Piaohong Wang",
          "affiliation": null
        },
        {
          "name": "Jincheng Ren",
          "affiliation": null
        },
        {
          "name": "Xinlong Yang",
          "affiliation": null
        },
        {
          "name": "Qianben Chen",
          "affiliation": null
        },
        {
          "name": "Xiaowan Li",
          "affiliation": null
        },
        {
          "name": "Dingfeng Shi",
          "affiliation": null
        },
        {
          "name": "Jiaxian Li",
          "affiliation": null
        },
        {
          "name": "Qiexiang Wang",
          "affiliation": null
        },
        {
          "name": "Sinuo Wang",
          "affiliation": null
        },
        {
          "name": "Xinpeng Liu",
          "affiliation": null
        },
        {
          "name": "Jiaqi Wu",
          "affiliation": null
        },
        {
          "name": "Minghao Liu",
          "affiliation": null
        },
        {
          "name": "Wangchunshu Zhou",
          "affiliation": null
        }
      ],
      "abstract": "The performance gap between closed-source and open-source large language models (LLMs) is largely attributed to disparities in access to high-quality training data. To bridge this gap, we introduce a novel framework for the automated synthesis of sophisticated, research-grade instructional data. Our approach centers on a multi-agent workflow where collaborative AI agents simulate complex tool-integrated reasoning to generate diverse and high-fidelity data end-to-end. Leveraging this synthesized data, we develop a two-stage training strategy that integrates supervised fine-tuning with a novel reinforcement learning method, designed to maximize model alignment and capability. Extensive experiments demonstrate that our framework empowers open-source models across multiple scales, enabling them to achieve new state-of-the-art performance on the major deep research benchmark. This work provides a scalable and effective pathway for advancing open-source LLMs without relying on proprietary data or models.",
      "publishedDate": "2026-01-07T09:31:10Z",
      "updatedDate": "2026-01-07T09:31:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03743v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03743",
      "comment": "22 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "multi-agent",
        "prompting",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "multi-agent",
          "prompting",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03733",
      "title": "RadDiff: Describing Differences in Radiology Image Sets with Natural Language",
      "authors": [
        {
          "name": "Xiaoxian Shen",
          "affiliation": null
        },
        {
          "name": "Yuhui Zhang",
          "affiliation": null
        },
        {
          "name": "Sahithi Ankireddy",
          "affiliation": null
        },
        {
          "name": "Xiaohan Wang",
          "affiliation": null
        },
        {
          "name": "Maya Varma",
          "affiliation": null
        },
        {
          "name": "Henry Guo",
          "affiliation": null
        },
        {
          "name": "Curtis Langlotz",
          "affiliation": null
        },
        {
          "name": "Serena Yeung-Levy",
          "affiliation": null
        }
      ],
      "abstract": "Understanding how two radiology image sets differ is critical for generating clinical insights and for interpreting medical AI systems. We introduce RadDiff, a multimodal agentic system that performs radiologist-style comparative reasoning to describe clinically meaningful differences between paired radiology studies. RadDiff builds on a proposer-ranker framework from VisDiff, and incorporates four innovations inspired by real diagnostic workflows: (1) medical knowledge injection through domain-adapted vision-language models; (2) multimodal reasoning that integrates images with their clinical reports; (3) iterative hypothesis refinement across multiple reasoning rounds; and (4) targeted visual search that localizes and zooms in on salient regions to capture subtle findings. To evaluate RadDiff, we construct RadDiffBench, a challenging benchmark comprising 57 expert-validated radiology study pairs with ground-truth difference descriptions. On RadDiffBench, RadDiff achieves 47% accuracy, and 50% accuracy when guided by ground-truth reports, significantly outperforming the general-domain VisDiff baseline. We further demonstrate RadDiff's versatility across diverse clinical tasks, including COVID-19 phenotype comparison, racial subgroup analysis, and discovery of survival-related imaging features. Together, RadDiff and RadDiffBench provide the first method-and-benchmark foundation for systematically uncovering meaningful differences in radiological data.",
      "publishedDate": "2026-01-07T09:25:04Z",
      "updatedDate": "2026-01-07T09:25:04Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03733v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03733",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03728",
      "title": "CSMCIR: CoT-Enhanced Symmetric Alignment with Memory Bank for Composed Image Retrieval",
      "authors": [
        {
          "name": "Zhipeng Qian",
          "affiliation": null
        },
        {
          "name": "Zihan Liang",
          "affiliation": null
        },
        {
          "name": "Yufei Ma",
          "affiliation": null
        },
        {
          "name": "Ben Chen",
          "affiliation": null
        },
        {
          "name": "Huangyu Dai",
          "affiliation": null
        },
        {
          "name": "Yiwei Ma",
          "affiliation": null
        },
        {
          "name": "Jiayi Ji",
          "affiliation": null
        },
        {
          "name": "Chenyi Lei",
          "affiliation": null
        },
        {
          "name": "Han Li",
          "affiliation": null
        },
        {
          "name": "Xiaoshuai Sun",
          "affiliation": null
        }
      ],
      "abstract": "Composed Image Retrieval (CIR) enables users to search for target images using both a reference image and manipulation text, offering substantial advantages over single-modality retrieval systems. However, existing CIR methods suffer from representation space fragmentation: queries and targets comprise heterogeneous modalities and are processed by distinct encoders, forcing models to bridge misaligned representation spaces only through post-hoc alignment, which fundamentally limits retrieval performance. This architectural asymmetry manifests as three distinct, well-separated clusters in the feature space, directly demonstrating how heterogeneous modalities create fundamentally misaligned representation spaces from initialization. In this work, we propose CSMCIR, a unified representation framework that achieves efficient query-target alignment through three synergistic components. First, we introduce a Multi-level Chain-of-Thought (MCoT) prompting strategy that guides Multimodal Large Language Models to generate discriminative, semantically compatible captions for target images, establishing modal symmetry. Building upon this, we design a symmetric dual-tower architecture where both query and target sides utilize the identical shared-parameter Q-Former for cross-modal encoding, ensuring consistent feature representations and further reducing the alignment gap. Finally, this architectural symmetry enables an entropy-based, temporally dynamic Memory Bank strategy that provides high-quality negative samples while maintaining consistency with the evolving model state. Extensive experiments on four benchmark datasets demonstrate that our CSMCIR achieves state-of-the-art performance with superior training efficiency. Comprehensive ablation studies further validate the effectiveness of each proposed component.",
      "publishedDate": "2026-01-07T09:21:38Z",
      "updatedDate": "2026-01-07T09:21:38Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03728v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03728",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "prompting",
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "rag",
          "prompting",
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03725",
      "title": "EDCO: Dynamic Curriculum Orchestration for Domain-specific Large Language Model Fine-tuning",
      "authors": [
        {
          "name": "Jing-Cheng Pang",
          "affiliation": null
        },
        {
          "name": "Liu Sun",
          "affiliation": null
        },
        {
          "name": "Chang Zhou",
          "affiliation": null
        },
        {
          "name": "Xian Tang",
          "affiliation": null
        },
        {
          "name": "Haichuan Ma",
          "affiliation": null
        },
        {
          "name": "Kun Jiang",
          "affiliation": null
        },
        {
          "name": "Jianlong Wang",
          "affiliation": null
        },
        {
          "name": "Kai Zhang",
          "affiliation": null
        },
        {
          "name": "Sijie Wu",
          "affiliation": null
        },
        {
          "name": "Haoran Cai",
          "affiliation": null
        },
        {
          "name": "Chenwei Wu",
          "affiliation": null
        },
        {
          "name": "Xubin Li",
          "affiliation": null
        },
        {
          "name": "Xin Chen",
          "affiliation": null
        }
      ],
      "abstract": "Domain-specific large language models (LLMs), typically developed by fine-tuning a pre-trained general-purpose LLM on specialized datasets, represent a significant advancement in applied AI. A common strategy in LLM fine-tuning is curriculum learning, which pre-orders training samples based on metrics like difficulty to improve learning efficiency compared to a random sampling strategy. However, most existing methods for LLM fine-tuning rely on a static curriculum, designed prior to training, which lacks adaptability to the model's evolving needs during fine-tuning. To address this, we propose EDCO, a novel framework based on two key concepts: inference entropy and dynamic curriculum orchestration. Inspired by recent findings that maintaining high answer entropy benefits long-term reasoning gains, EDCO prioritizes samples with high inference entropy in a continuously adapted curriculum. EDCO integrates three core components: an efficient entropy estimator that uses prefix tokens to approximate full-sequence entropy, an entropy-based curriculum generator that selects data points with the highest inference entropy, and an LLM trainer that optimizes the model on the selected curriculum. Comprehensive experiments in communication, medicine and law domains, EDCO outperforms traditional curriculum strategies for fine-tuning Qwen3-4B and Llama3.2-3B models under supervised and reinforcement learning settings. Furthermore, the proposed efficient entropy estimation reduces computational time by 83.5% while maintaining high accuracy.",
      "publishedDate": "2026-01-07T09:20:05Z",
      "updatedDate": "2026-01-07T09:20:05Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03725v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03725",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03723",
      "title": "ETR: Outcome-Guided Elastic Trust Regions for Policy Optimization",
      "authors": [
        {
          "name": "Shijie Zhang",
          "affiliation": null
        },
        {
          "name": "Kevin Zhang",
          "affiliation": null
        },
        {
          "name": "Zheyuan Gu",
          "affiliation": null
        },
        {
          "name": "Xiang Guo",
          "affiliation": null
        },
        {
          "name": "Rujun Guo",
          "affiliation": null
        },
        {
          "name": "Shaoyu Liu",
          "affiliation": null
        },
        {
          "name": "Guanjun Jiang",
          "affiliation": null
        },
        {
          "name": "Xiaozhao Wang",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as an important paradigm for unlocking reasoning capabilities in large language models, exemplified by the success of OpenAI o1 and DeepSeek-R1. Currently, Group Relative Policy Optimization (GRPO) stands as the dominant algorithm in this domain due to its stable training and critic-free efficiency. However, we argue that GRPO suffers from a structural limitation: it imposes a uniform, static trust region constraint across all samples. This design implicitly assumes signal homogeneity, a premise misaligned with the heterogeneous nature of outcome-driven learning, where advantage magnitudes and variances fluctuate significantly. Consequently, static constraints fail to fully exploit high-quality signals while insufficiently suppressing noise, often precipitating rapid entropy collapse. To address this, we propose \\textbf{E}lastic \\textbf{T}rust \\textbf{R}egions (\\textbf{ETR}), a dynamic mechanism that aligns optimization constraints with signal quality. ETR constructs a signal-aware landscape through dual-level elasticity: at the micro level, it scales clipping boundaries based on advantage magnitude to accelerate learning from high-confidence paths; at the macro level, it leverages group variance to implicitly allocate larger update budgets to tasks in the optimal learning zone. Extensive experiments on AIME and MATH benchmarks demonstrate that ETR consistently outperforms GRPO, achieving superior accuracy while effectively mitigating policy entropy degradation to ensure sustained exploration.",
      "publishedDate": "2026-01-07T09:19:53Z",
      "updatedDate": "2026-01-07T09:19:53Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03723v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03723",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "tool-use",
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "tool-use",
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03717",
      "title": "MIND: From Passive Mimicry to Active Reasoning through Capability-Aware Multi-Perspective CoT Distillation",
      "authors": [
        {
          "name": "Jin Cui",
          "affiliation": null
        },
        {
          "name": "Jiaqi Guo",
          "affiliation": null
        },
        {
          "name": "Jiepeng Zhou",
          "affiliation": null
        },
        {
          "name": "Ruixuan Yang",
          "affiliation": null
        },
        {
          "name": "Jiayi Lu",
          "affiliation": null
        },
        {
          "name": "Jiajun Xu",
          "affiliation": null
        },
        {
          "name": "Jiangcheng Song",
          "affiliation": null
        },
        {
          "name": "Boran Zhao",
          "affiliation": null
        },
        {
          "name": "Pengju Ren",
          "affiliation": null
        }
      ],
      "abstract": "While Large Language Models (LLMs) have emerged with remarkable capabilities in complex tasks through Chain-of-Thought reasoning, practical resource constraints have sparked interest in transferring these abilities to smaller models. However, achieving both domain performance and cross-domain generalization remains challenging. Existing approaches typically restrict students to following a single golden rationale and treat different reasoning paths independently. Due to distinct inductive biases and intrinsic preferences, alongside the student's evolving capacity and reasoning preferences during training, a teacher's \"optimal\" rationale could act as out-of-distribution noise. This misalignment leads to a degeneration of the student's latent reasoning distribution, causing suboptimal performance. To bridge this gap, we propose MIND, a capability-adaptive framework that transitions distillation from passive mimicry to active cognitive construction. We synthesize diverse teacher perspectives through a novel \"Teaching Assistant\" network. By employing a Feedback-Driven Inertia Calibration mechanism, this network utilizes inertia-filtered training loss to align supervision with the student's current adaptability, effectively enhancing performance while mitigating catastrophic forgetting. Extensive experiments demonstrate that MIND achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks, and our sophisticated latent space analysis further confirms the mechanism of reasoning ability internalization.",
      "publishedDate": "2026-01-07T09:08:59Z",
      "updatedDate": "2026-01-07T09:08:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03717v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03717",
      "comment": "13 pages, 8 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03713",
      "title": "BREATH-VL: Vision-Language-Guided 6-DoF Bronchoscopy Localization via Semantic-Geometric Fusion",
      "authors": [
        {
          "name": "Qingyao Tian",
          "affiliation": null
        },
        {
          "name": "Bingyu Yang",
          "affiliation": null
        },
        {
          "name": "Huai Liao",
          "affiliation": null
        },
        {
          "name": "Xinyan Huang",
          "affiliation": null
        },
        {
          "name": "Junyong Li",
          "affiliation": null
        },
        {
          "name": "Dong Yi",
          "affiliation": null
        },
        {
          "name": "Hongbin Liu",
          "affiliation": null
        }
      ],
      "abstract": "Vision-language models (VLMs) have recently shown remarkable performance in navigation and localization tasks by leveraging large-scale pretraining for semantic understanding. However, applying VLMs to 6-DoF endoscopic camera localization presents several challenges: 1) the lack of large-scale, high-quality, densely annotated, and localization-oriented vision-language datasets in real-world medical settings; 2) limited capability for fine-grained pose regression; and 3) high computational latency when extracting temporal features from past frames. To address these issues, we first construct BREATH dataset, the largest in-vivo endoscopic localization dataset to date, collected in the complex human airway. Building on this dataset, we propose BREATH-VL, a hybrid framework that integrates semantic cues from VLMs with geometric information from vision-based registration methods for accurate 6-DoF pose estimation. Our motivation lies in the complementary strengths of both approaches: VLMs offer generalizable semantic understanding, while registration methods provide precise geometric alignment. To further enhance the VLM's ability to capture temporal context, we introduce a lightweight context-learning mechanism that encodes motion history as linguistic prompts, enabling efficient temporal reasoning without expensive video-level computation. Extensive experiments demonstrate that the vision-language module delivers robust semantic localization in challenging surgical scenes. Building on this, our BREATH-VL outperforms state-of-the-art vision-only localization methods in both accuracy and generalization, reducing translational error by 25.5% compared with the best-performing baseline, while achieving competitive computational latency.",
      "publishedDate": "2026-01-07T09:00:52Z",
      "updatedDate": "2026-01-07T09:00:52Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03713v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03713",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03703",
      "title": "TreeAdv: Tree-Structured Advantage Redistribution for Group-Based RL",
      "authors": [
        {
          "name": "Lang Cao",
          "affiliation": null
        },
        {
          "name": "Hui Ruan",
          "affiliation": null
        },
        {
          "name": "Yongqian Li",
          "affiliation": null
        },
        {
          "name": "Peng Chao",
          "affiliation": null
        },
        {
          "name": "Wu Ning",
          "affiliation": null
        },
        {
          "name": "Haonan Song",
          "affiliation": null
        },
        {
          "name": "Renhong Chen",
          "affiliation": null
        },
        {
          "name": "Yitong Li",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning with group-based objectives, such as Group Relative Policy Optimization (GRPO), is a common framework for aligning large language models on complex reasoning tasks. However, standard GRPO treats each rollout trajectory as an independent flat sequence and assigns a single sequence-level advantage to all tokens, which leads to sample inefficiency and a length bias toward verbose, redundant chains of thought without improving logical depth. We introduce TreeAdv (Tree-Structured Advantage Redistribution for Group-Based RL), which makes the tree structure of group rollouts explicit for both exploration and advantage assignment. Specifically, TreeAdv builds a group of trees (a forest) based on an entropy-driven sampling method where each tree branches at high-uncertainty decisions while sharing low-uncertainty tokens across rollouts. Then, TreeAdv aggregates token-level advantages for internal tree segments by redistributing the advantages of complete rollouts (all leaf nodes), and TreeAdv can easily apply to group-based objectives such as GRPO or GSPO. Across 10 math reasoning benchmarks, TreeAdv consistently outperforms GRPO and GSPO, while using substantially fewer generated tokens under identical supervision, data, and decoding budgets.",
      "publishedDate": "2026-01-07T08:42:14Z",
      "updatedDate": "2026-01-07T08:42:14Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03703v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03703",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03682",
      "title": "From Implicit to Explicit: Token-Efficient Logical Supervision for Mathematical Reasoning in LLMs",
      "authors": [
        {
          "name": "Shaojie Wang",
          "affiliation": null
        },
        {
          "name": "Liang Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Recent studies reveal that large language models (LLMs) exhibit limited logical reasoning abilities in mathematical problem-solving, instead often relying on pattern-matching and memorization. We systematically analyze this limitation, focusing on logical relationship understanding, which is a core capability underlying genuine logical reasoning, and reveal that errors related to this capability account for over 90\\% of incorrect predictions, with Chain-of-Thought Supervised Fine-Tuning (CoT-SFT) failing to substantially reduce these errors. To address this bottleneck, we propose First-Step Logical Reasoning (FSLR), a lightweight training framework targeting logical relationship understanding. Our key insight is that the first planning step-identifying which variables to use and which operation to apply-encourages the model to derive logical relationships directly from the problem statement. By training models on this isolated step, FSLR provides explicit supervision for logical relationship understanding, unlike CoT-SFT which implicitly embeds such relationships within complete solution trajectories. Extensive experiments across multiple models and datasets demonstrate that FSLR consistently outperforms CoT-SFT under both in-distribution and out-of-distribution settings, with average improvements of 3.2\\% and 4.6\\%, respectively. Moreover, FSLR achieves 4-6x faster training and reduces training token consumption by over 80\\%.",
      "publishedDate": "2026-01-07T08:15:01Z",
      "updatedDate": "2026-01-07T08:15:01Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03682v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03682",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "planning",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "planning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03661",
      "title": "AMIR-GRPO: Inducing Implicit Preference Signals into GRPO",
      "authors": [
        {
          "name": "Amir Hossein Yari",
          "affiliation": null
        },
        {
          "name": "Fajri Koto",
          "affiliation": null
        }
      ],
      "abstract": "Reinforcement learning has become the primary paradigm for aligning large language models (LLMs) on complex reasoning tasks, with group relative policy optimization (GRPO) widely used in large-scale post-training. However, GRPO faces structural limitations in reasoning-heavy settings: sequence-level advantage normalization introduces systematic length bias, penalties for low-quality trajectories are diluted, and the scalar objective discards rich pairwise preference information embedded in within-group reward rankings. As a result, valuable supervision from costly rollouts remains underutilized. We propose AMIR-GRPO, which augments GRPO with an implicit DPO-style contrastive regularizer constructed directly from intra-group reward rankings, requiring no additional annotations. This mechanism amplifies suppression of low-reward trajectories, attenuates response-level length bias, and transforms each rollout group into a denser set of supervision constraints. Across multiple mathematical reasoning benchmarks, AMIR-GRPO consistently outperforms strong GRPO baselines, yields clearer separation between correct and incorrect reasoning chains, and delivers broader coverage gains beyond the subset of instances solved by standard GRPO.",
      "publishedDate": "2026-01-07T07:22:58Z",
      "updatedDate": "2026-01-07T07:22:58Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03661v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03661",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03615",
      "title": "Analyzing Reasoning Shifts in Audio Deepfake Detection under Adversarial Attacks: The Reasoning Tax versus Shield Bifurcation",
      "authors": [
        {
          "name": "Binh Nguyen",
          "affiliation": null
        },
        {
          "name": "Thai Le",
          "affiliation": null
        }
      ],
      "abstract": "Audio Language Models (ALMs) offer a promising shift towards explainable audio deepfake detections (ADDs), moving beyond \\textit{black-box} classifiers by providing some level of transparency into their predictions via reasoning traces. This necessitates a new class of model robustness analysis: robustness of the predictive reasoning under adversarial attacks, which goes beyond existing paradigm that mainly focuses on the shifts of the final predictions (e.g., fake v.s. real). To analyze such reasoning shifts, we introduce a forensic auditing framework to evaluate the robustness of ALMs' reasoning under adversarial attacks in three inter-connected dimensions: acoustic perception, cognitive coherence, and cognitive dissonance. Our systematic analysis reveals that explicit reasoning does not universally enhance robustness. Instead, we observe a bifurcation: for models exhibiting robust acoustic perception, reasoning acts as a defensive \\textit{``shield''}, protecting them from adversarial attacks. However, for others, it imposes a performance \\textit{``tax''}, particularly under linguistic attacks which reduce cognitive coherence and increase attack success rate. Crucially, even when classification fails, high cognitive dissonance can serve as a \\textit{silent alarm}, flagging potential manipulation. Overall, this work provides a critical evaluation of the role of reasoning in forensic audio deepfake analysis and its vulnerabilities.",
      "publishedDate": "2026-01-07T05:46:45Z",
      "updatedDate": "2026-01-07T05:46:45Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.SD",
        "eess.AS"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03615v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03615",
      "comment": "Preprint for ACL 2026 submission",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03604",
      "title": "Interleaved Tool-Call Reasoning for Protein Function Understanding",
      "authors": [
        {
          "name": "Chuanliu Fan",
          "affiliation": null
        },
        {
          "name": "Zicheng Ma",
          "affiliation": null
        },
        {
          "name": "Huanran Meng",
          "affiliation": null
        },
        {
          "name": "Aijia Zhang",
          "affiliation": null
        },
        {
          "name": "Wenjie Du",
          "affiliation": null
        },
        {
          "name": "Jun Zhang",
          "affiliation": null
        },
        {
          "name": "Yi Qin Gao",
          "affiliation": null
        },
        {
          "name": "Ziqiang Cao",
          "affiliation": null
        },
        {
          "name": "Guohong Fu",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in large language models (LLMs) have highlighted the effectiveness of chain-of-thought reasoning in symbolic domains such as mathematics and programming. However, our study shows that directly transferring such text-based reasoning paradigms to protein function understanding is ineffective: reinforcement learning mainly amplifies superficial keyword patterns while failing to introduce new biological knowledge, resulting in limited generalization. We argue that protein function prediction is a knowledge-intensive scientific task that fundamentally relies on external biological priors and computational tools rather than purely internal reasoning. To address this gap, we propose PFUA, a tool-augmented protein reasoning agent that unifies problem decomposition, tool invocation, and grounded answer generation. Instead of relying on long unconstrained reasoning traces, PFUA integrates domain-specific tools to produce verifiable intermediate evidence. Experiments on four benchmarks demonstrate that PFUA consistently outperforms text-only reasoning models with an average performance improvement of 103%.",
      "publishedDate": "2026-01-07T05:34:38Z",
      "updatedDate": "2026-01-07T05:34:38Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03604v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03604",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03603",
      "title": "A Comparative Study of Traditional Machine Learning, Deep Learning, and Large Language Models for Mental Health Forecasting using Smartphone Sensing Data",
      "authors": [
        {
          "name": "Kaidong Feng",
          "affiliation": null
        },
        {
          "name": "Zhu Sun",
          "affiliation": null
        },
        {
          "name": "Roy Ka-Wei Lee",
          "affiliation": null
        },
        {
          "name": "Xun Jiang",
          "affiliation": null
        },
        {
          "name": "Yin-Leng Theng",
          "affiliation": null
        },
        {
          "name": "Yi Ding",
          "affiliation": null
        }
      ],
      "abstract": "Smartphone sensing offers an unobtrusive and scalable way to track daily behaviors linked to mental health, capturing changes in sleep, mobility, and phone use that often precede symptoms of stress, anxiety, or depression. While most prior studies focus on detection that responds to existing conditions, forecasting mental health enables proactive support through Just-in-Time Adaptive Interventions. In this paper, we present the first comprehensive benchmarking study comparing traditional machine learning (ML), deep learning (DL), and large language model (LLM) approaches for mental health forecasting using the College Experience Sensing (CES) dataset, the most extensive longitudinal dataset of college student mental health to date. We systematically evaluate models across temporal windows, feature granularities, personalization strategies, and class imbalance handling. Our results show that DL models, particularly Transformer (Macro-F1 = 0.58), achieve the best overall performance, while LLMs show strength in contextual reasoning but weaker temporal modeling. Personalization substantially improves forecasts of severe mental health states. By revealing how different modeling approaches interpret phone sensing behavioral data over time, this work lays the groundwork for next-generation, adaptive, and human-centered mental health technologies that can advance both research and real-world well-being.",
      "publishedDate": "2026-01-07T05:33:00Z",
      "updatedDate": "2026-01-07T05:33:00Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03603v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03603",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03597",
      "title": "From Chains to Graphs: Self-Structured Reasoning for General-Domain LLMs",
      "authors": [
        {
          "name": "Yingjian Chen",
          "affiliation": null
        },
        {
          "name": "Haoran Liu",
          "affiliation": null
        },
        {
          "name": "Yinhong Liu",
          "affiliation": null
        },
        {
          "name": "Sherry T. Tong",
          "affiliation": null
        },
        {
          "name": "Aosong Feng",
          "affiliation": null
        },
        {
          "name": "Jinghui Lu",
          "affiliation": null
        },
        {
          "name": "Juntao Zhang",
          "affiliation": null
        },
        {
          "name": "Yusuke Iwasawa",
          "affiliation": null
        },
        {
          "name": "Yutaka Matsuo",
          "affiliation": null
        },
        {
          "name": "Irene Li",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) show strong reasoning ability in open-domain question answering, yet their reasoning processes are typically linear and often logically inconsistent. In contrast, real-world reasoning requires integrating multiple premises and solving subproblems in parallel. Existing methods, such as Chain-of-Thought (CoT), express reasoning in a linear textual form, which may appear coherent but frequently leads to inconsistent conclusions. Recent approaches rely on externally provided graphs and do not explore how LLMs can construct and use their own graph-structured reasoning, particularly in open-domain QA. To fill this gap, we novelly explore graph-structured reasoning of LLMs in general-domain question answering. We propose Self-Graph Reasoning (SGR), a framework that enables LLMs to explicitly represent their reasoning process as a structured graph before producing the final answer. We further construct a graph-structured reasoning dataset that merges multiple candidate reasoning graphs into refined graph structures for model training. Experiments on five QA benchmarks across both general and specialized domains show that SGR consistently improves reasoning consistency and yields a 17.74% gain over the base model. The LLaMA-3.3-70B model fine-tuned with SGR performs comparably to GPT-4o and surpasses Claude-3.5-Haiku, demonstrating the effectiveness of graph-structured reasoning.",
      "publishedDate": "2026-01-07T05:27:41Z",
      "updatedDate": "2026-01-07T05:27:41Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03597v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03597",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03590",
      "title": "Can LLMs See Without Pixels? Benchmarking Spatial Intelligence from Textual Descriptions",
      "authors": [
        {
          "name": "Zhongbin Guo",
          "affiliation": null
        },
        {
          "name": "Zhen Yang",
          "affiliation": null
        },
        {
          "name": "Yushan Li",
          "affiliation": null
        },
        {
          "name": "Xinyue Zhang",
          "affiliation": null
        },
        {
          "name": "Wenyu Gao",
          "affiliation": null
        },
        {
          "name": "Jiacheng Wang",
          "affiliation": null
        },
        {
          "name": "Chengzhi Li",
          "affiliation": null
        },
        {
          "name": "Xiangrui Liu",
          "affiliation": null
        },
        {
          "name": "Ping Jian",
          "affiliation": null
        }
      ],
      "abstract": "Recent advancements in Spatial Intelligence (SI) have predominantly relied on Vision-Language Models (VLMs), yet a critical question remains: does spatial understanding originate from visual encoders or the fundamental reasoning backbone? Inspired by this question, we introduce SiT-Bench, a novel benchmark designed to evaluate the SI performance of Large Language Models (LLMs) without pixel-level input, comprises over 3,800 expert-annotated items across five primary categories and 17 subtasks, ranging from egocentric navigation and perspective transformation to fine-grained robotic manipulation. By converting single/multi-view scenes into high-fidelity, coordinate-aware textual descriptions, we challenge LLMs to perform symbolic textual reasoning rather than visual pattern matching. Evaluation results of state-of-the-art (SOTA) LLMs reveals that while models achieve proficiency in localized semantic tasks, a significant \"spatial gap\" remains in global consistency. Notably, we find that explicit spatial reasoning significantly boosts performance, suggesting that LLMs possess latent world-modeling potential. Our proposed dataset SiT-Bench serves as a foundational resource to foster the development of spatially-grounded LLM backbones for future VLMs and embodied agents. Our code and benchmark will be released at https://github.com/binisalegend/SiT-Bench .",
      "publishedDate": "2026-01-07T05:13:52Z",
      "updatedDate": "2026-01-07T05:13:52Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03590v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03590",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "robotics",
        "agents",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "robotics",
          "agents",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03589",
      "title": "OLA: Output Language Alignment in Code-Switched LLM Interactions",
      "authors": [
        {
          "name": "Juhyun Oh",
          "affiliation": null
        },
        {
          "name": "Haneul Yoo",
          "affiliation": null
        },
        {
          "name": "Faiz Ghifari Haznitrama",
          "affiliation": null
        },
        {
          "name": "Alice Oh",
          "affiliation": null
        }
      ],
      "abstract": "Code-switching, alternating between languages within a conversation, is natural for multilingual users, yet poses fundamental challenges for large language models (LLMs). When a user code-switches in their prompt to an LLM, they typically do not specify the expected language of the LLM response, and thus LLMs must infer the output language from contextual and pragmatic cues. We find that current LLMs systematically fail to align with this expectation, responding in undesired languages even when cues are clear to humans. We introduce OLA, a benchmark to evaluate LLMs' Output Language Alignment in code-switched interactions. OLA focuses on Korean--English code-switching and spans simple intra-sentential mixing to instruction-content mismatches. Even frontier models frequently misinterpret implicit language expectation, exhibiting a bias toward non-English responses. We further show this bias generalizes beyond Korean to Chinese and Indonesian pairs. Models also show instability through mid-response switching and language intrusions. Chain-of-Thought prompting fails to resolve these errors, indicating weak pragmatic reasoning about output language. However, Code-Switching Aware DPO with minimal data (about 1K examples) substantially reduces misalignment, suggesting these failures stem from insufficient alignment rather than fundamental limitations. Our results highlight the need to align multilingual LLMs with users' implicit expectations in real-world code-switched interactions.",
      "publishedDate": "2026-01-07T05:07:22Z",
      "updatedDate": "2026-01-07T05:07:22Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03589v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03589",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "reasoning",
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "reasoning",
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03570",
      "title": "How Do Large Language Models Learn Concepts During Continual Pre-Training?",
      "authors": [
        {
          "name": "Barry Menglong Yao",
          "affiliation": null
        },
        {
          "name": "Sha Li",
          "affiliation": null
        },
        {
          "name": "Yunzhi Yao",
          "affiliation": null
        },
        {
          "name": "Minqian Liu",
          "affiliation": null
        },
        {
          "name": "Zaishuo Xia",
          "affiliation": null
        },
        {
          "name": "Qifan Wang",
          "affiliation": null
        },
        {
          "name": "Lifu Huang",
          "affiliation": null
        }
      ],
      "abstract": "Human beings primarily understand the world through concepts (e.g., dog), abstract mental representations that structure perception, reasoning, and learning. However, how large language models (LLMs) acquire, retain, and forget such concepts during continual pretraining remains poorly understood. In this work, we study how individual concepts are acquired and forgotten, as well as how multiple concepts interact through interference and synergy. We link these behavioral dynamics to LLMs' internal Concept Circuits, computational subgraphs associated with specific concepts, and incorporate Graph Metrics to characterize circuit structure. Our analysis reveals: (1) LLMs concept circuits provide a non-trivial, statistically significant signal of concept learning and forgetting; (2) Concept circuits exhibit a stage-wise temporal pattern during continual pretraining, with an early increase followed by gradual decrease and stabilization; (3) concepts with larger learning gains tend to exhibit greater forgetting under subsequent training; (4) semantically similar concepts induce stronger interference than weakly related ones; (5) conceptual knowledge differs in their transferability, with some significantly facilitating the learning of others. Together, our findings offer a circuit-level view of concept learning dynamics and inform the design of more interpretable and robust concept-aware training strategies for LLMs.",
      "publishedDate": "2026-01-07T04:29:15Z",
      "updatedDate": "2026-01-07T04:29:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03570v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03570",
      "comment": "12 pages, 19 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03559",
      "title": "DiffCoT: Diffusion-styled Chain-of-Thought Reasoning in LLMs",
      "authors": [
        {
          "name": "Shidong Cao",
          "affiliation": null
        },
        {
          "name": "Hongzhan Lin",
          "affiliation": null
        },
        {
          "name": "Yuxuan Gu",
          "affiliation": null
        },
        {
          "name": "Ziyang Luo",
          "affiliation": null
        },
        {
          "name": "Jing Ma",
          "affiliation": null
        }
      ],
      "abstract": "Chain-of-Thought (CoT) reasoning improves multi-step mathematical problem solving in large language models but remains vulnerable to exposure bias and error accumulation, as early mistakes propagate irreversibly through autoregressive decoding. In this work, we propose DiffCoT, a diffusion-styled CoT framework that reformulates CoT reasoning as an iterative denoising process. DiffCoT integrates diffusion principles at the reasoning-step level via a sliding-window mechanism, enabling unified generation and retrospective correction of intermediate steps while preserving token-level autoregression. To maintain causal consistency, we further introduce a causal diffusion noise schedule that respects the temporal structure of reasoning chains. Extensive experiments on three multi-step CoT reasoning benchmarks across diverse model backbones demonstrate that DiffCoT consistently outperforms existing CoT preference optimization methods, yielding improved robustness and error-correction capability in CoT reasoning.",
      "publishedDate": "2026-01-07T03:58:42Z",
      "updatedDate": "2026-01-07T03:58:42Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03559v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03559",
      "comment": "DiffCoT improves multi-step LLM reasoning by applying diffusion-based iterative denoising to correct intermediate Chain-of-Thought steps",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03555",
      "title": "SCRIBE: Structured Mid-Level Supervision for Tool-Using Language Models",
      "authors": [
        {
          "name": "Yuxuan Jiang",
          "affiliation": null
        },
        {
          "name": "Francis Ferraro",
          "affiliation": null
        }
      ],
      "abstract": "Training reliable tool-augmented agents remains a significant challenge, largely due to the difficulty of credit assignment in multi-step reasoning. While process-level reward models offer a promising direction, existing LLM-based judges often produce noisy and inconsistent signals because they lack fine-grained, task-specific rubrics to distinguish high-level planning from low-level execution. In this work, we introduce SCRIBE (Skill-Conditioned Reward with Intermediate Behavioral Evaluation), a reinforcement learning framework that intervenes at a novel mid-level abstraction. SCRIBE grounds reward modeling in a curated library of skill prototypes, transforming open-ended LLM evaluation into a constrained verification problem. By routing each subgoal to a corresponding prototype, the reward model is equipped with precise, structured rubrics that substantially reduce reward variance. Experimental results show that SCRIBE achieves state-of-the-art performance across a range of reasoning and tool-use benchmarks. In particular, it improves the AIME25 accuracy of a Qwen3-4B model from 43.3% to 63.3%, and significantly increases success rates in complex multi-turn tool interactions. Further analysis of training dynamics reveals a co-evolution across abstraction levels, where mastery of mid-level skills consistently precedes the emergence of effective high-level planning behaviors. Finally, we demonstrate that SCRIBE is additive to low-level tool optimizations, providing a scalable and complementary pathway toward more autonomous and reliable tool-using agents.",
      "publishedDate": "2026-01-07T03:49:48Z",
      "updatedDate": "2026-01-07T03:49:48Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03555v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03555",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning",
        "evaluation",
        "planning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning",
          "evaluation",
          "planning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03550",
      "title": "ReEfBench: Quantifying the Reasoning Efficiency of LLMs",
      "authors": [
        {
          "name": "Zhizhang Fu",
          "affiliation": null
        },
        {
          "name": "Yuancheng Gu",
          "affiliation": null
        },
        {
          "name": "Chenkai Hu",
          "affiliation": null
        },
        {
          "name": "Hanmeng Liu",
          "affiliation": null
        },
        {
          "name": "Yue Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Test-time scaling has enabled Large Language Models (LLMs) to tackle complex reasoning, yet the limitations of current Chain-of-Thought (CoT) evaluation obscures whether performance gains stem from genuine reasoning or mere verbosity. To address this, (1) we propose a novel neuro-symbolic framework for the non-intrusive, comprehensive process-centric evaluation of reasoning. (2) Through this lens, we identify four distinct behavioral prototypes and diagnose the failure modes. (3) We examine the impact of inference mode, training strategy, and model scale. Our analysis reveals that extended token generation is not a prerequisite for deep reasoning. Furthermore, we reveal critical constraints: mixing long and short CoT data in training risks in premature saturation and collapse, while distillation into smaller models captures behavioral length but fails to replicate logical efficacy due to intrinsic capacity limits.",
      "publishedDate": "2026-01-07T03:33:07Z",
      "updatedDate": "2026-01-07T03:33:07Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03550v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03550",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03542",
      "title": "Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models",
      "authors": [
        {
          "name": "Xukai Liu",
          "affiliation": null
        },
        {
          "name": "Ye Liu",
          "affiliation": null
        },
        {
          "name": "Jipeng Zhang",
          "affiliation": null
        },
        {
          "name": "Yanghai Zhang",
          "affiliation": null
        },
        {
          "name": "Kai Zhang",
          "affiliation": null
        },
        {
          "name": "Qi Liu",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) perform well on multi-hop reasoning, yet how they internally compose multiple facts remains unclear. Recent work proposes \\emph{hop-aligned circuit hypothesis}, suggesting that bridge entities are computed sequentially across layers before later-hop answers. Through systematic analyses on real-world multi-hop queries, we show that this hop-aligned assumption does not generalize: later-hop answer entities can become decodable earlier than bridge entities, a phenomenon we call \\emph{layer-order inversion}, which strengthens with total hops. To explain this behavior, we propose a \\emph{probabilistic recall-and-extract} framework that models multi-hop reasoning as broad probabilistic recall in shallow MLP layers followed by selective extraction in deeper attention layers. This framework is empirically validated through systematic probing analyses, reinterpreting prior layer-wise decoding evidence, explaining chain-of-thought gains, and providing a mechanistic diagnosis of multi-hop failures despite correct single-hop knowledge. Code is available at https://github.com/laquabe/Layer-Order-Inversion.",
      "publishedDate": "2026-01-07T03:13:03Z",
      "updatedDate": "2026-01-07T03:13:03Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03542v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03542",
      "comment": "16 pages, 18 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03537",
      "title": "STAR-S: Improving Safety Alignment through Self-Taught Reasoning on Safety Rules",
      "authors": [
        {
          "name": "Di Wu",
          "affiliation": null
        },
        {
          "name": "Yanyan Zhao",
          "affiliation": null
        },
        {
          "name": "Xin Lu",
          "affiliation": null
        },
        {
          "name": "Mingzhe Li",
          "affiliation": null
        },
        {
          "name": "Bing Qin",
          "affiliation": null
        }
      ],
      "abstract": "Defending against jailbreak attacks is crucial for the safe deployment of Large Language Models (LLMs). Recent research has attempted to improve safety by training models to reason over safety rules before responding. However, a key issue lies in determining what form of safety reasoning effectively defends against jailbreak attacks, which is difficult to explicitly design or directly obtain. To address this, we propose \\textbf{STAR-S} (\\textbf{S}elf-\\textbf{TA}ught \\textbf{R}easoning based on \\textbf{S}afety rules), a framework that integrates the learning of safety rule reasoning into a self-taught loop. The core of STAR-S involves eliciting reasoning and reflection guided by safety rules, then leveraging fine-tuning to enhance safety reasoning. Repeating this process creates a synergistic cycle. Improvements in the model's reasoning and interpretation of safety rules allow it to produce better reasoning data under safety rule prompts, which is then utilized for further training. Experiments show that STAR-S effectively defends against jailbreak attacks, outperforming baselines. Code is available at: https://github.com/pikepokenew/STAR_S.git.",
      "publishedDate": "2026-01-07T03:06:55Z",
      "updatedDate": "2026-01-07T03:06:55Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03537v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03537",
      "comment": "19 pages,4 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03534",
      "title": "Persona-aware and Explainable Bikeability Assessment: A Vision-Language Model Approach",
      "authors": [
        {
          "name": "Yilong Dai",
          "affiliation": null
        },
        {
          "name": "Ziyi Wang",
          "affiliation": null
        },
        {
          "name": "Chenguang Wang",
          "affiliation": null
        },
        {
          "name": "Kexin Zhou",
          "affiliation": null
        },
        {
          "name": "Yiheng Qian",
          "affiliation": null
        },
        {
          "name": "Susu Xu",
          "affiliation": null
        },
        {
          "name": "Xiang Yan",
          "affiliation": null
        }
      ],
      "abstract": "Bikeability assessment is essential for advancing sustainable urban transportation and creating cyclist-friendly cities, and it requires incorporating users' perceptions of safety and comfort. Yet existing perception-based bikeability assessment approaches face key limitations in capturing the complexity of road environments and adequately accounting for heterogeneity in subjective user perceptions. This paper proposes a persona-aware Vision-Language Model framework for bikeability assessment with three novel contributions: (i) theory-grounded persona conditioning based on established cyclist typology that generates persona-specific explanations via chain-of-thought reasoning; (ii) multi-granularity supervised fine-tuning that combines scarce expert-annotated reasoning with abundant user ratings for joint prediction and explainable assessment; and (iii) AI-enabled data augmentation that creates controlled paired data to isolate infrastructure variable impacts. To test and validate this framework, we developed a panoramic image-based crowdsourcing system and collected 12,400 persona-conditioned assessments from 427 cyclists. Experiment results show that the proposed framework offers competitive bikeability rating prediction while uniquely enabling explainable factor attribution.",
      "publishedDate": "2026-01-07T02:46:51Z",
      "updatedDate": "2026-01-07T02:46:51Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.CV",
        "cs.HC",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03534v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03534",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03531",
      "title": "PALM-Bench: A Comprehensive Benchmark for Personalized Audio-Language Models",
      "authors": [
        {
          "name": "Yuwen Wang",
          "affiliation": null
        },
        {
          "name": "Xinyuan Qian",
          "affiliation": null
        },
        {
          "name": "Tian-Hao Zhang",
          "affiliation": null
        },
        {
          "name": "Jiaran Gao",
          "affiliation": null
        },
        {
          "name": "Yuchen Pan",
          "affiliation": null
        },
        {
          "name": "Xin Wang",
          "affiliation": null
        },
        {
          "name": "Zhou Pan",
          "affiliation": null
        },
        {
          "name": "Chen Wei",
          "affiliation": null
        },
        {
          "name": "Yiming Wang",
          "affiliation": null
        }
      ],
      "abstract": "Large Audio-Language Models (LALMs) have demonstrated strong performance in audio understanding and generation. Yet, our extensive benchmarking reveals that their behavior is largely generic (e.g., summarizing spoken content) and fails to adequately support personalized question answering (e.g., summarizing what my best friend says). In contrast, human conditions their interpretation and decision-making on each individual's personal context. To bridge this gap, we formalize the task of Personalized LALMs (PALM) for recognizing personal concepts and reasoning within personal context. Moreover, we create the first benchmark (PALM-Bench) to foster the methodological advances in PALM and enable structured evaluation on several tasks across multi-speaker scenarios. Our extensive experiments on representative open-source LALMs, show that existing training-free prompting and supervised fine-tuning strategies, while yield improvements, remains limited in modeling personalized knowledge and transferring them across tasks robustly. Data and code will be released.",
      "publishedDate": "2026-01-07T02:44:38Z",
      "updatedDate": "2026-01-07T02:44:38Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03531v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03531",
      "comment": "Under review",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "prompting",
        "reasoning",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "prompting",
          "reasoning",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03519",
      "title": "A Vision-Language-Action Model with Visual Prompt for OFF-Road Autonomous Driving",
      "authors": [
        {
          "name": "Liangdong Zhang",
          "affiliation": null
        },
        {
          "name": "Yiming Nie",
          "affiliation": null
        },
        {
          "name": "Haoyang Li",
          "affiliation": null
        },
        {
          "name": "Fanjie Kong",
          "affiliation": null
        },
        {
          "name": "Baobao Zhang",
          "affiliation": null
        },
        {
          "name": "Shunxin Huang",
          "affiliation": null
        },
        {
          "name": "Kai Fu",
          "affiliation": null
        },
        {
          "name": "Chen Min",
          "affiliation": null
        },
        {
          "name": "Liang Xiao",
          "affiliation": null
        }
      ],
      "abstract": "Efficient trajectory planning in off-road terrains presents a formidable challenge for autonomous vehicles, often necessitating complex multi-step pipelines. However, traditional approaches exhibit limited adaptability in dynamic environments. To address these limitations, this paper proposes OFF-EMMA, a novel end-to-end multimodal framework designed to overcome the deficiencies of insufficient spatial perception and unstable reasoning in visual-language-action (VLA) models for off-road autonomous driving scenarios. The framework explicitly annotates input images through the design of a visual prompt block and introduces a chain-of-thought with self-consistency (COT-SC) reasoning strategy to enhance the accuracy and robustness of trajectory planning. The visual prompt block utilizes semantic segmentation masks as visual prompts, enhancing the spatial understanding ability of pre-trained visual-language models for complex terrains. The COT- SC strategy effectively mitigates the error impact of outliers on planning performance through a multi-path reasoning mechanism. Experimental results on the RELLIS-3D off-road dataset demonstrate that OFF-EMMA significantly outperforms existing methods, reducing the average L2 error of the Qwen backbone model by 13.3% and decreasing the failure rate from 16.52% to 6.56%.",
      "publishedDate": "2026-01-07T02:08:18Z",
      "updatedDate": "2026-01-07T02:08:18Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03519v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03519",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "agents",
        "planning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "agents",
          "planning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03515",
      "title": "Mem-Gallery: Benchmarking Multimodal Long-Term Conversational Memory for MLLM Agents",
      "authors": [
        {
          "name": "Yuanchen Bei",
          "affiliation": null
        },
        {
          "name": "Tianxin Wei",
          "affiliation": null
        },
        {
          "name": "Xuying Ning",
          "affiliation": null
        },
        {
          "name": "Yanjun Zhao",
          "affiliation": null
        },
        {
          "name": "Zhining Liu",
          "affiliation": null
        },
        {
          "name": "Xiao Lin",
          "affiliation": null
        },
        {
          "name": "Yada Zhu",
          "affiliation": null
        },
        {
          "name": "Hendrik Hamann",
          "affiliation": null
        },
        {
          "name": "Jingrui He",
          "affiliation": null
        },
        {
          "name": "Hanghang Tong",
          "affiliation": null
        }
      ],
      "abstract": "Long-term memory is a critical capability for multimodal large language model (MLLM) agents, particularly in conversational settings where information accumulates and evolves over time. However, existing benchmarks either evaluate multi-session memory in text-only conversations or assess multimodal understanding within localized contexts, failing to evaluate how multimodal memory is preserved, organized, and evolved across long-term conversational trajectories. Thus, we introduce Mem-Gallery, a new benchmark for evaluating multimodal long-term conversational memory in MLLM agents. Mem-Gallery features high-quality multi-session conversations grounded in both visual and textual information, with long interaction horizons and rich multimodal dependencies. Building on this dataset, we propose a systematic evaluation framework that assesses key memory capabilities along three functional dimensions: memory extraction and test-time adaptation, memory reasoning, and memory knowledge management. Extensive benchmarking across thirteen memory systems reveals several key findings, highlighting the necessity of explicit multimodal information retention and memory organization, the persistent limitations in memory reasoning and knowledge management, as well as the efficiency bottleneck of current models.",
      "publishedDate": "2026-01-07T02:03:13Z",
      "updatedDate": "2026-01-07T02:03:13Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03515v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03515",
      "comment": "34 pages, 18 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03500",
      "title": "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models",
      "authors": [
        {
          "name": "Yuxuan Xia",
          "affiliation": null
        },
        {
          "name": "Siheng Wang",
          "affiliation": null
        },
        {
          "name": "Peng Li",
          "affiliation": null
        }
      ],
      "abstract": "Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process. We identify that visual statistical bias, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations. To address this, we introduce a training-free algorithm called Structure-Disrupted Contrastive Decoding (SDCD), which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.",
      "publishedDate": "2026-01-07T01:27:58Z",
      "updatedDate": "2026-01-07T01:27:58Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03500v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03500",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03471",
      "title": "EpiQAL: Benchmarking Large Language Models in Epidemiological Question Answering for Enhanced Alignment and Reasoning",
      "authors": [
        {
          "name": "Mingyang Wei",
          "affiliation": null
        },
        {
          "name": "Dehai Min",
          "affiliation": null
        },
        {
          "name": "Zewen Liu",
          "affiliation": null
        },
        {
          "name": "Yuzhang Xie",
          "affiliation": null
        },
        {
          "name": "Guanchen Wu",
          "affiliation": null
        },
        {
          "name": "Carl Yang",
          "affiliation": null
        },
        {
          "name": "Max S. Y. Lau",
          "affiliation": null
        },
        {
          "name": "Qi He",
          "affiliation": null
        },
        {
          "name": "Lu Cheng",
          "affiliation": null
        },
        {
          "name": "Wei Jin",
          "affiliation": null
        }
      ],
      "abstract": "Reliable epidemiological reasoning requires synthesizing study evidence to infer disease burden, transmission dynamics, and intervention effects at the population level. Existing medical question answering benchmarks primarily emphasize clinical knowledge or patient-level reasoning, yet few systematically evaluate evidence-grounded epidemiological inference. We present EpiQAL, the first diagnostic benchmark for epidemiological question answering across diverse diseases, comprising three subsets built from open-access literature. The subsets respectively evaluate text-grounded factual recall, multi-step inference linking document evidence with epidemiological principles, and conclusion reconstruction with the Discussion section withheld. Construction combines expert-designed taxonomy guidance, multi-model verification, and retrieval-based difficulty control. Experiments on ten open models reveal that current LLMs show limited performance on epidemiological reasoning, with multi-step inference posing the greatest challenge. Model rankings shift across subsets, and scale alone does not predict success. Chain-of-Thought prompting benefits multi-step inference but yields mixed results elsewhere. EpiQAL provides fine-grained diagnostic signals for evidence grounding, inferential reasoning, and conclusion reconstruction.",
      "publishedDate": "2026-01-06T23:49:10Z",
      "updatedDate": "2026-01-06T23:49:10Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03471v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03471",
      "comment": "21 pages, 3 figures, 12 tables",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "prompting",
        "evaluation",
        "rag"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "prompting",
          "evaluation",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03458",
      "title": "Automated Feedback Generation for Undergraduate Mathematics: Development and Evaluation of an AI Teaching Assistant",
      "authors": [
        {
          "name": "Aron Gohr",
          "affiliation": null
        },
        {
          "name": "Marie-Amelie Lawn",
          "affiliation": null
        },
        {
          "name": "Kevin Gao",
          "affiliation": null
        },
        {
          "name": "Inigo Serjeant",
          "affiliation": null
        },
        {
          "name": "Stephen Heslip",
          "affiliation": null
        }
      ],
      "abstract": "Intelligent tutoring systems have long enabled automated immediate feedback on student work when it is presented in a tightly structured format and when problems are very constrained, but reliably assessing free-form mathematical reasoning remains challenging. We present a system that processes free-form natural language input, handles a wide range of edge cases, and comments competently not only on the technical correctness of submitted proofs, but also on style and presentation issues. We discuss the advantages and disadvantages of various approaches to the evaluation of such a system, and show that by the metrics we evaluate, the quality of the feedback generated is comparable to that produced by human experts when assessing early undergraduate homework. We stress-test our system with a small set of more advanced and unusual questions, and report both significant gaps and encouraging successes in that more challenging setting. Our system uses large language models in a modular workflow. The workflow configuration is human-readable and editable without programming knowledge, and allows some intermediate steps to be precomputed or injected by the instructor. A version of our tool is deployed on the Imperial mathematics homework platform Lambdafeedback. We report also on the integration of our tool into this platform.",
      "publishedDate": "2026-01-06T23:02:22Z",
      "updatedDate": "2026-01-06T23:02:22Z",
      "primaryCategory": "cs.CY",
      "arxivCategories": [
        "cs.CY",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03458v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03458",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03449",
      "title": "FIRE-VLM: A Vision-Language-Driven Reinforcement Learning Framework for UAV Wildfire Tracking in a Physics-Grounded Fire Digital Twin",
      "authors": [
        {
          "name": "Chris Webb",
          "affiliation": null
        },
        {
          "name": "Mobin Habibpour",
          "affiliation": null
        },
        {
          "name": "Mayamin Hamid Raha",
          "affiliation": null
        },
        {
          "name": "Ali Reza Tavakkoli",
          "affiliation": null
        },
        {
          "name": "Janice Coen",
          "affiliation": null
        },
        {
          "name": "Fatemeh Afghah",
          "affiliation": null
        }
      ],
      "abstract": "Wildfire monitoring demands autonomous systems capable of reasoning under extreme visual degradation, rapidly evolving physical dynamics, and scarce real-world training data. Existing UAV navigation approaches rely on simplified simulators and supervised perception pipelines, and lack embodied agents interacting with physically realistic fire environments. We introduce FIRE-VLM, the first end-to-end vision-language model (VLM) guided reinforcement learning (RL) framework trained entirely within a high-fidelity, physics-grounded wildfire digital twin. Built from USGS Digital Elevation Model (DEM) terrain, LANDFIRE fuel inventories, and semi-physical fire-spread solvers, this twin captures terrain-induced runs, wind-driven acceleration, smoke plume occlusion, and dynamic fuel consumption. Within this environment, a PPO agent with dual-view UAV sensing is guided by a CLIP-style VLM. Wildfire-specific semantic alignment scores, derived from a single prompt describing active fire and smoke plumes, are integrated as potential-based reward shaping signals. Our contributions are: (1) a GIS-to-simulation pipeline for constructing wildfire digital twins; (2) a VLM-guided RL agent for UAV firefront tracking; and (3) a wildfire-aware reward design that combines physical terms with VLM semantics. Across five digital-twin evaluation tasks, our VLM-guided policy reduces time-to-detection by up to 6 times, increases time-in-FOV, and is, to our knowledge, the first RL-based UAV wildfire monitoring system demonstrated in kilometer-scale, physics-grounded digital-twin fires.",
      "publishedDate": "2026-01-06T22:31:57Z",
      "updatedDate": "2026-01-06T22:31:57Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03449v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03449",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "reasoning",
        "prompting",
        "robotics",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "reasoning",
          "prompting",
          "robotics",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03448",
      "title": "Enhancing Linguistic Competence of Language Models through Pre-training with Language Learning Tasks",
      "authors": [
        {
          "name": "Atsuki Yamaguchi",
          "affiliation": null
        },
        {
          "name": "Maggie Mi",
          "affiliation": null
        },
        {
          "name": "Nikolaos Aletras",
          "affiliation": null
        }
      ],
      "abstract": "Language models (LMs) are pre-trained on raw text datasets to generate text sequences token-by-token. While this approach facilitates the learning of world knowledge and reasoning, it does not explicitly optimize for linguistic competence. To bridge this gap, we propose L2T, a pre-training framework integrating Language Learning Tasks alongside standard next-token prediction. Inspired by human language acquisition, L2T transforms raw text into structured input-output pairs to provide explicit linguistic stimulation. Pre-training LMs on a mixture of raw text and L2T data not only improves overall performance on linguistic competence benchmarks but accelerates its acquisition, while maintaining competitive performance on general reasoning tasks.",
      "publishedDate": "2026-01-06T22:28:15Z",
      "updatedDate": "2026-01-06T22:28:15Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03448v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03448",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03436",
      "title": "MARVEL: A Multi Agent-based Research Validator and Enabler using Large Language Models",
      "authors": [
        {
          "name": "Nikhil Mukund",
          "affiliation": null
        },
        {
          "name": "Yifang Luo",
          "affiliation": null
        },
        {
          "name": "Fan Zhang",
          "affiliation": null
        },
        {
          "name": "Lisa Barsotti",
          "affiliation": null
        },
        {
          "name": "Erik Katsavounidis",
          "affiliation": null
        }
      ],
      "abstract": "We present MARVEL (https://ligogpt.mit.edu/marvel), a locally deployable, open-source framework for domain-aware question answering and assisted scientific research. It is designed to address the increasing demands of a digital assistant for scientific groups that can read highly technical data, cite precisely, and operate within authenticated networks. MARVEL combines a fast path for straightforward queries with a more deliberate DeepSearch mode that integrates retrieval-augmented generation and Monte Carlo Tree Search. It explores complementary subqueries, allocates more compute to promising branches, and maintains a global evidence ledger that preserves sources during drafting. We applied this framework in the context of gravitational-wave research related to the Laser Interferometer Gravitational-wave Observatory. Answers are grounded in a curated semantic index of research literature, doctoral theses, LIGO documents, and long-running detector electronic logbooks, with targeted web searches when appropriate. Because direct benchmarking against commercial LLMs cannot be performed on private data, we evaluated MARVEL on two publicly available surrogate datasets that capture comparable semantic and technical characteristics. On these benchmarks, MARVEL matches a GPT-4o mini baseline on literature-centric queries and substantially outperforms it on detector-operations content, where domain retrieval and guided reasoning are decisive. By making the complete framework and evaluation datasets openly available, we aim to provide a reproducible foundation for developing domain-specific scientific assistants.",
      "publishedDate": "2026-01-06T21:47:22Z",
      "updatedDate": "2026-01-06T21:47:22Z",
      "primaryCategory": "astro-ph.IM",
      "arxivCategories": [
        "astro-ph.IM",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03436v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03436",
      "comment": "18 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03434",
      "title": "VNU-Bench: A Benchmarking Dataset for Multi-Source Multimodal News Video Understanding",
      "authors": [
        {
          "name": "Zibo Liu",
          "affiliation": null
        },
        {
          "name": "Muyang Li",
          "affiliation": null
        },
        {
          "name": "Zhe Jiang",
          "affiliation": null
        },
        {
          "name": "Shigang Chen",
          "affiliation": null
        }
      ],
      "abstract": "News videos are carefully edited multimodal narratives that combine narration, visuals, and external quotations into coherent storylines. In recent years, there have been significant advances in evaluating multimodal large language models (MLLMs) for news video understanding. However, existing benchmarks largely focus on single-source, intra-video reasoning, where each report is processed in isolation. In contrast, real-world news consumption is inherently multi-sourced: the same event is reported by different outlets with complementary details, distinct narrative choices, and sometimes conflicting claims that unfold over time. Robust news understanding, therefore, requires models to compare perspectives from different sources, align multimodal evidence across sources, and synthesize multi-source information. To fill this gap, we introduce VNU-Bench, the first benchmark for multi-source, cross-video understanding in the news domain. We design a set of new question types that are unique in testing models' ability of understanding multi-source multimodal news from a variety of different angles. We design a novel hybrid human-model QA generation process that addresses the issues of scalability and quality control in building a large dataset for cross-source news understanding. The dataset comprises 429 news groups, 1,405 videos, and 2,501 high-quality questions. Comprehensive evaluation of both closed- and open-source multimodal models shows that VNU-Bench poses substantial challenges for current MLLMs.",
      "publishedDate": "2026-01-06T21:42:44Z",
      "updatedDate": "2026-01-06T21:42:44Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03434v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03434",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03432",
      "title": "CodeEval: A pedagogical approach for targeted evaluation of code-trained Large Language Models",
      "authors": [
        {
          "name": "Danny Brahman",
          "affiliation": null
        },
        {
          "name": "Mohammad Mahoor",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are predominantly assessed based on their common sense reasoning, language comprehension, and logical reasoning abilities. While models trained in specialized domains like mathematics or coding have demonstrated remarkable advancements in logical reasoning, there remains a significant gap in evaluating their code generation capabilities. Existing benchmark datasets fall short in pinpointing specific strengths and weaknesses, impeding targeted enhancements in models' reasoning abilities to synthesize code. To bridge this gap, our paper introduces an innovative, pedagogical benchmarking method that mirrors the evaluation processes encountered in academic programming courses. We introduce CodeEval, a multi-dimensional benchmark dataset designed to rigorously evaluate LLMs across 24 distinct aspects of Python programming. The dataset covers three proficiency levels - beginner, intermediate, and advanced - and includes both class-based and function-based problem types with detailed problem specifications and comprehensive test suites. To facilitate widespread adoption, we also developed RunCodeEval, an open-source execution framework that provides researchers with a ready-to-use evaluation pipeline for CodeEval. RunCodeEval handles test execution, context setup, and metrics generation, enabling researchers to quickly obtain detailed insights into model strengths and weaknesses across complexity levels, problem types, and programming categories. This combination enables targeted evaluation and guides improvements in LLMs' programming proficiencies.",
      "publishedDate": "2026-01-06T21:42:01Z",
      "updatedDate": "2026-01-06T21:42:01Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03432v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03432",
      "comment": "Accepted at the International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics, 2025. Will be published at ACL anthology",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "code-generation",
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "code-generation",
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03423",
      "title": "Training-Free Adaptation of New-Generation LLMs using Legacy Clinical Models",
      "authors": [
        {
          "name": "Sasha Ronaghi",
          "affiliation": null
        },
        {
          "name": "Chloe Stanwyck",
          "affiliation": null
        },
        {
          "name": "Asad Aali",
          "affiliation": null
        },
        {
          "name": "Amir Ronaghi",
          "affiliation": null
        },
        {
          "name": "Miguel Fuentes",
          "affiliation": null
        },
        {
          "name": "Tina Hernandez-Boussard",
          "affiliation": null
        },
        {
          "name": "Emily Alsentzer",
          "affiliation": null
        }
      ],
      "abstract": "Adapting language models to the clinical domain through continued pretraining and fine-tuning requires costly retraining for each new model generation. We propose Cross-Architecture Proxy Tuning (CAPT), a model-ensembling approach that enables training-free adaptation of state-of-the-art general-domain models using existing clinical models. CAPT supports models with disjoint vocabularies, leveraging contrastive decoding to selectively inject clinically relevant signals while preserving the general-domain model's reasoning and fluency. On six clinical classification and text-generation tasks, CAPT with a new-generation general-domain model and an older-generation clinical model consistently outperforms both models individually and state-of-the-art ensembling approaches (average +17.6% over UniTE, +41.4% over proxy tuning across tasks). Through token-level analysis and physician case studies, we demonstrate that CAPT amplifies clinically actionable language, reduces context errors, and increases clinical specificity.",
      "publishedDate": "2026-01-06T21:23:47Z",
      "updatedDate": "2026-01-06T21:23:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03423v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03423",
      "comment": "29 pages, 3 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03416",
      "title": "GAMBIT: A Gamified Jailbreak Framework for Multimodal Large Language Models",
      "authors": [
        {
          "name": "Xiangdong Hu",
          "affiliation": null
        },
        {
          "name": "Yangyang Jiang",
          "affiliation": null
        },
        {
          "name": "Qin Hu",
          "affiliation": null
        },
        {
          "name": "Xiaojun Jia",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have become widely deployed, yet their safety alignment remains fragile under adversarial inputs. Previous work has shown that increasing inference steps can disrupt safety mechanisms and lead MLLMs to generate attacker-desired harmful content. However, most existing attacks focus on increasing the complexity of the modified visual task itself and do not explicitly leverage the model's own reasoning incentives. This leads to them underperforming on reasoning models (Models with Chain-of-Thoughts) compared to non-reasoning ones (Models without Chain-of-Thoughts). If a model can think like a human, can we influence its cognitive-stage decisions so that it proactively completes a jailbreak? To validate this idea, we propose GAMBI} (Gamified Adversarial Multimodal Breakout via Instructional Traps), a novel multimodal jailbreak framework that decomposes and reassembles harmful visual semantics, then constructs a gamified scene that drives the model to explore, reconstruct intent, and answer as part of winning the game. The resulting structured reasoning chain increases task complexity in both vision and text, positioning the model as a participant whose goal pursuit reduces safety attention and induces it to answer the reconstructed malicious query. Extensive experiments on popular reasoning and non-reasoning MLLMs demonstrate that GAMBIT achieves high Attack Success Rates (ASR), reaching 92.13% on Gemini 2.5 Flash, 91.20% on QvQ-MAX, and 85.87% on GPT-4o, significantly outperforming baselines.",
      "publishedDate": "2026-01-06T21:09:10Z",
      "updatedDate": "2026-01-06T21:09:10Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03416v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03416",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03400",
      "title": "Eye-Q: A Multilingual Benchmark for Visual Word Puzzle Solving and Image-to-Phrase Reasoning",
      "authors": [
        {
          "name": "Ali Najar",
          "affiliation": null
        },
        {
          "name": "Alireza Mirrokni",
          "affiliation": null
        },
        {
          "name": "Arshia Izadyari",
          "affiliation": null
        },
        {
          "name": "Sadegh Mohammadian",
          "affiliation": null
        },
        {
          "name": "Amir Homayoon Sharifizade",
          "affiliation": null
        },
        {
          "name": "Asal Meskin",
          "affiliation": null
        },
        {
          "name": "Mobin Bagherian",
          "affiliation": null
        },
        {
          "name": "Ehsaneddin Asgari",
          "affiliation": null
        }
      ],
      "abstract": "Vision-Language Models (VLMs) have achieved strong performance on standard vision-language benchmarks, yet often rely on surface-level recognition rather than deeper reasoning. We propose visual word puzzles as a challenging alternative, as they require discovering implicit visual cues, generating and revising hypotheses, and mapping perceptual evidence to non-literal concepts in ways that are difficult to solve via literal grounding, OCR-heavy shortcuts, or simple retrieval-style matching. We introduce Eye-Q, a multilingual benchmark designed to assess this form of complex visual understanding. Eye-Q contains 1,343 puzzles in which a model observes a conceptually dense scene with a brief description and must infer a specific target word or phrase. The puzzles are intentionally unstructured and cue-implicit, with distractors and contextual relationships that demand selective attention, abstraction, and associative inference. The benchmark spans English, Persian, Arabic, and cross-lingual puzzles. We evaluate state-of-the-art VLMs using an open-ended, human-aligned protocol that probes hypothesis formation and revision under lightweight assistance. Results reveal substantial performance gaps, especially on abstract and cross-lingual puzzles, highlighting limitations in current models' ability to construct and search over appropriate conceptual representations for flexible image-to-phrase inference; maximum accuracy reaches only 60.27%.",
      "publishedDate": "2026-01-06T20:27:29Z",
      "updatedDate": "2026-01-06T20:27:29Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03400v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03400",
      "comment": "8 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03388",
      "title": "Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models",
      "authors": [
        {
          "name": "Zhibo Hu",
          "affiliation": null
        },
        {
          "name": "Chen Wang",
          "affiliation": null
        },
        {
          "name": "Yanfeng Shu",
          "affiliation": null
        },
        {
          "name": "Hye-young Paik",
          "affiliation": null
        },
        {
          "name": "Liming Zhu",
          "affiliation": null
        }
      ],
      "abstract": "Earlier research has shown that metaphors influence human's decision making, which raises the question of whether metaphors also influence large language models (LLMs)' reasoning pathways, considering their training data contain a large number of metaphors. In this work, we investigate the problem in the scope of the emergent misalignment problem where LLMs can generalize patterns learned from misaligned content in one domain to another domain. We discover a strong causal relationship between metaphors in training data and the misalignment degree of LLMs' reasoning contents. With interventions using metaphors in pre-training, fine-tuning and re-alignment phases, models' cross-domain misalignment degrees change significantly. As we delve deeper into the causes behind this phenomenon, we observe that there is a connection between metaphors and the activation of global and local latent features of large reasoning models. By monitoring these latent features, we design a detector that predict misaligned content with high accuracy.",
      "publishedDate": "2026-01-06T19:50:58Z",
      "updatedDate": "2026-01-06T19:50:58Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03388v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03388",
      "comment": "17 pages, 7 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03369",
      "title": "RiskCueBench: Benchmarking Anticipatory Reasoning from Early Risk Cues in Video-Language Models",
      "authors": [
        {
          "name": "Sha Luo",
          "affiliation": null
        },
        {
          "name": "Yogesh Prabhu",
          "affiliation": null
        },
        {
          "name": "Tim Ossowski",
          "affiliation": null
        },
        {
          "name": "Kaiping Chen",
          "affiliation": null
        },
        {
          "name": "Junjie Hu",
          "affiliation": null
        }
      ],
      "abstract": "With the rapid growth of video centered social media, the ability to anticipate risky events from visual data is a promising direction for ensuring public safety and preventing real world accidents. Prior work has extensively studied supervised video risk assessment across domains such as driving, protests, and natural disasters. However, many existing datasets provide models with access to the full video sequence, including the accident itself, which substantially reduces the difficulty of the task. To better reflect real world conditions, we introduce a new video understanding benchmark RiskCueBench in which videos are carefully annotated to identify a risk signal clip, defined as the earliest moment that indicates a potential safety concern. Experimental results reveal a significant gap in current systems ability to interpret evolving situations and anticipate future risky events from early visual signals, highlighting important challenges for deploying video risk prediction models in practice.",
      "publishedDate": "2026-01-06T19:14:49Z",
      "updatedDate": "2026-01-06T19:14:49Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03369v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03369",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "tool-use",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "tool-use",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03331",
      "title": "MMErroR: A Benchmark for Erroneous Reasoning in Vision-Language Models",
      "authors": [
        {
          "name": "Yang Shi",
          "affiliation": null
        },
        {
          "name": "Yifeng Xie",
          "affiliation": null
        },
        {
          "name": "Minzhe Guo",
          "affiliation": null
        },
        {
          "name": "Liangsi Lu",
          "affiliation": null
        },
        {
          "name": "Mingxuan Huang",
          "affiliation": null
        },
        {
          "name": "Jingchao Wang",
          "affiliation": null
        },
        {
          "name": "Zhihong Zhu",
          "affiliation": null
        },
        {
          "name": "Boyan Xu",
          "affiliation": null
        },
        {
          "name": "Zhiqi Huang",
          "affiliation": null
        }
      ],
      "abstract": "Recent advances in Vision-Language Models (VLMs) have improved performance in multi-modal learning, raising the question of whether these models truly understand the content they process. Crucially, can VLMs detect when a reasoning process is wrong and identify its error type? To answer this, we present MMErroR, a multi-modal benchmark of 2,013 samples, each embedding a single coherent reasoning error. These samples span 24 subdomains across six top-level domains, ensuring broad coverage and taxonomic richness. Unlike existing benchmarks that focus on answer correctness, MMErroR targets a process-level, error-centric evaluation that requires models to detect incorrect reasoning and classify the error type within both visual and linguistic contexts. We evaluate 20 advanced VLMs, even the best model (Gemini-3.0-Pro) classifies the error in only 66.47\\% of cases, underscoring the challenge of identifying erroneous reasoning. Furthermore, the ability to accurately identify errors offers valuable insights into the capabilities of multi-modal reasoning models. Project Page: https://mmerror-benchmark.github.io",
      "publishedDate": "2026-01-06T17:45:26Z",
      "updatedDate": "2026-01-06T17:45:26Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03331v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03331",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning",
        "rag"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning",
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03321",
      "title": "Aligning Findings with Diagnosis: A Self-Consistent Reinforcement Learning Framework for Trustworthy Radiology Reporting",
      "authors": [
        {
          "name": "Kun Zhao",
          "affiliation": null
        },
        {
          "name": "Siyuan Dai",
          "affiliation": null
        },
        {
          "name": "Pan Wang",
          "affiliation": null
        },
        {
          "name": "Jifeng Song",
          "affiliation": null
        },
        {
          "name": "Hui Ji",
          "affiliation": null
        },
        {
          "name": "Chenghua Lin",
          "affiliation": null
        },
        {
          "name": "Liang Zhan",
          "affiliation": null
        },
        {
          "name": "Haoteng Tang",
          "affiliation": null
        }
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have shown strong potential for radiology report generation, yet their clinical translation is hindered by architectural heterogeneity and the prevalence of factual hallucinations. Standard supervised fine-tuning often fails to strictly align linguistic outputs with visual evidence, while existing reinforcement learning approaches struggle with either prohibitive computational costs or limited exploration. To address these challenges, we propose a comprehensive framework for self-consistent radiology report generation. First, we conduct a systematic evaluation to identify optimal vision encoder and LLM backbone configurations for medical imaging. Building on this foundation, we introduce a novel \"Reason-then-Summarize\" architecture optimized via Group Relative Policy Optimization (GRPO). This framework restructures generation into two distinct components: a think block for detailed findings and an answer block for structured disease labels. By utilizing a multi-dimensional composite reward function, we explicitly penalize logical discrepancies between the generated narrative and the final diagnosis. Extensive experiments on the MIMIC-CXR benchmark demonstrate that our method achieves state-of-the-art performance in clinical efficacy metrics and significantly reduces hallucinations compared to strong supervised baselines.",
      "publishedDate": "2026-01-06T14:17:44Z",
      "updatedDate": "2026-01-06T14:17:44Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03321v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03321",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03320",
      "title": "Ratio-Variance Regularized Policy Optimization for Efficient LLM Fine-tuning",
      "authors": [
        {
          "name": "Yu Luo",
          "affiliation": null
        },
        {
          "name": "Shuo Han",
          "affiliation": null
        },
        {
          "name": "Yihan Hu",
          "affiliation": null
        },
        {
          "name": "Dong Li",
          "affiliation": null
        },
        {
          "name": "Jianye Hao",
          "affiliation": null
        }
      ],
      "abstract": "On-policy reinforcement learning (RL), particularly Proximal Policy Optimization (PPO) and Group Relative Policy Optimization (GRPO), has become the dominant paradigm for fine-tuning large language models (LLMs). While policy ratio clipping stabilizes training, this heuristic hard constraint incurs a fundamental cost: it indiscriminately truncates gradients from high-return yet high-divergence actions, suppressing rare but highly informative \"eureka moments\" in complex reasoning. Moreover, once data becomes slightly stale, hard clipping renders it unusable, leading to severe sample inefficiency. In this work, we revisit the trust-region objective in policy optimization and show that explicitly constraining the \\emph{variance (second central moment) of the policy ratio} provides a principled and smooth relaxation of hard clipping. This distributional constraint stabilizes policy updates while preserving gradient signals from valuable trajectories. Building on this insight, we propose $R^2VPO$ (Ratio-Variance Regularized Policy Optimization), a novel primal-dual framework that supports stable on-policy learning and enables principled off-policy data reuse by dynamically reweighting stale samples rather than discarding them. We extensively evaluate $R^2VPO$ on fine-tuning state-of-the-art LLMs, including DeepSeek-Distill-Qwen-1.5B and the openPangu-Embedded series (1B and 7B), across challenging mathematical reasoning benchmarks. Experimental results show that $R^2VPO$ consistently achieves superior asymptotic performance, with average relative gains of up to 17% over strong clipping-based baselines, while requiring approximately 50% fewer rollouts to reach convergence. These findings establish ratio-variance control as a promising direction for improving both stability and data efficiency in RL-based LLM alignment.",
      "publishedDate": "2026-01-06T14:01:42Z",
      "updatedDate": "2026-01-06T14:01:42Z",
      "primaryCategory": "cs.LG",
      "arxivCategories": [
        "cs.LG",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03320v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03320",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "reasoning",
        "rag",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "reasoning",
          "rag",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03287",
      "title": "Automated Post-Incident Policy Gap Analysis via Threat-Informed Evidence Mapping using Large Language Models",
      "authors": [
        {
          "name": "Huan Lin Oh",
          "affiliation": null
        },
        {
          "name": "Jay Yong Jun Jie",
          "affiliation": null
        },
        {
          "name": "Mandy Lee Ling Siu",
          "affiliation": null
        },
        {
          "name": "Jonathan Pan",
          "affiliation": null
        }
      ],
      "abstract": "Cybersecurity post-incident reviews are essential for identifying control failures and improving organisational resilience, yet they remain labour-intensive, time-consuming, and heavily reliant on expert judgment. This paper investigates whether Large Language Models (LLMs) can augment post-incident review workflows by autonomously analysing system evidence and identifying security policy gaps. We present a threat-informed, agentic framework that ingests log data, maps observed behaviours to the MITRE ATT&CK framework, and evaluates organisational security policies for adequacy and compliance. Using a simulated brute-force attack scenario against a Windows OpenSSH service (MITRE ATT&CK T1110), the system leverages GPT-4o for reasoning, LangGraph for multi-agent workflow orchestration, and LlamaIndex for traceable policy retrieval. Experimental results indicate that the LLM-based pipeline can interpret log-derived evidence, identify insufficient or missing policy controls, and generate actionable remediation recommendations with explicit evidence-to-policy traceability. Unlike prior work that treats log analysis and policy validation as isolated tasks, this study integrates both into a unified end-to-end proof-of-concept post-incident review framework. The findings suggest that LLM-assisted analysis has the potential to improve the efficiency, consistency, and auditability of post-incident evaluations, while highlighting the continued need for human oversight in high-stakes cybersecurity decision-making.",
      "publishedDate": "2026-01-04T01:39:20Z",
      "updatedDate": "2026-01-04T01:39:20Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03287v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03287",
      "comment": "5 pages, 1 figure. Preprint",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "rag",
        "reasoning",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "rag",
          "reasoning",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03286",
      "title": "HyperCLOVA X 32B Think",
      "authors": [
        {
          "name": "NAVER Cloud HyperCLOVA X Team",
          "affiliation": null
        }
      ],
      "abstract": "In this report, we present HyperCLOVA X 32B Think, a vision-language model designed with particular emphasis on reasoning within the Korean linguistic and cultural context, as well as agentic ability. HyperCLOVA X 32B Think is pre-trained with a strong focus on reasoning capabilities and subsequently post-trained to support multimodal understanding, enhanced reasoning, agentic behaviors, and alignment with human preferences. Experimental evaluations against comparably sized models demonstrate that our model achieves strong performance on Korean text-to-text and vision-to-text benchmarks, as well as on agent-oriented evaluation tasks. By open-sourcing HyperCLOVA X 32B Think, we aim to support broader adoption and facilitate further research and innovation across both academic and industrial communities.",
      "publishedDate": "2026-01-03T06:39:38Z",
      "updatedDate": "2026-01-03T06:39:38Z",
      "primaryCategory": "cs.CV",
      "arxivCategories": [
        "cs.CV",
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03286v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03286",
      "comment": "Technical Report",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03285",
      "title": "Feedback Indices to Evaluate LLM Responses to Rebuttals for Multiple Choice Type Questions",
      "authors": [
        {
          "name": "Justin C. Dunlap",
          "affiliation": null
        },
        {
          "name": "Anne-Simone Parent",
          "affiliation": null
        },
        {
          "name": "Ralf Widenhorn",
          "affiliation": null
        }
      ],
      "abstract": "We present a systematic framework of indices designed to characterize Large Language Model (LLM) responses when challenged with rebuttals during a chat. Assessing how LLMs respond to user dissent is crucial for understanding their reliability and behavior patterns, yet the complexity of human-LLM interactions makes systematic evaluation challenging. Our approach employs a fictitious-response rebuttal method that quantifies LLM behavior when presented with multiple-choice questions followed by deliberate challenges to their fictitious previous response. The indices are specifically designed to detect and measure what could be characterized as sycophantic behavior (excessive agreement with user challenges) or stubborn responses (rigid adherence to the fictitious response in the chat history) from LLMs. These metrics allow investigation of the relationships between sycophancy, stubbornness, and the model's actual mastery of the subject matter. We demonstrate the utility of these indices using two physics problems as test scenarios with various OpenAI models. The framework is intentionally generalizable to any multiple-choice format question, including on topics without universally accepted correct answers. Our results reveal measurable differences across OpenAI model generations, with trends indicating that newer models and those employing greater \"Reasoning Effort\" exhibit reduced sycophantic behavior. The FR pairing method combined with our proposed indices provides a practical, adaptable toolkit for systematically comparing LLM dialogue behaviors across different models and contexts.",
      "publishedDate": "2026-01-02T21:16:43Z",
      "updatedDate": "2026-01-02T21:16:43Z",
      "primaryCategory": "physics.ed-ph",
      "arxivCategories": [
        "physics.ed-ph",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03285v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03285",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "reasoning"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "reasoning"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03281",
      "title": "$α^3$-Bench: A Unified Benchmark of Safety, Robustness, and Efficiency for LLM-Based UAV Agents over 6G Networks",
      "authors": [
        {
          "name": "Mohamed Amine Ferrag",
          "affiliation": null
        },
        {
          "name": "Abderrahmane Lakas",
          "affiliation": null
        },
        {
          "name": "Merouane Debbah",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) are increasingly used as high level controllers for autonomous Unmanned Aerial Vehicle (UAV) missions. However, existing evaluations rarely assess whether such agents remain safe, protocol compliant, and effective under realistic next generation networking constraints. This paper introduces $α^3$-Bench, a benchmark for evaluating LLM driven UAV autonomy as a multi turn conversational reasoning and control problem operating under dynamic 6G conditions. Each mission is formulated as a language mediated control loop between an LLM based UAV agent and a human operator, where decisions must satisfy strict schema validity, mission policies, speaker alternation, and safety constraints while adapting to fluctuating network slices, latency, jitter, packet loss, throughput, and edge load variations. To reflect modern agentic workflows, $α^3$-Bench integrates a dual action layer supporting both tool calls and agent to agent coordination, enabling evaluation of tool use consistency and multi agent interactions. We construct a large scale corpus of 113k conversational UAV episodes grounded in UAVBench scenarios and evaluate 17 state of the art LLMs using a fixed subset of 50 episodes per scenario under deterministic decoding. We propose a composite $α^3$ metric that unifies six pillars: Task Outcome, Safety Policy, Tool Consistency, Interaction Quality, Network Robustness, and Communication Cost, with efficiency normalized scores per second and per thousand tokens. Results show that while several models achieve high mission success and safety compliance, robustness and efficiency vary significantly under degraded 6G conditions, highlighting the need for network aware and resource efficient LLM based UAV agents. The dataset is publicly available on GitHub : https://github.com/maferrag/AlphaBench",
      "publishedDate": "2026-01-01T12:07:06Z",
      "updatedDate": "2026-01-01T12:07:06Z",
      "primaryCategory": "eess.SY",
      "arxivCategories": [
        "eess.SY",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03281v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03281",
      "comment": "20 pages",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "multi-agent",
        "tool-use",
        "reasoning",
        "rag",
        "code-generation"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "multi-agent",
          "tool-use",
          "reasoning",
          "rag",
          "code-generation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03509",
      "title": "Evolving Programmatic Skill Networks",
      "authors": [
        {
          "name": "Haochen Shi",
          "affiliation": null
        },
        {
          "name": "Xingdi Yuan",
          "affiliation": null
        },
        {
          "name": "Bang Liu",
          "affiliation": null
        }
      ],
      "abstract": "We study continual skill acquisition in open-ended embodied environments where an agent must construct, refine, and reuse an expanding library of executable skills. We introduce the Programmatic Skill Network (PSN), a framework in which skills are executable symbolic programs forming a compositional network that evolves through experience. PSN defines three core mechanisms instantiated via large language models: (1)REFLECT for structured fault localization over skill compositions, (2) progressive optimization with maturity-aware update gating that stabilizes reliable skills while maintaining plasticity for uncertain ones, and (3) canonical structural refactoring under rollback validation that maintains network compactness. We further show that PSN's learning dynamics exhibit structural parallels to neural network training. Experiments on MineDojo and Crafter demonstrate robust skill reuse, rapid adaptation, and strong generalization across open-ended task distributions.\\footnote{We plan to open-source the code.",
      "publishedDate": "2026-01-07T01:43:25Z",
      "updatedDate": "2026-01-07T01:43:25Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03509v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03509",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "code-generation",
        "robotics"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "code-generation",
          "robotics"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03398",
      "title": "Towards Zero-Knowledge Task Planning via a Language-based Approach",
      "authors": [
        {
          "name": "Liam Merz Hoffmeister",
          "affiliation": null
        },
        {
          "name": "Brian Scassellati",
          "affiliation": null
        },
        {
          "name": "Daniel Rakita",
          "affiliation": null
        }
      ],
      "abstract": "In this work, we introduce and formalize the Zero-Knowledge Task Planning (ZKTP) problem, i.e., formulating a sequence of actions to achieve some goal without task-specific knowledge. Additionally, we present a first investigation and approach for ZKTP that leverages a large language model (LLM) to decompose natural language instructions into subtasks and generate behavior trees (BTs) for execution. If errors arise during task execution, the approach also uses an LLM to adjust the BTs on-the-fly in a refinement loop. Experimental validation in the AI2-THOR simulator demonstrate our approach's effectiveness in improving overall task performance compared to alternative approaches that leverage task-specific knowledge. Our work demonstrates the potential of LLMs to effectively address several aspects of the ZKTP problem, providing a robust framework for automated behavior generation with no task-specific setup.",
      "publishedDate": "2026-01-06T20:18:15Z",
      "updatedDate": "2026-01-06T20:18:15Z",
      "primaryCategory": "cs.RO",
      "arxivCategories": [
        "cs.RO"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03398v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03398",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "planning",
        "agents",
        "rag",
        "prompting"
      ],
      "tags": {
        "auto": [
          "planning",
          "agents",
          "rag",
          "prompting"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03641",
      "title": "Agent-Dice: Disentangling Knowledge Updates via Geometric Consensus for Agent Continual Learning",
      "authors": [
        {
          "name": "Zheng Wu",
          "affiliation": null
        },
        {
          "name": "Xingyu Lou",
          "affiliation": null
        },
        {
          "name": "Xinbei Ma",
          "affiliation": null
        },
        {
          "name": "Yansi Li",
          "affiliation": null
        },
        {
          "name": "Weiwen Liu",
          "affiliation": null
        },
        {
          "name": "Weinan Zhang",
          "affiliation": null
        },
        {
          "name": "Jun Wang",
          "affiliation": null
        },
        {
          "name": "Zhuosheng Zhang",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Model (LLM)-based agents significantly extend the utility of LLMs by interacting with dynamic environments. However, enabling agents to continually learn new tasks without catastrophic forgetting remains a critical challenge, known as the stability-plasticity dilemma. In this work, we argue that this dilemma fundamentally arises from the failure to explicitly distinguish between common knowledge shared across tasks and conflicting knowledge introduced by task-specific interference. To address this, we propose Agent-Dice, a parameter fusion framework based on directional consensus evaluation. Concretely, Agent-Dice disentangles knowledge updates through a two-stage process: geometric consensus filtering to prune conflicting gradients, and curvature-based importance weighting to amplify shared semantics. We provide a rigorous theoretical analysis that establishes the validity of the proposed fusion scheme and offers insight into the origins of the stability-plasticity dilemma. Extensive experiments on GUI agents and tool-use agent domains demonstrate that Agent-Dice exhibits outstanding continual learning performance with minimal computational overhead and parameter updates.",
      "publishedDate": "2026-01-07T06:43:50Z",
      "updatedDate": "2026-01-07T06:43:50Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03641v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03641",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.04034",
      "title": "HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense",
      "authors": [
        {
          "name": "Siyuan Li",
          "affiliation": null
        },
        {
          "name": "Xi Lin",
          "affiliation": null
        },
        {
          "name": "Jun Wu",
          "affiliation": null
        },
        {
          "name": "Zehao Liu",
          "affiliation": null
        },
        {
          "name": "Haoyu Li",
          "affiliation": null
        },
        {
          "name": "Tianjie Ju",
          "affiliation": null
        },
        {
          "name": "Xiang Chen",
          "affiliation": null
        },
        {
          "name": "Jianhua Li",
          "affiliation": null
        }
      ],
      "abstract": "Jailbreak attacks pose significant threats to large language models (LLMs), enabling attackers to bypass safeguards. However, existing reactive defense approaches struggle to keep up with the rapidly evolving multi-turn jailbreaks, where attackers continuously deepen their attacks to exploit vulnerabilities. To address this critical challenge, we propose HoneyTrap, a novel deceptive LLM defense framework leveraging collaborative defenders to counter jailbreak attacks. It integrates four defensive agents, Threat Interceptor, Misdirection Controller, Forensic Tracker, and System Harmonizer, each performing a specialized security role and collaborating to complete a deceptive defense. To ensure a comprehensive evaluation, we introduce MTJ-Pro, a challenging multi-turn progressive jailbreak dataset that combines seven advanced jailbreak strategies designed to gradually deepen attack strategies across multi-turn attacks. Besides, we present two novel metrics: Mislead Success Rate (MSR) and Attack Resource Consumption (ARC), which provide more nuanced assessments of deceptive defense beyond conventional measures. Experimental results on GPT-4, GPT-3.5-turbo, Gemini-1.5-pro, and LLaMa-3.1 demonstrate that HoneyTrap achieves an average reduction of 68.77% in attack success rates compared to state-of-the-art baselines. Notably, even in a dedicated adaptive attacker setting with intensified conditions, HoneyTrap remains resilient, leveraging deceptive engagement to prolong interactions, significantly increasing the time and computational costs required for successful exploitation. Unlike simple rejection, HoneyTrap strategically wastes attacker resources without impacting benign queries, improving MSR and ARC by 118.11% and 149.16%, respectively.",
      "publishedDate": "2026-01-07T15:47:28Z",
      "updatedDate": "2026-01-07T15:47:28Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.04034v1",
      "arxivUrl": "https://arxiv.org/abs/2601.04034",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "evaluation",
        "agents",
        "tool-use",
        "rag",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "evaluation",
          "agents",
          "tool-use",
          "rag",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03671",
      "title": "NeuronScope: A Multi-Agent Framework for Explaining Polysemantic Neurons in Language Models",
      "authors": [
        {
          "name": "Weiqi Liu",
          "affiliation": null
        },
        {
          "name": "Yongliang Miao",
          "affiliation": null
        },
        {
          "name": "Haiyan Zhao",
          "affiliation": null
        },
        {
          "name": "Yanguang Liu",
          "affiliation": null
        },
        {
          "name": "Mengnan Du",
          "affiliation": null
        }
      ],
      "abstract": "Neuron-level interpretation in large language models (LLMs) is fundamentally challenged by widespread polysemanticity, where individual neurons respond to multiple distinct semantic concepts. Existing single-pass interpretation methods struggle to faithfully capture such multi-concept behavior. In this work, we propose NeuronScope, a multi-agent framework that reformulates neuron interpretation as an iterative, activation-guided process. NeuronScope explicitly deconstructs neuron activations into atomic semantic components, clusters them into distinct semantic modes, and iteratively refines each explanation using neuron activation feedback. Experiments demonstrate that NeuronScope uncovers hidden polysemanticity and produces explanations with significantly higher activation correlation compared to single-pass baselines.",
      "publishedDate": "2026-01-07T07:50:47Z",
      "updatedDate": "2026-01-07T07:50:47Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL",
        "cs.LG"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03671v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03671",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03359",
      "title": "Enhancing LLM Instruction Following: An Evaluation-Driven Multi-Agentic Workflow for Prompt Instructions Optimization",
      "authors": [
        {
          "name": "Alberto Purpura",
          "affiliation": null
        },
        {
          "name": "Li Wang",
          "affiliation": null
        },
        {
          "name": "Sahil Badyal",
          "affiliation": null
        },
        {
          "name": "Eugenio Beaufrand",
          "affiliation": null
        },
        {
          "name": "Adam Faulkner",
          "affiliation": null
        }
      ],
      "abstract": "Large Language Models (LLMs) often generate substantively relevant content but fail to adhere to formal constraints, leading to outputs that are conceptually correct but procedurally flawed. Traditional prompt refinement approaches focus on rephrasing the description of the primary task an LLM has to perform, neglecting the granular constraints that function as acceptance criteria for its response. We propose a novel multi-agentic workflow that decouples optimization of the primary task description from its constraints, using quantitative scores as feedback to iteratively rewrite and improve them. Our evaluation demonstrates this method produces revised prompts that yield significantly higher compliance scores from models like Llama 3.1 8B and Mixtral-8x 7B.",
      "publishedDate": "2026-01-06T19:02:14Z",
      "updatedDate": "2026-01-06T19:02:14Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03359v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03359",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "prompting",
        "agents",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "prompting",
          "agents",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03335",
      "title": "Digital Red Queen: Adversarial Program Evolution in Core War with LLMs",
      "authors": [
        {
          "name": "Akarsh Kumar",
          "affiliation": null
        },
        {
          "name": "Ryan Bahlous-Boldi",
          "affiliation": null
        },
        {
          "name": "Prafull Sharma",
          "affiliation": null
        },
        {
          "name": "Phillip Isola",
          "affiliation": null
        },
        {
          "name": "Sebastian Risi",
          "affiliation": null
        },
        {
          "name": "Yujin Tang",
          "affiliation": null
        },
        {
          "name": "David Ha",
          "affiliation": null
        }
      ],
      "abstract": "Large language models (LLMs) are increasingly being used to evolve solutions to problems in many domains, in a process inspired by biological evolution. However, unlike biological evolution, most LLM-evolution frameworks are formulated as static optimization problems, overlooking the open-ended adversarial dynamics that characterize real-world evolutionary processes. Here, we study Digital Red Queen (DRQ), a simple self-play algorithm that embraces these so-called \"Red Queen\" dynamics via continual adaptation to a changing objective. DRQ uses an LLM to evolve assembly-like programs, called warriors, which compete against each other for control of a virtual machine in the game of Core War, a Turing-complete environment studied in artificial life and connected to cybersecurity. In each round of DRQ, the model evolves a new warrior to defeat all previous ones, producing a sequence of adapted warriors. Over many rounds, we observe that warriors become increasingly general (relative to a set of held-out human warriors). Interestingly, warriors also become less behaviorally diverse across independent runs, indicating a convergence pressure toward a general-purpose behavioral strategy, much like convergent evolution in nature. This result highlights a potential value of shifting from static objectives to dynamic Red Queen objectives. Our work positions Core War as a rich, controllable sandbox for studying adversarial adaptation in artificial systems and for evaluating LLM-based evolution methods. More broadly, the simplicity and effectiveness of DRQ suggest that similarly minimal self-play approaches could prove useful in other more practical multi-agent adversarial domains, like real-world cybersecurity or combating drug resistance.",
      "publishedDate": "2026-01-06T18:58:17Z",
      "updatedDate": "2026-01-06T18:58:17Z",
      "primaryCategory": "cs.AI",
      "arxivCategories": [
        "cs.AI",
        "cs.NE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03335v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03335",
      "comment": "14 pages, 13 figures",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "multi-agent"
      ],
      "tags": {
        "auto": [
          "agents",
          "multi-agent"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03328",
      "title": "LLM-Enabled Multi-Agent Systems: Empirical Evaluation and Insights into Emerging Design Patterns & Paradigms",
      "authors": [
        {
          "name": "Harri Renney",
          "affiliation": null
        },
        {
          "name": "Maxim N Nethercott",
          "affiliation": null
        },
        {
          "name": "Nathan Renney",
          "affiliation": null
        },
        {
          "name": "Peter Hayes",
          "affiliation": null
        }
      ],
      "abstract": "This paper formalises the literature on emerging design patterns and paradigms for Large Language Model (LLM)-enabled multi-agent systems (MAS), evaluating their practical utility across various domains. We define key architectural components, including agent orchestration, communication mechanisms, and control-flow strategies, and demonstrate how these enable rapid development of modular, domain-adaptive solutions. Three real-world case studies are tested in controlled, containerised pilots in telecommunications security, national heritage asset management, and utilities customer service automation. Initial empirical results show that, for these case studies, prototypes were delivered within two weeks and pilot-ready solutions within one month, suggesting reduced development overhead compared to conventional approaches and improved user accessibility. However, findings also reinforce limitations documented in the literature, including variability in LLM behaviour that leads to challenges in transitioning from prototype to production maturity. We conclude by outlining critical research directions for improving reliability, scalability, and governance in MAS architectures and the further work needed to mature MAS design patterns to mitigate the inherent challenges.",
      "publishedDate": "2026-01-06T16:50:49Z",
      "updatedDate": "2026-01-06T16:50:49Z",
      "primaryCategory": "cs.MA",
      "arxivCategories": [
        "cs.MA"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03328v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03328",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "agents",
        "tool-use",
        "multi-agent",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "agents",
          "tool-use",
          "multi-agent",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03979",
      "title": "SoK: Privacy Risks and Mitigations in Retrieval-Augmented Generation Systems",
      "authors": [
        {
          "name": "Andreea-Elena Bodea",
          "affiliation": null
        },
        {
          "name": "Stephen Meisenbacher",
          "affiliation": null
        },
        {
          "name": "Alexandra Klymenko",
          "affiliation": null
        },
        {
          "name": "Florian Matthes",
          "affiliation": null
        }
      ],
      "abstract": "The continued promise of Large Language Models (LLMs), particularly in their natural language understanding and generation capabilities, has driven a rapidly increasing interest in identifying and developing LLM use cases. In an effort to complement the ingrained \"knowledge\" of LLMs, Retrieval-Augmented Generation (RAG) techniques have become widely popular. At its core, RAG involves the coupling of LLMs with domain-specific knowledge bases, whereby the generation of a response to a user question is augmented with contextual and up-to-date information. The proliferation of RAG has sparked concerns about data privacy, particularly with the inherent risks that arise when leveraging databases with potentially sensitive information. Numerous recent works have explored various aspects of privacy risks in RAG systems, from adversarial attacks to proposed mitigations. With the goal of surveying and unifying these works, we ask one simple question: What are the privacy risks in RAG, and how can they be measured and mitigated? To answer this question, we conduct a systematic literature review of RAG works addressing privacy, and we systematize our findings into a comprehensive set of privacy risks, mitigation techniques, and evaluation strategies. We supplement these findings with two primary artifacts: a Taxonomy of RAG Privacy Risks and a RAG Privacy Process Diagram. Our work contributes to the study of privacy in RAG not only by conducting the first systematization of risks and mitigations, but also by uncovering important considerations when mitigating privacy risks in RAG systems and assessing the current maturity of proposed mitigations.",
      "publishedDate": "2026-01-07T14:50:41Z",
      "updatedDate": "2026-01-07T14:50:41Z",
      "primaryCategory": "cs.CR",
      "arxivCategories": [
        "cs.CR",
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03979v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03979",
      "comment": "17 pages, 3 figures, 5 tables. This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML 2026). The final version will be available on IEEE Xplore",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "tool-use",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "tool-use",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03908",
      "title": "Decide Then Retrieve: A Training-Free Framework with Uncertainty-Guided Triggering and Dual-Path Retrieval",
      "authors": [
        {
          "name": "Wang Chen",
          "affiliation": null
        },
        {
          "name": "Guanqiang Qi",
          "affiliation": null
        },
        {
          "name": "Weikang Li",
          "affiliation": null
        },
        {
          "name": "Yang Li",
          "affiliation": null
        },
        {
          "name": "Deguo Xia",
          "affiliation": null
        },
        {
          "name": "Jizhou Huang",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge, but existing approaches indiscriminately trigger retrieval and rely on single-path evidence construction, often introducing noise and limiting performance gains. In this work, we propose Decide Then Retrieve (DTR), a training-free framework that adaptively determines when retrieval is necessary and how external information should be selected. DTR leverages generation uncertainty to guide retrieval triggering and introduces a dual-path retrieval mechanism with adaptive information selection to better handle sparse and ambiguous queries. Extensive experiments across five open-domain QA benchmarks, multiple model scales, and different retrievers demonstrate that DTR consistently improves EM and F1 over standard RAG and strong retrieval-enhanced baselines, while reducing unnecessary retrievals. The code and data used in this paper are available at https://github.com/ChenWangHKU/DTR.",
      "publishedDate": "2026-01-07T13:20:59Z",
      "updatedDate": "2026-01-07T13:20:59Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03908v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03908",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03748",
      "title": "Bridging OLAP and RAG: A Multidimensional Approach to the Design of Corpus Partitioning",
      "authors": [
        {
          "name": "Dario Maio",
          "affiliation": null
        },
        {
          "name": "Stefano Rizzi",
          "affiliation": null
        }
      ],
      "abstract": "Retrieval-Augmented Generation (RAG) systems are increasingly deployed on large-scale document collections, often comprising millions of documents and tens of millions of text chunks. In industrial-scale retrieval platforms, scalability is typically addressed through horizontal sharding and a combination of Approximate Nearest-Neighbor search, hybrid indexing, and optimized metadata filtering. Although effective from an efficiency perspective, these mechanisms rely on bottom-up, similarity-driven organization and lack a conceptual rationale for corpus partitioning. In this paper, we claim that the design of large-scale RAG systems may benefit from the combination of two orthogonal strategies: semantic clustering, which optimizes locality in embedding space, and multidimensional partitioning, which governs where retrieval should occur based on conceptual dimensions such as time and organizational context. Although such dimensions are already implicitly present in current systems, they are used in an ad hoc and poorly structured manner. We propose the Dimensional Fact Model (DFM) as a conceptual framework to guide the design of multidimensional partitions for RAG corpora. The DFM provides a principled way to reason about facts, dimensions, hierarchies, and granularity in retrieval-oriented settings. This framework naturally supports hierarchical routing and controlled fallback strategies, ensuring that retrieval remains robust even in the presence of incomplete metadata, while transforming the search process from a 'black-box' similarity matching into a governable and deterministic workflow. This work is intended as a position paper; its goal is to bridge the gap between OLAP-style multidimensional modeling and modern RAG architectures, and to stimulate further research on principled, explainable, and governable retrieval strategies at scale.",
      "publishedDate": "2026-01-07T09:37:36Z",
      "updatedDate": "2026-01-07T09:37:36Z",
      "primaryCategory": "cs.IR",
      "arxivCategories": [
        "cs.IR",
        "cs.AI"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03748v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03748",
      "comment": null,
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag"
      ],
      "tags": {
        "auto": [
          "rag"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03746",
      "title": "Whose Facts Win? LLM Source Preferences under Knowledge Conflicts",
      "authors": [
        {
          "name": "Jakob Schuster",
          "affiliation": null
        },
        {
          "name": "Vagrant Gautam",
          "affiliation": null
        },
        {
          "name": "Katja Markert",
          "affiliation": null
        }
      ],
      "abstract": "As large language models (LLMs) are more frequently used in retrieval-augmented generation pipelines, it is increasingly relevant to study their behavior under knowledge conflicts. Thus far, the role of the source of the retrieved information has gone unexamined. We address this gap with a novel framework to investigate how source preferences affect LLM resolution of inter-context knowledge conflicts in English, motivated by interdisciplinary research on credibility. With a comprehensive, tightly-controlled evaluation of 13 open-weight LLMs, we find that LLMs prefer institutionally-corroborated information (e.g., government or newspaper sources) over information from people and social media. However, these source preferences can be reversed by simply repeating information from less credible sources. To mitigate repetition effects and maintain consistent preferences, we propose a novel method that reduces repetition bias by up to 99.8%, while also maintaining at least 88.8% of original preferences. We release all data and code to encourage future work on credibility and source preferences in knowledge-intensive NLP.",
      "publishedDate": "2026-01-07T09:35:35Z",
      "updatedDate": "2026-01-07T09:35:35Z",
      "primaryCategory": "cs.CL",
      "arxivCategories": [
        "cs.CL"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03746v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03746",
      "comment": "Data and code: https://github.com/JaSchuste/llm-source-preference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03618",
      "title": "The Pneuma Project: Reifying Information Needs as Relational Schemas to Automate Discovery, Guide Preparation, and Align Data with Intent",
      "authors": [
        {
          "name": "Muhammad Imam Luthfi Balaka",
          "affiliation": null
        },
        {
          "name": "Raul Castro Fernandez",
          "affiliation": null
        }
      ],
      "abstract": "Data discovery and preparation remain persistent bottlenecks in the data management lifecycle, especially when user intent is vague, evolving, or difficult to operationalize. The Pneuma Project introduces Pneuma-Seeker, a system that helps users articulate and fulfill information needs through iterative interaction with a language model-powered platform. The system reifies the user's evolving information need as a relational data model and incrementally converges toward a usable document aligned with that intent. To achieve this, the system combines three architectural ideas: context specialization to reduce LLM burden across subtasks, a conductor-style planner to assemble dynamic execution plans, and a convergence mechanism based on shared state. The system integrates recent advances in retrieval-augmented generation (RAG), agentic frameworks, and structured data preparation to support semi-automatic, language-guided workflows. We evaluate the system through LLM-based user simulations and show that it helps surface latent intent, guide discovery, and produce fit-for-purpose documents. It also acts as an emergent documentation layer, capturing institutional knowledge and supporting organizational memory.",
      "publishedDate": "2026-01-07T05:58:54Z",
      "updatedDate": "2026-01-07T05:58:54Z",
      "primaryCategory": "cs.DB",
      "arxivCategories": [
        "cs.DB"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03618v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03618",
      "comment": "CIDR 2026 Paper",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "agents"
      ],
      "tags": {
        "auto": [
          "rag",
          "agents"
        ],
        "manual": []
      }
    },
    {
      "id": "2601.03378",
      "title": "RepoShapley: Shapley-Enhanced Context Filtering for Repository-Level Code Completion",
      "authors": [
        {
          "name": "Yu Huo",
          "affiliation": null
        },
        {
          "name": "Siyu Zhang",
          "affiliation": null
        },
        {
          "name": "Kun Zeng",
          "affiliation": null
        },
        {
          "name": "Yuquan Lu",
          "affiliation": null
        },
        {
          "name": "Cheng Yang",
          "affiliation": null
        },
        {
          "name": "Yifu Guo",
          "affiliation": null
        },
        {
          "name": "Xiaoying Tang",
          "affiliation": null
        }
      ],
      "abstract": "Repository-level code completion benefits from retrieval-augmented generation (RAG). However, controlling cross-file evidence is difficult because chunk utility is often interaction-dependent: some snippets help only when paired with complementary context, while others harm decoding when they conflict. We propose RepoShapley, a coalition-aware context filtering framework supervised by Shapley-style marginal contributions. Our module ChunkShapley constructs offline labels by (i) single-chunk probing with teacher-forced likelihood to estimate signed, weighted effects, (ii) a surrogate game that captures saturation and interference, (iii) exact Shapley computation for small retrieval sets, and (iv) bounded post-verification that selects a decoding-optimal coalition using the frozen generator. We distill verified $KEEP$ or $DROP$ decisions and retrieval triggering into a single model via discrete control tokens. Experiments across benchmarks and backbones show that RepoShapley improves completion quality while reducing harmful context and unnecessary retrieval. Code: https://anonymous.4open.science/r/a7f3c9.",
      "publishedDate": "2026-01-06T19:27:32Z",
      "updatedDate": "2026-01-06T19:27:32Z",
      "primaryCategory": "cs.SE",
      "arxivCategories": [
        "cs.SE"
      ],
      "pdfUrl": "https://arxiv.org/pdf/2601.03378v1",
      "arxivUrl": "https://arxiv.org/abs/2601.03378",
      "comment": "22pages, 9 figures, conference",
      "journalRef": null,
      "doi": null,
      "fetchedAt": "2026-01-08T03:28:12.179Z",
      "categories": [
        "rag",
        "code-generation",
        "evaluation"
      ],
      "tags": {
        "auto": [
          "rag",
          "code-generation",
          "evaluation"
        ],
        "manual": []
      }
    }
  ]
}